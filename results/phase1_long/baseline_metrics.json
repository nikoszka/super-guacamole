{
  "long": [
    {
      "example_id": "dpql_3081--77/77_636065.txt#0_2",
      "dataset_type": "long",
      "response": "donald trump",
      "accuracy": 1.0,
      "g_nll": 0.08662388087395811,
      "average_nll": 0.02165597021848953,
      "perplexity": 1.0218921626522732,
      "avg_token_probability": 0.9787909927862137,
      "sequence_length": 4
    },
    {
      "example_id": "qw_11987--33/33_444596.txt#0_0",
      "dataset_type": "long",
      "response": "puerto rico",
      "accuracy": 1.0,
      "g_nll": 0.06864210916683078,
      "average_nll": 0.022880703055610258,
      "perplexity": 1.023144474256906,
      "avg_token_probability": 0.9774606956606157,
      "sequence_length": 3
    },
    {
      "example_id": "odql_13428--179/179_2328241.txt#0_2",
      "dataset_type": "long",
      "response": "roumania",
      "accuracy": 0.0,
      "g_nll": 8.392784561379813,
      "average_nll": 2.797594853793271,
      "perplexity": 16.405142517091328,
      "avg_token_probability": 0.6482028842023148,
      "sequence_length": 3
    },
    {
      "example_id": "wh_252--162/162_2659147.txt#0_1",
      "dataset_type": "long",
      "response": "the battle of the Somme",
      "accuracy": 1.0,
      "g_nll": 1.6501972748155822,
      "average_nll": 0.23574246783079747,
      "perplexity": 1.2658482715347141,
      "avg_token_probability": 0.856026242612358,
      "sequence_length": 7
    },
    {
      "example_id": "qz_3971--21/21_196365.txt#0_0",
      "dataset_type": "long",
      "response": "michelle ryan",
      "accuracy": 1.0,
      "g_nll": 0.0873784784507663,
      "average_nll": 0.01747569569015326,
      "perplexity": 1.0176292890726522,
      "avg_token_probability": 0.9828065472898173,
      "sequence_length": 5
    },
    {
      "example_id": "qg_762--191/191_2525665.txt#0_2",
      "dataset_type": "long",
      "response": "adam's apple",
      "accuracy": 0.0,
      "g_nll": 0.15133707982022315,
      "average_nll": 0.03783426995505579,
      "perplexity": 1.0385590981689283,
      "avg_token_probability": 0.9638591940312548,
      "sequence_length": 4
    },
    {
      "example_id": "qb_2537--177/177_336227.txt#0_1",
      "dataset_type": "long",
      "response": "transfusion",
      "accuracy": 0.0,
      "g_nll": 0.11745710531249642,
      "average_nll": 0.03915236843749881,
      "perplexity": 1.039928923924451,
      "avg_token_probability": 0.9623723804322443,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2182--78/78_65006.txt#0_0",
      "dataset_type": "long",
      "response": "pope",
      "accuracy": 0.0,
      "g_nll": 2.3268776908516884,
      "average_nll": 1.1634388454258442,
      "perplexity": 3.2009218467404983,
      "avg_token_probability": 0.4987053513873002,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14528--95/95_493438.txt#0_0",
      "dataset_type": "long",
      "response": "the duchess of alba",
      "accuracy": 0.0,
      "g_nll": 4.01991708483547,
      "average_nll": 0.5024896356044337,
      "perplexity": 1.6528310997429827,
      "avg_token_probability": 0.7004723220627445,
      "sequence_length": 8
    },
    {
      "example_id": "bb_8426--98/98_1020444.txt#0_1",
      "dataset_type": "long",
      "response": "goat",
      "accuracy": 1.0,
      "g_nll": 0.12255712412297726,
      "average_nll": 0.06127856206148863,
      "perplexity": 1.0631950387425038,
      "avg_token_probability": 0.9412040328378284,
      "sequence_length": 2
    },
    {
      "example_id": "odql_9757--12/12_139633.txt#0_2",
      "dataset_type": "long",
      "response": "goliath frog",
      "accuracy": 1.0,
      "g_nll": 0.8155991530729807,
      "average_nll": 0.16311983061459615,
      "perplexity": 1.1771777431694885,
      "avg_token_probability": 0.8846562109218622,
      "sequence_length": 5
    },
    {
      "example_id": "bb_4945--Germany.txt#0_0",
      "dataset_type": "long",
      "response": "bundesliga",
      "accuracy": 0.0,
      "g_nll": 0.9000746191013604,
      "average_nll": 0.2250186547753401,
      "perplexity": 1.2523460782086941,
      "avg_token_probability": 0.834265261322413,
      "sequence_length": 4
    },
    {
      "example_id": "qz_5999--134/134_385487.txt#0_0",
      "dataset_type": "long",
      "response": "stans getz",
      "accuracy": 0.0,
      "g_nll": 1.9657845869360244,
      "average_nll": 0.3931569173872049,
      "perplexity": 1.4816508678717608,
      "avg_token_probability": 0.7486154861532726,
      "sequence_length": 5
    },
    {
      "example_id": "qf_944--160/160_2466400.txt#0_1",
      "dataset_type": "long",
      "response": "basketball",
      "accuracy": 1.0,
      "g_nll": 0.02377992309629917,
      "average_nll": 0.011889961548149586,
      "perplexity": 1.0119609281253443,
      "avg_token_probability": 0.9882157114957322,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_127--7/7_554431.txt#0_1",
      "dataset_type": "long",
      "response": "melrose",
      "accuracy": 1.0,
      "g_nll": 0.9915516723121982,
      "average_nll": 0.3305172241040661,
      "perplexity": 1.3916877567963262,
      "avg_token_probability": 0.781275110850101,
      "sequence_length": 3
    },
    {
      "example_id": "odql_5884--11/11_405432.txt#0_0",
      "dataset_type": "long",
      "response": "the indian ocean",
      "accuracy": 0.0,
      "g_nll": 2.4954110415419564,
      "average_nll": 0.6238527603854891,
      "perplexity": 1.866103860644298,
      "avg_token_probability": 0.7246304602262897,
      "sequence_length": 4
    },
    {
      "example_id": "qz_1794--3/3_142632.txt#0_1",
      "dataset_type": "long",
      "response": "sunday post",
      "accuracy": 1.0,
      "g_nll": 0.15500127582345158,
      "average_nll": 0.051667091941150524,
      "perplexity": 1.0530251236048538,
      "avg_token_probability": 0.9508466035659541,
      "sequence_length": 3
    },
    {
      "example_id": "qb_6369--37/37_442927.txt#0_0",
      "dataset_type": "long",
      "response": "bosworth field",
      "accuracy": 0.0,
      "g_nll": 1.0075675073967432,
      "average_nll": 0.2518918768491858,
      "perplexity": 1.2864569339877432,
      "avg_token_probability": 0.8150389302608463,
      "sequence_length": 4
    },
    {
      "example_id": "odql_6714--124/124_3212527.txt#0_0",
      "dataset_type": "long",
      "response": "oasis",
      "accuracy": 1.0,
      "g_nll": 0.13890857249498367,
      "average_nll": 0.06945428624749184,
      "perplexity": 1.0719230584593276,
      "avg_token_probability": 0.9330987253392821,
      "sequence_length": 2
    },
    {
      "example_id": "bb_4857--20/20_940277.txt#0_1",
      "dataset_type": "long",
      "response": "barbator",
      "accuracy": 0.0,
      "g_nll": 1.4080279096961021,
      "average_nll": 0.35200697742402554,
      "perplexity": 1.4219184450118854,
      "avg_token_probability": 0.7327813232440749,
      "sequence_length": 4
    },
    {
      "example_id": "odql_11176--12/12_2284937.txt#0_0",
      "dataset_type": "long",
      "response": "bcc",
      "accuracy": 1.0,
      "g_nll": 0.08561008737888187,
      "average_nll": 0.028536695792960625,
      "perplexity": 1.0289477681960206,
      "avg_token_probability": 0.9722393396948036,
      "sequence_length": 3
    },
    {
      "example_id": "bt_3472--182/182_791380.txt#0_1",
      "dataset_type": "long",
      "response": "snow",
      "accuracy": 1.0,
      "g_nll": 1.0406494960188866,
      "average_nll": 0.5203247480094433,
      "perplexity": 1.6825739735339422,
      "avg_token_probability": 0.6537225548399073,
      "sequence_length": 2
    },
    {
      "example_id": "qw_15697--74/74_1333011.txt#0_0",
      "dataset_type": "long",
      "response": "peter gabriel, daryl hall and john oates, kiss, nirvana, linda ronstadt, cat stevens",
      "accuracy": 0.0,
      "g_nll": 4.074766169609944,
      "average_nll": 0.15091726554110904,
      "perplexity": 1.1629004421628628,
      "avg_token_probability": 0.9015693382289937,
      "sequence_length": 27
    },
    {
      "example_id": "tb_1169--191/191_2063376.txt#0_2",
      "dataset_type": "long",
      "response": "luxor",
      "accuracy": 1.0,
      "g_nll": 0.1048803515700456,
      "average_nll": 0.034960117190015204,
      "perplexity": 1.035578406199198,
      "avg_token_probability": 0.9661964356712046,
      "sequence_length": 3
    },
    {
      "example_id": "qw_11463--117/117_93387.txt#0_0",
      "dataset_type": "long",
      "response": "moon",
      "accuracy": 1.0,
      "g_nll": 0.06079251691699028,
      "average_nll": 0.03039625845849514,
      "perplexity": 1.0308629411907577,
      "avg_token_probability": 0.9700664900801477,
      "sequence_length": 2
    },
    {
      "example_id": "wh_2955--50/50_794801.txt#0_0",
      "dataset_type": "long",
      "response": "ireland",
      "accuracy": 0.0,
      "g_nll": 0.9834012500805329,
      "average_nll": 0.32780041669351095,
      "perplexity": 1.3879119405906621,
      "avg_token_probability": 0.7847145398981988,
      "sequence_length": 3
    },
    {
      "example_id": "qg_4089--181/181_2265370.txt#0_2",
      "dataset_type": "long",
      "response": "washington",
      "accuracy": 1.0,
      "g_nll": 0.06424863077700138,
      "average_nll": 0.03212431538850069,
      "perplexity": 1.0326458710986806,
      "avg_token_probability": 0.9686687497809622,
      "sequence_length": 2
    },
    {
      "example_id": "bb_5497--152/152_643813.txt#0_2",
      "dataset_type": "long",
      "response": "portugal",
      "accuracy": 1.0,
      "g_nll": 0.040961019164569734,
      "average_nll": 0.013653673054856577,
      "perplexity": 1.0137473101267909,
      "avg_token_probability": 0.9865904002777155,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_17967--123/123_103337.txt#0_0",
      "dataset_type": "long",
      "response": "charles atlas",
      "accuracy": 0.0,
      "g_nll": 0.02166018928073754,
      "average_nll": 0.005415047320184385,
      "perplexity": 1.0054297351887909,
      "avg_token_probability": 0.9946153511515737,
      "sequence_length": 4
    },
    {
      "example_id": "bb_3805--121/121_2943039.txt#0_2",
      "dataset_type": "long",
      "response": "coconut shy",
      "accuracy": 0.0,
      "g_nll": 0.037261299788951874,
      "average_nll": 0.018630649894475937,
      "perplexity": 1.0188052832775105,
      "avg_token_probability": 0.9815468638957195,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_6274--62/62_943590.txt#0_0",
      "dataset_type": "long",
      "response": "botticelli",
      "accuracy": 0.0,
      "g_nll": 0.3717956565769782,
      "average_nll": 0.09294891414424455,
      "perplexity": 1.097405671928211,
      "avg_token_probability": 0.9202501963489862,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_7181--189/189_1916422.txt#0_1",
      "dataset_type": "long",
      "response": "sternum",
      "accuracy": 0.0,
      "g_nll": 0.0247028417797992,
      "average_nll": 0.0082342805932664,
      "perplexity": 1.0082682755255858,
      "avg_token_probability": 0.9918307215018926,
      "sequence_length": 3
    },
    {
      "example_id": "qg_488--37/37_82446.txt#0_0",
      "dataset_type": "long",
      "response": "galileo galilei",
      "accuracy": 1.0,
      "g_nll": 0.30259175894752843,
      "average_nll": 0.05043195982458807,
      "perplexity": 1.0517253013464964,
      "avg_token_probability": 0.9526588668081698,
      "sequence_length": 6
    },
    {
      "example_id": "qf_1931--68/68_2482724.txt#0_0",
      "dataset_type": "long",
      "response": "midnight cowboy",
      "accuracy": 1.0,
      "g_nll": 0.05147836962714791,
      "average_nll": 0.01715945654238264,
      "perplexity": 1.017307525733293,
      "avg_token_probability": 0.9831204791205855,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1696--66/66_26750.txt#0_0",
      "dataset_type": "long",
      "response": "tanzania",
      "accuracy": 1.0,
      "g_nll": 0.18028374754760534,
      "average_nll": 0.045070936886901336,
      "perplexity": 1.0461020645006012,
      "avg_token_probability": 0.9576090819075278,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1494--24/24_306802.txt#0_1",
      "dataset_type": "long",
      "response": "sheffield",
      "accuracy": 1.0,
      "g_nll": 0.06694515596609563,
      "average_nll": 0.022315051988698542,
      "perplexity": 1.0225658951458105,
      "avg_token_probability": 0.9782414769292398,
      "sequence_length": 3
    },
    {
      "example_id": "qb_5390--56/56_2628509.txt#0_1",
      "dataset_type": "long",
      "response": "malvinas",
      "accuracy": 0.0,
      "g_nll": 0.09486423537327937,
      "average_nll": 0.031621411791093124,
      "perplexity": 1.0321266803376503,
      "avg_token_probability": 0.9698284924132691,
      "sequence_length": 3
    },
    {
      "example_id": "qw_661--149/149_1063026.txt#0_1",
      "dataset_type": "long",
      "response": "france",
      "accuracy": 1.0,
      "g_nll": 0.040536108426749706,
      "average_nll": 0.020268054213374853,
      "perplexity": 1.0204748459500106,
      "avg_token_probability": 0.9800368750053207,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8182--60/60_558286.txt#0_0",
      "dataset_type": "long",
      "response": "smell",
      "accuracy": 1.0,
      "g_nll": 0.07216048985719681,
      "average_nll": 0.036080244928598404,
      "perplexity": 1.0367390362030382,
      "avg_token_probability": 0.9647060321610215,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14324--47/47_1545305.txt#0_1",
      "dataset_type": "long",
      "response": "audi",
      "accuracy": 1.0,
      "g_nll": 0.04679122846573591,
      "average_nll": 0.023395614232867956,
      "perplexity": 1.0236714384408816,
      "avg_token_probability": 0.976907689661184,
      "sequence_length": 2
    },
    {
      "example_id": "bt_4353--105/105_2445487.txt#0_0",
      "dataset_type": "long",
      "response": "sheryl crow",
      "accuracy": 1.0,
      "g_nll": 0.08565488667227328,
      "average_nll": 0.02141372166806832,
      "perplexity": 1.0216446407393065,
      "avg_token_probability": 0.9793555445008504,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2257--150/150_49392.txt#0_0",
      "dataset_type": "long",
      "response": "december",
      "accuracy": 1.0,
      "g_nll": 0.5345972049981356,
      "average_nll": 0.2672986024990678,
      "perplexity": 1.3064304915031457,
      "avg_token_probability": 0.7890566604485184,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_20420--125/125_1922602.txt#0_1",
      "dataset_type": "long",
      "response": "wisconsin",
      "accuracy": 1.0,
      "g_nll": 0.043904806134378305,
      "average_nll": 0.014634935378126102,
      "perplexity": 1.0147425503835596,
      "avg_token_probability": 0.9855915543369177,
      "sequence_length": 3
    },
    {
      "example_id": "bt_4479--28/28_2447641.txt#0",
      "dataset_type": "long",
      "response": "the press gang",
      "accuracy": 0.0,
      "g_nll": 0.7831823292362969,
      "average_nll": 0.19579558230907423,
      "perplexity": 1.2162782511289039,
      "avg_token_probability": 0.8417072625043197,
      "sequence_length": 4
    },
    {
      "example_id": "qb_3147--114/114_354667.txt#0_2",
      "dataset_type": "long",
      "response": "the gondoliers",
      "accuracy": 1.0,
      "g_nll": 0.6037575870732326,
      "average_nll": 0.10062626451220542,
      "perplexity": 1.1058632641748274,
      "avg_token_probability": 0.9131726097940133,
      "sequence_length": 6
    },
    {
      "example_id": "qb_388--149/149_18617.txt#0_2",
      "dataset_type": "long",
      "response": "billie jean king",
      "accuracy": 0.0,
      "g_nll": 1.0202559281569847,
      "average_nll": 0.20405118563139696,
      "perplexity": 1.2263609239079352,
      "avg_token_probability": 0.8565656673359129,
      "sequence_length": 5
    },
    {
      "example_id": "qz_3777--128/128_191983.txt#0_2",
      "dataset_type": "long",
      "response": "vienna",
      "accuracy": 1.0,
      "g_nll": 0.4239519193797605,
      "average_nll": 0.14131730645992016,
      "perplexity": 1.1517900604344604,
      "avg_token_probability": 0.8800192585247416,
      "sequence_length": 3
    },
    {
      "example_id": "qw_1816--138/138_1086182.txt#0_2",
      "dataset_type": "long",
      "response": "paul gauguin",
      "accuracy": 1.0,
      "g_nll": 0.09084318886709752,
      "average_nll": 0.018168637773419503,
      "perplexity": 1.0183346916055391,
      "avg_token_probability": 0.9821878020396232,
      "sequence_length": 5
    },
    {
      "example_id": "qb_6828--25/25_2632281.txt#0_0",
      "dataset_type": "long",
      "response": "young",
      "accuracy": 0.0,
      "g_nll": 0.6703283786773682,
      "average_nll": 0.3351641893386841,
      "perplexity": 1.3981699309738034,
      "avg_token_probability": 0.7210547940049774,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24386--33/33_2007786.txt#0_2",
      "dataset_type": "long",
      "response": "squeeze",
      "accuracy": 1.0,
      "g_nll": 6.246371776796877e-05,
      "average_nll": 6.246371776796877e-05,
      "perplexity": 1.0000624656686665,
      "avg_token_probability": 0.9999375382330494,
      "sequence_length": 1
    },
    {
      "example_id": "qb_3702--159/159_370045.txt#0_0",
      "dataset_type": "long",
      "response": "smoking",
      "accuracy": 0.0,
      "g_nll": 2.1674200892448425,
      "average_nll": 1.0837100446224213,
      "perplexity": 2.955624734603856,
      "avg_token_probability": 0.5251677745104071,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_13345--191/191_1767828.txt#0_0",
      "dataset_type": "long",
      "response": "mir",
      "accuracy": 1.0,
      "g_nll": 0.06821347400546074,
      "average_nll": 0.03410673700273037,
      "perplexity": 1.0346950410817222,
      "avg_token_probability": 0.9664828055611335,
      "sequence_length": 2
    },
    {
      "example_id": "qw_12886--79/79_2977981.txt#0_1",
      "dataset_type": "long",
      "response": "lisieux",
      "accuracy": 1.0,
      "g_nll": 0.07949973083577788,
      "average_nll": 0.026499910278592626,
      "perplexity": 1.026854155130878,
      "avg_token_probability": 0.9742315503680476,
      "sequence_length": 3
    },
    {
      "example_id": "qf_2195--13/13_325145.txt#0_0",
      "dataset_type": "long",
      "response": "andre agassi",
      "accuracy": 1.0,
      "g_nll": 0.05537482908493985,
      "average_nll": 0.013843707271234962,
      "perplexity": 1.0139399751083662,
      "avg_token_probability": 0.9863223417278342,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5330--140/140_950316.txt#0_1",
      "dataset_type": "long",
      "response": "the red sea",
      "accuracy": 1.0,
      "g_nll": 1.521061995314085,
      "average_nll": 0.507020665104695,
      "perplexity": 1.6603371183690945,
      "avg_token_probability": 0.7361303356134602,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3467--85/85_645957.txt#0_1",
      "dataset_type": "long",
      "response": "willie nelson",
      "accuracy": 1.0,
      "g_nll": 0.3343161977057889,
      "average_nll": 0.06686323954115778,
      "perplexity": 1.0691492508336542,
      "avg_token_probability": 0.9388342128276299,
      "sequence_length": 5
    },
    {
      "example_id": "odql_3325--52/52_2810331.txt#0_2",
      "dataset_type": "long",
      "response": "Carlisle",
      "accuracy": 1.0,
      "g_nll": 0.42592672933096765,
      "average_nll": 0.14197557644365588,
      "perplexity": 1.1525484988600476,
      "avg_token_probability": 0.8809234236618101,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1076--114/114_1377217.txt#0_1",
      "dataset_type": "long",
      "response": "maine",
      "accuracy": 1.0,
      "g_nll": 0.06538867758354172,
      "average_nll": 0.021796225861180574,
      "perplexity": 1.0220354988459268,
      "avg_token_probability": 0.9786962318954581,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1706--93/93_2394404.txt#0_0",
      "dataset_type": "long",
      "response": "zip",
      "accuracy": 0.0,
      "g_nll": 2.9671870470046997,
      "average_nll": 1.4835935235023499,
      "perplexity": 4.408760232430639,
      "avg_token_probability": 0.5037783695062154,
      "sequence_length": 2
    },
    {
      "example_id": "tc_635--8/8_16499.txt#0_0",
      "dataset_type": "long",
      "response": "peru",
      "accuracy": 1.0,
      "g_nll": 0.12465861067175865,
      "average_nll": 0.062329305335879326,
      "perplexity": 1.0643127709007647,
      "avg_token_probability": 0.93986581886781,
      "sequence_length": 2
    },
    {
      "example_id": "qg_1526--104/104_341553.txt#0_2",
      "dataset_type": "long",
      "response": "mercury",
      "accuracy": 1.0,
      "g_nll": 0.035613634856417775,
      "average_nll": 0.017806817428208888,
      "perplexity": 1.0179663040449587,
      "avg_token_probability": 0.9825023756311388,
      "sequence_length": 2
    },
    {
      "example_id": "bb_6324--63/63_972094.txt#0_0",
      "dataset_type": "long",
      "response": "zambia",
      "accuracy": 1.0,
      "g_nll": 0.08735266739677172,
      "average_nll": 0.029117555798923906,
      "perplexity": 1.0295456164190426,
      "avg_token_probability": 0.971517381544308,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22542--154/154_1968503.txt#0_0",
      "dataset_type": "long",
      "response": "daewoo",
      "accuracy": 1.0,
      "g_nll": 0.42002440672376906,
      "average_nll": 0.10500610168094227,
      "perplexity": 1.110717387578145,
      "avg_token_probability": 0.9108246260382874,
      "sequence_length": 4
    },
    {
      "example_id": "odql_3105--120/120_2809792.txt#0_0",
      "dataset_type": "long",
      "response": "enid blyton",
      "accuracy": 0.0,
      "g_nll": 0.10843168198994135,
      "average_nll": 0.02168633639798827,
      "perplexity": 1.0219231940842632,
      "avg_token_probability": 0.9790907315685772,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_6987--15/15_2753573.txt#0_0",
      "dataset_type": "long",
      "response": "photography",
      "accuracy": 0.0,
      "g_nll": 0.027675965800881386,
      "average_nll": 0.013837982900440693,
      "perplexity": 1.013934170956598,
      "avg_token_probability": 0.9863361951587376,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8861--190/190_510847.txt#0_0",
      "dataset_type": "long",
      "response": "litai",
      "accuracy": 0.0,
      "g_nll": 1.0074345022439957,
      "average_nll": 0.33581150074799854,
      "perplexity": 1.3990752753105298,
      "avg_token_probability": 0.7481392552511531,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1783--27/27_359862.txt#0_1",
      "dataset_type": "long",
      "response": "heather stanning",
      "accuracy": 1.0,
      "g_nll": 0.3048586960542252,
      "average_nll": 0.06097173921084505,
      "perplexity": 1.0628688762495384,
      "avg_token_probability": 0.9464562376728779,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_8966--137/137_1670959.txt#0_2",
      "dataset_type": "long",
      "response": "mussolini",
      "accuracy": 1.0,
      "g_nll": 0.2806701092631556,
      "average_nll": 0.09355670308771853,
      "perplexity": 1.098072865698074,
      "avg_token_probability": 0.9140196535254438,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22266--109/109_693915.txt#0_1",
      "dataset_type": "long",
      "response": "carry on cleo",
      "accuracy": 1.0,
      "g_nll": 0.26201385410968214,
      "average_nll": 0.05240277082193643,
      "perplexity": 1.0538000969800467,
      "avg_token_probability": 0.9510904129475032,
      "sequence_length": 5
    },
    {
      "example_id": "qb_1502--134/134_2618044.txt#0_0",
      "dataset_type": "long",
      "response": "bats",
      "accuracy": 1.0,
      "g_nll": 0.0518085528165102,
      "average_nll": 0.0259042764082551,
      "perplexity": 1.0262427081335335,
      "avg_token_probability": 0.9744402695336484,
      "sequence_length": 2
    },
    {
      "example_id": "qw_4625--182/182_2700800.txt#0_0",
      "dataset_type": "long",
      "response": "aqueous humor",
      "accuracy": 1.0,
      "g_nll": 0.7663437845162662,
      "average_nll": 0.19158594612906654,
      "perplexity": 1.2111689239474868,
      "avg_token_probability": 0.8634258625223215,
      "sequence_length": 4
    },
    {
      "example_id": "bb_476--74/74_94569.txt#0_0",
      "dataset_type": "long",
      "response": "July 1969",
      "accuracy": 0.0,
      "g_nll": 5.594741641223095,
      "average_nll": 1.118948328244619,
      "perplexity": 3.0616326769840736,
      "avg_token_probability": 0.6184476284146256,
      "sequence_length": 5
    },
    {
      "example_id": "qz_2001--6/6_147605.txt#0_1",
      "dataset_type": "long",
      "response": "pablo neruda",
      "accuracy": 1.0,
      "g_nll": 0.05898252357292222,
      "average_nll": 0.011796504714584443,
      "perplexity": 1.0118663578804967,
      "avg_token_probability": 0.9883251261683895,
      "sequence_length": 5
    },
    {
      "example_id": "qb_5694--135/135_424629.txt#0_0",
      "dataset_type": "long",
      "response": "queen",
      "accuracy": 1.0,
      "g_nll": 0.265899864025414,
      "average_nll": 0.132949932012707,
      "perplexity": 1.1421928096041665,
      "avg_token_probability": 0.881614861627795,
      "sequence_length": 2
    },
    {
      "example_id": "odql_206--73/73_813141.txt#0_1",
      "dataset_type": "long",
      "response": "neptune",
      "accuracy": 1.0,
      "g_nll": 0.40309106302447617,
      "average_nll": 0.1343636876748254,
      "perplexity": 1.1438087331473585,
      "avg_token_probability": 0.8843180305297184,
      "sequence_length": 3
    },
    {
      "example_id": "bt_3256--60/60_2424928.txt#0_1",
      "dataset_type": "long",
      "response": "patent flour",
      "accuracy": 1.0,
      "g_nll": 0.5054150756914169,
      "average_nll": 0.16847169189713895,
      "perplexity": 1.183494723880279,
      "avg_token_probability": 0.8606259432398877,
      "sequence_length": 3
    },
    {
      "example_id": "qw_10365--192/192_1229944.txt#0_0",
      "dataset_type": "long",
      "response": "old trafford",
      "accuracy": 1.0,
      "g_nll": 0.8466367590162918,
      "average_nll": 0.21165918975407294,
      "perplexity": 1.2357266649959249,
      "avg_token_probability": 0.8466917517381192,
      "sequence_length": 4
    },
    {
      "example_id": "bb_455--108/108_838985.txt#0_2",
      "dataset_type": "long",
      "response": "amazon",
      "accuracy": 1.0,
      "g_nll": 0.31550224125385284,
      "average_nll": 0.15775112062692642,
      "perplexity": 1.1708747518679257,
      "avg_token_probability": 0.854883876088737,
      "sequence_length": 2
    },
    {
      "example_id": "odql_14628--194/194_851470.txt#0_0",
      "dataset_type": "long",
      "response": "amelia",
      "accuracy": 0.0,
      "g_nll": 0.4917307981631893,
      "average_nll": 0.16391026605439643,
      "perplexity": 1.1781085940169178,
      "avg_token_probability": 0.8690597509873398,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23316--131/131_1618418.txt#0_0",
      "dataset_type": "long",
      "response": "vienna",
      "accuracy": 1.0,
      "g_nll": 0.05000141089112731,
      "average_nll": 0.0166671369637091,
      "perplexity": 1.0168068085873834,
      "avg_token_probability": 0.983616836555651,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5580--182/182_233604.txt#0_1",
      "dataset_type": "long",
      "response": "Nero",
      "accuracy": 0.0,
      "g_nll": 1.0165898352861404,
      "average_nll": 0.5082949176430702,
      "perplexity": 1.662454155690369,
      "avg_token_probability": 0.6677257293812213,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24100--95/95_438302.txt#0_0",
      "dataset_type": "long",
      "response": "friends",
      "accuracy": 0.0,
      "g_nll": 0.10162430629134178,
      "average_nll": 0.05081215314567089,
      "perplexity": 1.0521252363031643,
      "avg_token_probability": 0.9505533973783269,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7625--62/62_2634404.txt#0_1",
      "dataset_type": "long",
      "response": "vietnam",
      "accuracy": 1.0,
      "g_nll": 0.08009110504644923,
      "average_nll": 0.026697035015483078,
      "perplexity": 1.0270565934381766,
      "avg_token_probability": 0.9742965217716156,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12623--90/90_717718.txt#0_2",
      "dataset_type": "long",
      "response": "utah",
      "accuracy": 1.0,
      "g_nll": 0.03951910906471312,
      "average_nll": 0.013173036354904374,
      "perplexity": 1.0132601830400216,
      "avg_token_probability": 0.9870355426092655,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_12132--72/72_1740634.txt#0_1",
      "dataset_type": "long",
      "response": "saints",
      "accuracy": 0.0,
      "g_nll": 0.8188734352588654,
      "average_nll": 0.4094367176294327,
      "perplexity": 1.505969260220834,
      "avg_token_probability": 0.7117362561967439,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8699--41/41_310048.txt#0_1",
      "dataset_type": "long",
      "response": "red",
      "accuracy": 1.0,
      "g_nll": 0.002057103905826807,
      "average_nll": 0.002057103905826807,
      "perplexity": 1.002059221195646,
      "avg_token_probability": 0.9979450104823256,
      "sequence_length": 1
    },
    {
      "example_id": "odql_2538--189/189_425837.txt#0_1",
      "dataset_type": "long",
      "response": "volleyball",
      "accuracy": 1.0,
      "g_nll": 0.19133394211530685,
      "average_nll": 0.09566697105765343,
      "perplexity": 1.1003925404019883,
      "avg_token_probability": 0.9118279399977325,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3059--26/26_1399947.txt#0_1",
      "dataset_type": "long",
      "response": "judas",
      "accuracy": 1.0,
      "g_nll": 0.8313804863992118,
      "average_nll": 0.27712682879973727,
      "perplexity": 1.3193336899326542,
      "avg_token_probability": 0.7738116428365247,
      "sequence_length": 3
    },
    {
      "example_id": "jp_4006--145/145_1449904.txt#0_2",
      "dataset_type": "long",
      "response": "ORSON WELLES",
      "accuracy": 1.0,
      "g_nll": 6.451133285914693,
      "average_nll": 1.0751888809857821,
      "perplexity": 2.9305463727496326,
      "avg_token_probability": 0.8300659063383035,
      "sequence_length": 6
    },
    {
      "example_id": "odql_2197--13/13_665800.txt#0_2",
      "dataset_type": "long",
      "response": "chile",
      "accuracy": 1.0,
      "g_nll": 0.043726572766559,
      "average_nll": 0.014575524255519667,
      "perplexity": 1.0146822651803085,
      "avg_token_probability": 0.9856466232036132,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3075--57/57_635879.txt#0_2",
      "dataset_type": "long",
      "response": "michaela tabb",
      "accuracy": 1.0,
      "g_nll": 0.24175890953529233,
      "average_nll": 0.048351781907058466,
      "perplexity": 1.0495397995037452,
      "avg_token_probability": 0.9543719479982918,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_5662--54/54_415167.txt#0_2",
      "dataset_type": "long",
      "response": "pansy",
      "accuracy": 0.0,
      "g_nll": 10.761712957173586,
      "average_nll": 3.5872376523911953,
      "perplexity": 36.13412292950295,
      "avg_token_probability": 0.5105028798867767,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_1971--27/27_3001523.txt#0_2",
      "dataset_type": "long",
      "response": "sir cloudesley shovell",
      "accuracy": 1.0,
      "g_nll": 1.0058371604691274,
      "average_nll": 0.12572964505864093,
      "perplexity": 1.1339755509435583,
      "avg_token_probability": 0.9151866876384858,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_3203--Hell's_Kitchen_(UK_TV_series).txt#0_0",
      "dataset_type": "long",
      "response": "barry mcguigan",
      "accuracy": 0.0,
      "g_nll": 0.9673013553041301,
      "average_nll": 0.16121689255068836,
      "perplexity": 1.174939776864998,
      "avg_token_probability": 0.8929956047072397,
      "sequence_length": 6
    },
    {
      "example_id": "bt_264--110/110_1711.txt#0_0",
      "dataset_type": "long",
      "response": "amelia earhart",
      "accuracy": 1.0,
      "g_nll": 5.711648038857675,
      "average_nll": 1.142329607771535,
      "perplexity": 3.1340610002610183,
      "avg_token_probability": 0.7904583038477067,
      "sequence_length": 5
    },
    {
      "example_id": "jp_4375--83/83_2736108.txt#0_0",
      "dataset_type": "long",
      "response": "yosemite",
      "accuracy": 1.0,
      "g_nll": 0.20960957804345526,
      "average_nll": 0.06986985934781842,
      "perplexity": 1.0723686134219592,
      "avg_token_probability": 0.9358591221094846,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1864--103/103_769441.txt#0_0",
      "dataset_type": "long",
      "response": "ringway",
      "accuracy": 1.0,
      "g_nll": 0.137376429134747,
      "average_nll": 0.04579214304491567,
      "perplexity": 1.0468567918757323,
      "avg_token_probability": 0.9568613620710559,
      "sequence_length": 3
    },
    {
      "example_id": "qw_3063--131/131_82670.txt#0_0",
      "dataset_type": "long",
      "response": "barnum & bailey",
      "accuracy": 1.0,
      "g_nll": 0.991718062585278,
      "average_nll": 0.165286343764213,
      "perplexity": 1.179730878931948,
      "avg_token_probability": 0.8705994300357975,
      "sequence_length": 6
    },
    {
      "example_id": "odql_2542--11/11_2808626.txt#0_1",
      "dataset_type": "long",
      "response": "thailand",
      "accuracy": 1.0,
      "g_nll": 0.06185901554636075,
      "average_nll": 0.02061967184878692,
      "perplexity": 1.0208337259929194,
      "avg_token_probability": 0.9797154916828091,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_4014--107/107_660870.txt#0_2",
      "dataset_type": "long",
      "response": "elen degeneres",
      "accuracy": 1.0,
      "g_nll": 3.256158842559671,
      "average_nll": 0.5426931404266119,
      "perplexity": 1.7206345383315615,
      "avg_token_probability": 0.7140793011457486,
      "sequence_length": 6
    },
    {
      "example_id": "qz_3311--68/68_97141.txt#0_1",
      "dataset_type": "long",
      "response": "one",
      "accuracy": 0.0,
      "g_nll": 0.881121538579464,
      "average_nll": 0.440560769289732,
      "perplexity": 1.553578173214924,
      "avg_token_probability": 0.6867051182892594,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_21423--144/144_2788272.txt#0_0",
      "dataset_type": "long",
      "response": "byward tower",
      "accuracy": 0.0,
      "g_nll": 0.17643362935632467,
      "average_nll": 0.04410840733908117,
      "perplexity": 1.0450956447855413,
      "avg_token_probability": 0.9579714358827962,
      "sequence_length": 4
    },
    {
      "example_id": "wh_1190--127/127_411355.txt#0_0",
      "dataset_type": "long",
      "response": "peter stuyvesant",
      "accuracy": 1.0,
      "g_nll": 0.13345160139669332,
      "average_nll": 0.022241933566115552,
      "perplexity": 1.0224911294739776,
      "avg_token_probability": 0.9783307677131657,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_15186--22/22_2773455.txt#0_0",
      "dataset_type": "long",
      "response": "the spider",
      "accuracy": 0.0,
      "g_nll": 1.4433102793991566,
      "average_nll": 0.48110342646638554,
      "perplexity": 1.6178586056475481,
      "avg_token_probability": 0.6592690963732726,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8229--63/63_493737.txt#0_1",
      "dataset_type": "long",
      "response": "the rescuers",
      "accuracy": 1.0,
      "g_nll": 0.3766615316271782,
      "average_nll": 0.09416538290679455,
      "perplexity": 1.0987414439457008,
      "avg_token_probability": 0.9142326393267883,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_8614--100/100_1662867.txt#0_5",
      "dataset_type": "long",
      "response": "mtzuma",
      "accuracy": 0.0,
      "g_nll": 6.5889032781124115,
      "average_nll": 1.6472258195281029,
      "perplexity": 5.1925547435697315,
      "avg_token_probability": 0.5818236612905674,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_22477--53/53_318005.txt#0_2",
      "dataset_type": "long",
      "response": "27, 27 and 27",
      "accuracy": 0.0,
      "g_nll": 1.8341155536763836,
      "average_nll": 0.20379061707515372,
      "perplexity": 1.2260414144414273,
      "avg_token_probability": 0.8543554112312202,
      "sequence_length": 9
    },
    {
      "example_id": "bb_2276--42/42_883302.txt#0_1",
      "dataset_type": "long",
      "response": "scotland",
      "accuracy": 1.0,
      "g_nll": 0.04840199905447662,
      "average_nll": 0.01613399968482554,
      "perplexity": 1.0162648554528493,
      "avg_token_probability": 0.9842053011530334,
      "sequence_length": 3
    },
    {
      "example_id": "tc_1938--155/155_57137.txt#0_1",
      "dataset_type": "long",
      "response": "south africa",
      "accuracy": 1.0,
      "g_nll": 0.08472427679225802,
      "average_nll": 0.02824142559741934,
      "perplexity": 1.0286439954370314,
      "avg_token_probability": 0.9724648219212518,
      "sequence_length": 3
    },
    {
      "example_id": "odql_1853--0/0_2112627.txt#0_0",
      "dataset_type": "long",
      "response": "hoag",
      "accuracy": 0.0,
      "g_nll": 1.3984509855508804,
      "average_nll": 0.46615032851696014,
      "perplexity": 1.5938465819329102,
      "avg_token_probability": 0.7069999704230613,
      "sequence_length": 3
    },
    {
      "example_id": "qg_3271--66/66_2533261.txt#0_2",
      "dataset_type": "long",
      "response": "kentucky derby",
      "accuracy": 1.0,
      "g_nll": 0.21236223936057286,
      "average_nll": 0.053090559840143214,
      "perplexity": 1.054525138423324,
      "avg_token_probability": 0.9508750383882365,
      "sequence_length": 4
    },
    {
      "example_id": "qz_5877--148/148_240409.txt#0_0",
      "dataset_type": "long",
      "response": "tartar",
      "accuracy": 1.0,
      "g_nll": 0.6655126697814921,
      "average_nll": 0.22183755659383073,
      "perplexity": 1.2483685721494893,
      "avg_token_probability": 0.8343409514933847,
      "sequence_length": 3
    },
    {
      "example_id": "odql_14853--114/114_1247260.txt#0_1",
      "dataset_type": "long",
      "response": "1994",
      "accuracy": 0.0,
      "g_nll": 1.9526892152498476,
      "average_nll": 0.4881723038124619,
      "perplexity": 1.629335566517505,
      "avg_token_probability": 0.7535660086178748,
      "sequence_length": 4
    },
    {
      "example_id": "wh_156--I'm_a_Celebrity...Get_Me_Out_of_Here!_(UK_series_5).txt#0_0",
      "dataset_type": "long",
      "response": "sheree murphy",
      "accuracy": 0.0,
      "g_nll": 1.676644748013132,
      "average_nll": 0.3353289496026264,
      "perplexity": 1.3984003127990308,
      "avg_token_probability": 0.8268526176159636,
      "sequence_length": 5
    },
    {
      "example_id": "odql_4802--148/148_1652317.txt#0_1",
      "dataset_type": "long",
      "response": "acne",
      "accuracy": 0.0,
      "g_nll": 0.2864486873149872,
      "average_nll": 0.2864486873149872,
      "perplexity": 1.3316898335600509,
      "avg_token_probability": 0.7509256095517877,
      "sequence_length": 1
    },
    {
      "example_id": "qf_364--185/185_2457360.txt#0_0",
      "dataset_type": "long",
      "response": "lautere",
      "accuracy": 0.0,
      "g_nll": 8.403575792908669,
      "average_nll": 2.8011919309695563,
      "perplexity": 16.46425934095968,
      "avg_token_probability": 0.39218303015912737,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8247--176/176_1183968.txt#0_0",
      "dataset_type": "long",
      "response": "baseball",
      "accuracy": 1.0,
      "g_nll": 0.04829917475581169,
      "average_nll": 0.024149587377905846,
      "perplexity": 1.0244435502540172,
      "avg_token_probability": 0.9761396960204655,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_25528--123/123_2033010.txt#0_0",
      "dataset_type": "long",
      "response": "susie dent",
      "accuracy": 1.0,
      "g_nll": 0.015899995119980304,
      "average_nll": 0.007949997559990152,
      "perplexity": 1.0079816827005321,
      "avg_token_probability": 0.9921127050593965,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2553--36/36_81264.txt#0_1",
      "dataset_type": "long",
      "response": "e pluribus unum",
      "accuracy": 1.0,
      "g_nll": 0.03265588479280268,
      "average_nll": 0.005442647465467114,
      "perplexity": 1.0054574855785094,
      "avg_token_probability": 0.9946094350340849,
      "sequence_length": 6
    },
    {
      "example_id": "bt_2852--75/75_2417406.txt#0_1",
      "dataset_type": "long",
      "response": "claire goose",
      "accuracy": 1.0,
      "g_nll": 0.2448139053885825,
      "average_nll": 0.061203476347145624,
      "perplexity": 1.0631152109805335,
      "avg_token_probability": 0.9432724459706667,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_13456--144/144_1578174.txt#0_1",
      "dataset_type": "long",
      "response": "doncaster rovers",
      "accuracy": 1.0,
      "g_nll": 0.4953578307031421,
      "average_nll": 0.09907156614062843,
      "perplexity": 1.1041453161503618,
      "avg_token_probability": 0.9173040462361108,
      "sequence_length": 5
    },
    {
      "example_id": "qw_14570--139/139_1315393.txt#0_0",
      "dataset_type": "long",
      "response": "tarantula",
      "accuracy": 1.0,
      "g_nll": 5.703375731537562,
      "average_nll": 1.4258439328843906,
      "perplexity": 4.161368276795142,
      "avg_token_probability": 0.7472783176616511,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_10437--135/135_1466322.txt#0_0",
      "dataset_type": "long",
      "response": "adam smith",
      "accuracy": 1.0,
      "g_nll": 0.34773299616063014,
      "average_nll": 0.11591099872021005,
      "perplexity": 1.1228959285110571,
      "avg_token_probability": 0.8987126048200524,
      "sequence_length": 3
    },
    {
      "example_id": "qz_2980--37/37_55261.txt#0_2",
      "dataset_type": "long",
      "response": "persistence of memory",
      "accuracy": 1.0,
      "g_nll": 0.06321597234637011,
      "average_nll": 0.015803993086592527,
      "perplexity": 1.0159295366767451,
      "avg_token_probability": 0.9845368794656391,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1688--101/101_312407.txt#0_0",
      "dataset_type": "long",
      "response": "valentino garavani",
      "accuracy": 1.0,
      "g_nll": 0.34509867165155583,
      "average_nll": 0.04929981023593655,
      "perplexity": 1.0505352647560326,
      "avg_token_probability": 0.9559614602217225,
      "sequence_length": 7
    },
    {
      "example_id": "qw_9748--135/135_39202.txt#0_2",
      "dataset_type": "long",
      "response": "taekwondo",
      "accuracy": 1.0,
      "g_nll": 0.027695174320285787,
      "average_nll": 0.005539034864057157,
      "perplexity": 1.005554403680704,
      "avg_token_probability": 0.9945066756161367,
      "sequence_length": 5
    },
    {
      "example_id": "jp_2683--186/186_1418025.txt#0_2",
      "dataset_type": "long",
      "response": "Kix",
      "accuracy": 1.0,
      "g_nll": 4.860646348446608,
      "average_nll": 1.6202154494822025,
      "perplexity": 5.0541791195429004,
      "avg_token_probability": 0.6370298256636997,
      "sequence_length": 3
    },
    {
      "example_id": "odql_4564--45/45_1774711.txt#0_0",
      "dataset_type": "long",
      "response": "dogs",
      "accuracy": 1.0,
      "g_nll": 0.03202856332063675,
      "average_nll": 0.03202856332063675,
      "perplexity": 1.032546997854886,
      "avg_token_probability": 0.9684789187102356,
      "sequence_length": 1
    },
    {
      "example_id": "odql_8365--188/188_262755.txt#0_1",
      "dataset_type": "long",
      "response": "harvard",
      "accuracy": 1.0,
      "g_nll": 0.022942115379919414,
      "average_nll": 0.007647371793306472,
      "perplexity": 1.007676687623014,
      "avg_token_probability": 0.9923994498039542,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1878--25/25_1397691.txt#0_0",
      "dataset_type": "long",
      "response": "Shaft",
      "accuracy": 1.0,
      "g_nll": 2.3996877148747444,
      "average_nll": 1.1998438574373722,
      "perplexity": 3.3195985516429563,
      "avg_token_probability": 0.5145025410037363,
      "sequence_length": 2
    },
    {
      "example_id": "qz_3023--106/106_173524.txt#0_0",
      "dataset_type": "long",
      "response": "prince edward of wales and susan tevez",
      "accuracy": 0.0,
      "g_nll": 13.479802661044232,
      "average_nll": 1.1233168884203526,
      "perplexity": 3.075036860743682,
      "avg_token_probability": 0.6063950605774174,
      "sequence_length": 12
    },
    {
      "example_id": "tc_783--72/72_2593823.txt#0_1",
      "dataset_type": "long",
      "response": "cartoons",
      "accuracy": 1.0,
      "g_nll": 0.18807683140039444,
      "average_nll": 0.09403841570019722,
      "perplexity": 1.0986019486696406,
      "avg_token_probability": 0.9110812649652347,
      "sequence_length": 2
    },
    {
      "example_id": "tc_2359--127/127_71063.txt#0_0",
      "dataset_type": "long",
      "response": "john Buchan",
      "accuracy": 1.0,
      "g_nll": 6.557144433072608,
      "average_nll": 1.639286108268152,
      "perplexity": 5.151490592674994,
      "avg_token_probability": 0.736273864610566,
      "sequence_length": 4
    },
    {
      "example_id": "qb_574--35/35_280986.txt#0_1",
      "dataset_type": "long",
      "response": "tokyo",
      "accuracy": 1.0,
      "g_nll": 0.7333873589441282,
      "average_nll": 0.18334683973603205,
      "perplexity": 1.2012309704691715,
      "avg_token_probability": 0.8628731886425286,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1631--122/122_596455.txt#0_0",
      "dataset_type": "long",
      "response": "lost in translation",
      "accuracy": 1.0,
      "g_nll": 0.07339916093769716,
      "average_nll": 0.01834979023442429,
      "perplexity": 1.0185191821510038,
      "avg_token_probability": 0.9821304838175859,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_634--98/98_901854.txt#0_0",
      "dataset_type": "long",
      "response": "bitter almond",
      "accuracy": 1.0,
      "g_nll": 0.07796684559434652,
      "average_nll": 0.02598894853144884,
      "perplexity": 1.026329605961403,
      "avg_token_probability": 0.974421392172141,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_25402--57/57_2030191.txt#0_2",
      "dataset_type": "long",
      "response": "patrick armstrong",
      "accuracy": 0.0,
      "g_nll": 0.117066233571677,
      "average_nll": 0.023413246714335402,
      "perplexity": 1.0236894884676815,
      "avg_token_probability": 0.9773402750298523,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_668--20/20_1363097.txt#0_1",
      "dataset_type": "long",
      "response": "demi moore",
      "accuracy": 1.0,
      "g_nll": 1.4502009409478092,
      "average_nll": 0.4834003136492697,
      "perplexity": 1.621578915272726,
      "avg_token_probability": 0.7434400695246532,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1572--159/159_685600.txt#0_0",
      "dataset_type": "long",
      "response": "cheers",
      "accuracy": 1.0,
      "g_nll": 0.02753967046737671,
      "average_nll": 0.02753967046737671,
      "perplexity": 1.0279223924601548,
      "avg_token_probability": 0.9728360889256168,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_25914--50_(number).txt#0_0",
      "dataset_type": "long",
      "response": "hydrogen",
      "accuracy": 0.0,
      "g_nll": 2.1168890427798033,
      "average_nll": 1.0584445213899016,
      "perplexity": 2.8818847909306555,
      "avg_token_probability": 0.5475257964670125,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_9379--37/37_1679055.txt#0_1",
      "dataset_type": "long",
      "response": "honda",
      "accuracy": 1.0,
      "g_nll": 0.08934775460511446,
      "average_nll": 0.04467387730255723,
      "perplexity": 1.0456867821010736,
      "avg_token_probability": 0.9567888284521462,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7372--177/177_98429.txt#0_2",
      "dataset_type": "long",
      "response": "6",
      "accuracy": 0.0,
      "g_nll": 3.1473962208256125,
      "average_nll": 1.0491320736085374,
      "perplexity": 2.8551719632577743,
      "avg_token_probability": 0.6688440695940715,
      "sequence_length": 3
    },
    {
      "example_id": "bt_824--124/124_995752.txt#0_1",
      "dataset_type": "long",
      "response": "the drifters",
      "accuracy": 1.0,
      "g_nll": 3.2274159491061027,
      "average_nll": 0.8068539872765257,
      "perplexity": 2.2408471521105344,
      "avg_token_probability": 0.629283128684595,
      "sequence_length": 4
    },
    {
      "example_id": "wh_495--37/37_2926588.txt#0_1",
      "dataset_type": "long",
      "response": "tommi roe",
      "accuracy": 1.0,
      "g_nll": 5.419845891650766,
      "average_nll": 0.903307648608461,
      "perplexity": 2.467752083503945,
      "avg_token_probability": 0.6651856908000089,
      "sequence_length": 6
    },
    {
      "example_id": "tb_1170--172/172_2063410.txt#0_0",
      "dataset_type": "long",
      "response": "crete",
      "accuracy": 1.0,
      "g_nll": 0.05921931269404013,
      "average_nll": 0.019739770898013376,
      "perplexity": 1.0199358884888008,
      "avg_token_probability": 0.9805570388031294,
      "sequence_length": 3
    },
    {
      "example_id": "qw_1393--125/125_2694259.txt#0_0",
      "dataset_type": "long",
      "response": "elizabeth taylor",
      "accuracy": 0.0,
      "g_nll": 1.1773588461801126,
      "average_nll": 0.2354717692360225,
      "perplexity": 1.2655056545614702,
      "avg_token_probability": 0.8382691178304388,
      "sequence_length": 5
    },
    {
      "example_id": "qb_8996--151/151_2637831.txt#0_0",
      "dataset_type": "long",
      "response": "noel edmonds",
      "accuracy": 1.0,
      "g_nll": 1.6019535801228812,
      "average_nll": 0.32039071602457625,
      "perplexity": 1.3776659353505454,
      "avg_token_probability": 0.8351051263518917,
      "sequence_length": 5
    },
    {
      "example_id": "qw_2309--75/75_263961.txt#0_1",
      "dataset_type": "long",
      "response": "unesco",
      "accuracy": 1.0,
      "g_nll": 0.8343122044170741,
      "average_nll": 0.2781040681390247,
      "perplexity": 1.3206236249010448,
      "avg_token_probability": 0.8060951861367419,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_6084--154/154_2657783.txt#0_0",
      "dataset_type": "long",
      "response": "stanislas wawrinka",
      "accuracy": 1.0,
      "g_nll": 0.050047913167304614,
      "average_nll": 0.006255989145913077,
      "perplexity": 1.0062755987171041,
      "avg_token_probability": 0.9938000891951779,
      "sequence_length": 8
    },
    {
      "example_id": "qg_3077--116/116_1355464.txt#0_0",
      "dataset_type": "long",
      "response": "chile",
      "accuracy": 1.0,
      "g_nll": 0.06110238283770286,
      "average_nll": 0.020367460945900955,
      "perplexity": 1.0205762930622508,
      "avg_token_probability": 0.9800051143028193,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23639--86/86_1607077.txt#0_0",
      "dataset_type": "long",
      "response": "bear grylls",
      "accuracy": 1.0,
      "g_nll": 0.6121024026579107,
      "average_nll": 0.10201706710965179,
      "perplexity": 1.1074023717243557,
      "avg_token_probability": 0.9183327651916598,
      "sequence_length": 6
    },
    {
      "example_id": "jp_1655--92/92_1391976.txt#0_0",
      "dataset_type": "long",
      "response": "space jam",
      "accuracy": 1.0,
      "g_nll": 5.793182737310417,
      "average_nll": 1.9310609124368057,
      "perplexity": 6.89682328724471,
      "avg_token_probability": 0.6669621516144347,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1024--151/151_108800.txt#0_0",
      "dataset_type": "long",
      "response": "swindon",
      "accuracy": 1.0,
      "g_nll": 0.5732348859055492,
      "average_nll": 0.1433087214763873,
      "perplexity": 1.154086037819779,
      "avg_token_probability": 0.879236805601493,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2250--43/43_67037.txt#0_0",
      "dataset_type": "long",
      "response": "earthquake",
      "accuracy": 1.0,
      "g_nll": 0.1549012940376997,
      "average_nll": 0.07745064701884985,
      "perplexity": 1.0805289038199846,
      "avg_token_probability": 0.9269858201228904,
      "sequence_length": 2
    },
    {
      "example_id": "qw_3191--24/24_1085500.txt#0_2",
      "dataset_type": "long",
      "response": "jack brabham",
      "accuracy": 1.0,
      "g_nll": 0.08675636260869624,
      "average_nll": 0.017351272521739246,
      "perplexity": 1.017502680288931,
      "avg_token_probability": 0.9830375068960986,
      "sequence_length": 5
    },
    {
      "example_id": "jp_2475--173/173_133780.txt#0_2",
      "dataset_type": "long",
      "response": "alaska",
      "accuracy": 0.0,
      "g_nll": 0.24024715111590922,
      "average_nll": 0.08008238370530307,
      "perplexity": 1.0833763165537766,
      "avg_token_probability": 0.9246813531395408,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12438--52/52_1779413.txt#0_0",
      "dataset_type": "long",
      "response": "gary puckett",
      "accuracy": 1.0,
      "g_nll": 0.555563166535876,
      "average_nll": 0.138890791633969,
      "perplexity": 1.148998612890523,
      "avg_token_probability": 0.889034177700025,
      "sequence_length": 4
    },
    {
      "example_id": "qf_530--164/164_2459775.txt#0_0",
      "dataset_type": "long",
      "response": "tetanus",
      "accuracy": 1.0,
      "g_nll": 0.03793387714540586,
      "average_nll": 0.012644625715135286,
      "perplexity": 1.0127249070136775,
      "avg_token_probability": 0.9875534410652081,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1386--197/197_2415925.txt#0_2",
      "dataset_type": "long",
      "response": "jupiter",
      "accuracy": 1.0,
      "g_nll": 0.06432466175465379,
      "average_nll": 0.02144155391821793,
      "perplexity": 1.0216730758042158,
      "avg_token_probability": 0.9789041044821957,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_16856--9/9_1846523.txt#0_0",
      "dataset_type": "long",
      "response": "bangladesh",
      "accuracy": 1.0,
      "g_nll": 0.0488726555086032,
      "average_nll": 0.016290885169534402,
      "perplexity": 1.0164243051646353,
      "avg_token_probability": 0.9839062824153576,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_10025--172/172_3020164.txt#0_1",
      "dataset_type": "long",
      "response": "nutbush",
      "accuracy": 1.0,
      "g_nll": 0.06102390229349908,
      "average_nll": 0.01525597557337477,
      "perplexity": 1.0153729420241895,
      "avg_token_probability": 0.9851709499173542,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_21320--53/53_2087897.txt#0_0",
      "dataset_type": "long",
      "response": "gorky",
      "accuracy": 1.0,
      "g_nll": 0.45293411292050223,
      "average_nll": 0.1509780376401674,
      "perplexity": 1.1629711162112122,
      "avg_token_probability": 0.8784970751139878,
      "sequence_length": 3
    },
    {
      "example_id": "odql_7980--187/187_83028.txt#0_0",
      "dataset_type": "long",
      "response": "magma",
      "accuracy": 1.0,
      "g_nll": 0.03271584026515484,
      "average_nll": 0.01635792013257742,
      "perplexity": 1.016492443414165,
      "avg_token_probability": 0.9838242761949687,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_16964--98/98_1848963.txt#0_0",
      "dataset_type": "long",
      "response": "helen gurley brown",
      "accuracy": 1.0,
      "g_nll": 0.2444563109891078,
      "average_nll": 0.03492233014130112,
      "perplexity": 1.0355392754868376,
      "avg_token_probability": 0.967117511063382,
      "sequence_length": 7
    },
    {
      "example_id": "odql_6538--139/139_826850.txt#0_0",
      "dataset_type": "long",
      "response": "king william iv",
      "accuracy": 1.0,
      "g_nll": 1.2909686835409957,
      "average_nll": 0.2581937367081991,
      "perplexity": 1.2945896038586209,
      "avg_token_probability": 0.8468196329806957,
      "sequence_length": 5
    },
    {
      "example_id": "odql_5186--146/146_2175002.txt#0_1",
      "dataset_type": "long",
      "response": "m11",
      "accuracy": 1.0,
      "g_nll": 0.5914398920867825,
      "average_nll": 0.19714663069559415,
      "perplexity": 1.2179226124533826,
      "avg_token_probability": 0.8433935076034876,
      "sequence_length": 3
    },
    {
      "example_id": "bb_2691--72/72_892989.txt#0_2",
      "dataset_type": "long",
      "response": "obi",
      "accuracy": 1.0,
      "g_nll": 0.02079034770781618,
      "average_nll": 0.006930115902605394,
      "perplexity": 1.0069541847235992,
      "avg_token_probability": 0.9931336572063506,
      "sequence_length": 3
    },
    {
      "example_id": "qg_1199--189/189_3112721.txt#0_0",
      "dataset_type": "long",
      "response": "arizona diamondbacks",
      "accuracy": 1.0,
      "g_nll": 0.3972095090084622,
      "average_nll": 0.07944190180169244,
      "perplexity": 1.0826826557905194,
      "avg_token_probability": 0.931167619634922,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_24050--117/117_2000989.txt#0_0",
      "dataset_type": "long",
      "response": "leonard nimoy",
      "accuracy": 0.0,
      "g_nll": 0.280629442596819,
      "average_nll": 0.0467715737661365,
      "perplexity": 1.0478826178591505,
      "avg_token_probability": 0.9576805531517035,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_20082--77/77_957747.txt#0_1",
      "dataset_type": "long",
      "response": "volkswagen",
      "accuracy": 0.0,
      "g_nll": 0.40440035180654377,
      "average_nll": 0.1348001172688479,
      "perplexity": 1.1443080340753309,
      "avg_token_probability": 0.888766743550062,
      "sequence_length": 3
    },
    {
      "example_id": "odql_6144--10/10_2193236.txt#0_1",
      "dataset_type": "long",
      "response": "richard seddon",
      "accuracy": 1.0,
      "g_nll": 0.08541167778980707,
      "average_nll": 0.017082335557961414,
      "perplexity": 1.0172290730006641,
      "avg_token_probability": 0.9833230089650964,
      "sequence_length": 5
    },
    {
      "example_id": "qz_2845--76/76_169229.txt#0_0",
      "dataset_type": "long",
      "response": "brazil",
      "accuracy": 0.0,
      "g_nll": 1.1573396474123,
      "average_nll": 0.57866982370615,
      "perplexity": 1.783664264149714,
      "avg_token_probability": 0.6138848488350523,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_18636--83/83_1734.txt#0_0",
      "dataset_type": "long",
      "response": "1919",
      "accuracy": 1.0,
      "g_nll": 0.03532096173148602,
      "average_nll": 0.008830240432871506,
      "perplexity": 1.0088693420133008,
      "avg_token_probability": 0.991309708798696,
      "sequence_length": 4
    },
    {
      "example_id": "qw_10365--192/192_1229944.txt#0_2",
      "dataset_type": "long",
      "response": "old Trafford",
      "accuracy": 1.0,
      "g_nll": 1.5897495900280774,
      "average_nll": 0.5299165300093591,
      "perplexity": 1.6987905046729201,
      "avg_token_probability": 0.7182576381741042,
      "sequence_length": 3
    },
    {
      "example_id": "qb_2774--81/81_303935.txt#0_0",
      "dataset_type": "long",
      "response": "tiger",
      "accuracy": 0.0,
      "g_nll": 2.156238168478012,
      "average_nll": 1.078119084239006,
      "perplexity": 2.939146062531132,
      "avg_token_probability": 0.4205280657839511,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2761--66/66_24883.txt#0_1",
      "dataset_type": "long",
      "response": "cabaret",
      "accuracy": 1.0,
      "g_nll": 0.001528849097667262,
      "average_nll": 0.000764424548833631,
      "perplexity": 1.0007647167957412,
      "avg_token_probability": 0.9992360568511587,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_9852--85/85_243465.txt#0_0",
      "dataset_type": "long",
      "response": "leicestershire",
      "accuracy": 1.0,
      "g_nll": 0.3534946429103911,
      "average_nll": 0.07069892858207823,
      "perplexity": 1.073258049898396,
      "avg_token_probability": 0.9349477408430615,
      "sequence_length": 5
    },
    {
      "example_id": "qb_3427--81/81_362513.txt#0_3",
      "dataset_type": "long",
      "response": "Wolfgang Amadeus Mozart",
      "accuracy": 1.0,
      "g_nll": 0.9793685715376341,
      "average_nll": 0.16322809525627235,
      "perplexity": 1.177305196795278,
      "avg_token_probability": 0.8754403340248329,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_1674--197/197_3000842.txt#0_2",
      "dataset_type": "long",
      "response": "fiji",
      "accuracy": 1.0,
      "g_nll": 0.7130692622158676,
      "average_nll": 0.2376897540719559,
      "perplexity": 1.2683156420158894,
      "avg_token_probability": 0.8232724015912591,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_4253--27/27_289038.txt#0_1",
      "dataset_type": "long",
      "response": "calcutta cup",
      "accuracy": 1.0,
      "g_nll": 0.6471343980738311,
      "average_nll": 0.16178359951845778,
      "perplexity": 1.1756058121288442,
      "avg_token_probability": 0.8806690251587714,
      "sequence_length": 4
    },
    {
      "example_id": "odql_6028--County_Durham.txt#0_0",
      "dataset_type": "long",
      "response": "tyne and wear",
      "accuracy": 0.0,
      "g_nll": 2.363217998058417,
      "average_nll": 0.4726435996116834,
      "perplexity": 1.604229533050016,
      "avg_token_probability": 0.8109298726952721,
      "sequence_length": 5
    },
    {
      "example_id": "qw_3390--174/174_1038938.txt#0_0",
      "dataset_type": "long",
      "response": "winnipeg",
      "accuracy": 1.0,
      "g_nll": 0.27897908599697985,
      "average_nll": 0.06974477149924496,
      "perplexity": 1.0722344815285385,
      "avg_token_probability": 0.9374531893908775,
      "sequence_length": 4
    },
    {
      "example_id": "wh_4331--119/119_827530.txt#0_1",
      "dataset_type": "long",
      "response": "gironde",
      "accuracy": 1.0,
      "g_nll": 0.11775997505128544,
      "average_nll": 0.02943999376282136,
      "perplexity": 1.0298776345360865,
      "avg_token_probability": 0.9714188107033341,
      "sequence_length": 4
    },
    {
      "example_id": "qb_6543--195/195_826037.txt#0_0",
      "dataset_type": "long",
      "response": "john f. kennedy international airport",
      "accuracy": 0.0,
      "g_nll": 1.0435222079918276,
      "average_nll": 0.13044027599897845,
      "perplexity": 1.1393298925218853,
      "avg_token_probability": 0.8984354185606361,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_24154--30/30_69897.txt#0_0",
      "dataset_type": "long",
      "response": "george sutherland",
      "accuracy": 0.0,
      "g_nll": 1.744736148393713,
      "average_nll": 0.3489472296787426,
      "perplexity": 1.4175743825126155,
      "avg_token_probability": 0.7831794848267093,
      "sequence_length": 5
    },
    {
      "example_id": "qb_3569--170/170_366697.txt#0_0",
      "dataset_type": "long",
      "response": "popeye",
      "accuracy": 0.0,
      "g_nll": 0.8044489654712379,
      "average_nll": 0.2681496551570793,
      "perplexity": 1.307542805897371,
      "avg_token_probability": 0.796719708527387,
      "sequence_length": 3
    },
    {
      "example_id": "odql_9564--83/83_2256714.txt#0_2",
      "dataset_type": "long",
      "response": "white",
      "accuracy": 0.0,
      "g_nll": 3.190748106688261,
      "average_nll": 1.5953740533441305,
      "perplexity": 4.930172874908329,
      "avg_token_probability": 0.5112616819516527,
      "sequence_length": 2
    },
    {
      "example_id": "jp_2812--168/168_2992342.txt#0_0",
      "dataset_type": "long",
      "response": "a dragon",
      "accuracy": 1.0,
      "g_nll": 0.18833784038724843,
      "average_nll": 0.0627792801290828,
      "perplexity": 1.064791792585514,
      "avg_token_probability": 0.9406674385007426,
      "sequence_length": 3
    },
    {
      "example_id": "bt_462--66/66_2369917.txt#0_0",
      "dataset_type": "long",
      "response": "bogeyman",
      "accuracy": 1.0,
      "g_nll": 0.9951683910912834,
      "average_nll": 0.24879209777282085,
      "perplexity": 1.2824753758645138,
      "avg_token_probability": 0.8396675901367168,
      "sequence_length": 4
    },
    {
      "example_id": "qz_847--105/105_2601971.txt#0_0",
      "dataset_type": "long",
      "response": "happy birthday",
      "accuracy": 1.0,
      "g_nll": 0.0006220179202500731,
      "average_nll": 0.00031100896012503654,
      "perplexity": 1.000311057328426,
      "avg_token_probability": 0.999689058712925,
      "sequence_length": 2
    },
    {
      "example_id": "qz_2430--114/114_2605502.txt#0_1",
      "dataset_type": "long",
      "response": "castor",
      "accuracy": 1.0,
      "g_nll": 0.3537783920712627,
      "average_nll": 0.17688919603563136,
      "perplexity": 1.1934988413971699,
      "avg_token_probability": 0.8510147832998317,
      "sequence_length": 2
    },
    {
      "example_id": "tb_470--Bill_Gold.txt#0_0",
      "dataset_type": "long",
      "response": "film posters",
      "accuracy": 0.0,
      "g_nll": 2.9470285791903734,
      "average_nll": 0.9823428597301245,
      "perplexity": 2.6707060074645677,
      "avg_token_probability": 0.5527524141287072,
      "sequence_length": 3
    },
    {
      "example_id": "qf_2679--160/160_194946.txt#0_0",
      "dataset_type": "long",
      "response": "jon pertwee",
      "accuracy": 1.0,
      "g_nll": 0.5586340392401326,
      "average_nll": 0.11172680784802651,
      "perplexity": 1.1182073334453524,
      "avg_token_probability": 0.9023141008486701,
      "sequence_length": 5
    },
    {
      "example_id": "qf_2195--13/13_325145.txt#0_2",
      "dataset_type": "long",
      "response": "andre agassi",
      "accuracy": 1.0,
      "g_nll": 0.14361080201342702,
      "average_nll": 0.07180540100671351,
      "perplexity": 1.074446237562346,
      "avg_token_probability": 0.932845463347834,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7372--177/177_98429.txt#0_1",
      "dataset_type": "long",
      "response": "1",
      "accuracy": 0.0,
      "g_nll": 2.9018481746315956,
      "average_nll": 0.9672827248771986,
      "perplexity": 2.6307861684268334,
      "avg_token_probability": 0.6117342626363977,
      "sequence_length": 3
    },
    {
      "example_id": "qz_6793--102/102_2886022.txt#0_1",
      "dataset_type": "long",
      "response": "verruckt",
      "accuracy": 0.0,
      "g_nll": 3.3865146402604296,
      "average_nll": 0.8466286600651074,
      "perplexity": 2.3317723882658328,
      "avg_token_probability": 0.7503947457153259,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_17762--30/30_127115.txt#0_1",
      "dataset_type": "long",
      "response": "south africa",
      "accuracy": 1.0,
      "g_nll": 0.09130828641355038,
      "average_nll": 0.04565414320677519,
      "perplexity": 1.0467123357755856,
      "avg_token_probability": 0.9562693868118926,
      "sequence_length": 2
    },
    {
      "example_id": "bb_9184--196/196_1039737.txt#0_0",
      "dataset_type": "long",
      "response": "scales",
      "accuracy": 0.0,
      "g_nll": 0.02531849965453148,
      "average_nll": 0.01265924982726574,
      "perplexity": 1.0127397173245685,
      "avg_token_probability": 0.9874303637063468,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1265--Alan_Lake.txt#0_0",
      "dataset_type": "long",
      "response": "diana dors",
      "accuracy": 1.0,
      "g_nll": 0.39829112838197034,
      "average_nll": 0.09957278209549258,
      "perplexity": 1.1046988701127551,
      "avg_token_probability": 0.9098908409800706,
      "sequence_length": 4
    },
    {
      "example_id": "bb_1777--A_Gay_Girl_In_Damascus.txt#0_1",
      "dataset_type": "long",
      "response": "syria",
      "accuracy": 1.0,
      "g_nll": 0.042508597689447924,
      "average_nll": 0.014169532563149309,
      "perplexity": 1.0142703962235944,
      "avg_token_probability": 0.986055816555503,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_2901--81/81_239948.txt#0_4",
      "dataset_type": "long",
      "response": "the holy grail",
      "accuracy": 1.0,
      "g_nll": 0.31896603600705475,
      "average_nll": 0.06379320720141095,
      "perplexity": 1.0658719613240337,
      "avg_token_probability": 0.942970534858507,
      "sequence_length": 5
    },
    {
      "example_id": "odql_11679--16/16_33164.txt#0_0",
      "dataset_type": "long",
      "response": "Tasmania",
      "accuracy": 1.0,
      "g_nll": 3.8727374561131,
      "average_nll": 1.93636872805655,
      "perplexity": 6.933527677510942,
      "avg_token_probability": 0.49587038298535346,
      "sequence_length": 2
    },
    {
      "example_id": "bb_5083--184/184_944999.txt#0_1",
      "dataset_type": "long",
      "response": "guitar",
      "accuracy": 1.0,
      "g_nll": 0.2606186121702194,
      "average_nll": 0.1303093060851097,
      "perplexity": 1.1391806843550973,
      "avg_token_probability": 0.8826619205572905,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3061--16/16_2854536.txt#0_2",
      "dataset_type": "long",
      "response": "ragdoll",
      "accuracy": 0.0,
      "g_nll": 2.497219636279624,
      "average_nll": 0.8324065454265414,
      "perplexity": 2.2988443621534254,
      "avg_token_probability": 0.6799332851128131,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9983--123/123_77798.txt#0_0",
      "dataset_type": "long",
      "response": "florida",
      "accuracy": 1.0,
      "g_nll": 0.16222970932722092,
      "average_nll": 0.08111485466361046,
      "perplexity": 1.0844954487738785,
      "avg_token_probability": 0.9220995685304902,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_19297--8/8_1898303.txt#0_0",
      "dataset_type": "long",
      "response": "spa",
      "accuracy": 1.0,
      "g_nll": 1.3235452473163605,
      "average_nll": 0.6617726236581802,
      "perplexity": 1.9382250346116088,
      "avg_token_probability": 0.5671483577001043,
      "sequence_length": 2
    },
    {
      "example_id": "qg_762--191/191_2525665.txt#0_1",
      "dataset_type": "long",
      "response": "adam's apple",
      "accuracy": 0.0,
      "g_nll": 0.23312356532551348,
      "average_nll": 0.05828089133137837,
      "perplexity": 1.0600127022759536,
      "avg_token_probability": 0.9457162407144241,
      "sequence_length": 4
    },
    {
      "example_id": "jp_2596--47/47_1415882.txt#0_0",
      "dataset_type": "long",
      "response": "cleopatra",
      "accuracy": 1.0,
      "g_nll": 0.14301317924218893,
      "average_nll": 0.03575329481054723,
      "perplexity": 1.0364001296585497,
      "avg_token_probability": 0.9656073432450931,
      "sequence_length": 4
    },
    {
      "example_id": "qg_2692--122/122_2425619.txt#0_0",
      "dataset_type": "long",
      "response": "jon stewart",
      "accuracy": 1.0,
      "g_nll": 0.0934028397313682,
      "average_nll": 0.02335070993284205,
      "perplexity": 1.02362547222353,
      "avg_token_probability": 0.9771773183974319,
      "sequence_length": 4
    },
    {
      "example_id": "tb_1432--124/124_757762.txt#0_0",
      "dataset_type": "long",
      "response": "toledo",
      "accuracy": 1.0,
      "g_nll": 0.05057882305118255,
      "average_nll": 0.016859607683727518,
      "perplexity": 1.017002532960953,
      "avg_token_probability": 0.9834159887688342,
      "sequence_length": 3
    },
    {
      "example_id": "qw_14885--124/124_1320147.txt#0_1",
      "dataset_type": "long",
      "response": "mikado",
      "accuracy": 1.0,
      "g_nll": 1.9250667512169457,
      "average_nll": 0.6416889170723152,
      "perplexity": 1.899686584562491,
      "avg_token_probability": 0.6785019697219438,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_8765--71/71_3209534.txt#0_0",
      "dataset_type": "long",
      "response": "lord belborough",
      "accuracy": 1.0,
      "g_nll": 0.15588840187410824,
      "average_nll": 0.03897210046852706,
      "perplexity": 1.0397414749454912,
      "avg_token_probability": 0.9627790631244173,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_15153--128/128_1671883.txt#0_0",
      "dataset_type": "long",
      "response": "isaac",
      "accuracy": 0.0,
      "g_nll": 0.3799446590564912,
      "average_nll": 0.12664821968549708,
      "perplexity": 1.1350176706712236,
      "avg_token_probability": 0.892473281274991,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9024--160/160_3017466.txt#0_2",
      "dataset_type": "long",
      "response": "doubting castle",
      "accuracy": 1.0,
      "g_nll": 0.06881090707361182,
      "average_nll": 0.017202726768402954,
      "perplexity": 1.0173515458122362,
      "avg_token_probability": 0.9830866268733224,
      "sequence_length": 4
    },
    {
      "example_id": "qb_9081--150/150_353549.txt#0_2",
      "dataset_type": "long",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.05954701825976372,
      "average_nll": 0.02977350912988186,
      "perplexity": 1.0302211718376966,
      "avg_token_probability": 0.970691743455965,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24683--102/102_3210389.txt#0_0",
      "dataset_type": "long",
      "response": "paul bayes",
      "accuracy": 1.0,
      "g_nll": 0.0812548215908464,
      "average_nll": 0.0203137053977116,
      "perplexity": 1.0205214328986807,
      "avg_token_probability": 0.9800752151432246,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_5747--32/32_1597239.txt#0_1",
      "dataset_type": "long",
      "response": "the charites",
      "accuracy": 0.0,
      "g_nll": 0.838392501231283,
      "average_nll": 0.20959812530782074,
      "perplexity": 1.2331823755743254,
      "avg_token_probability": 0.842959089615799,
      "sequence_length": 4
    },
    {
      "example_id": "qb_10061--5/5_543694.txt#0_2",
      "dataset_type": "long",
      "response": "pisces",
      "accuracy": 1.0,
      "g_nll": 0.02841965219249687,
      "average_nll": 0.009473217397498956,
      "perplexity": 1.009518230348305,
      "avg_token_probability": 0.9906468316208623,
      "sequence_length": 3
    },
    {
      "example_id": "qb_9573--143/143_499089.txt#0_0",
      "dataset_type": "long",
      "response": "15",
      "accuracy": 1.0,
      "g_nll": 0.9577011485816911,
      "average_nll": 0.319233716193897,
      "perplexity": 1.3760728978463832,
      "avg_token_probability": 0.7618209226445153,
      "sequence_length": 3
    },
    {
      "example_id": "qw_3199--48/48_2960705.txt#0_2",
      "dataset_type": "long",
      "response": "atlantic",
      "accuracy": 1.0,
      "g_nll": 0.40091223451327096,
      "average_nll": 0.13363741150442365,
      "perplexity": 1.1429783137144012,
      "avg_token_probability": 0.8881981827147557,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8510--52/52_28284.txt#0_0",
      "dataset_type": "long",
      "response": "Delaware",
      "accuracy": 1.0,
      "g_nll": 3.4496433194726706,
      "average_nll": 1.7248216597363353,
      "perplexity": 5.611520180461408,
      "avg_token_probability": 0.503973145687217,
      "sequence_length": 2
    },
    {
      "example_id": "qb_603--163/163_281670.txt#0_1",
      "dataset_type": "long",
      "response": "belgium",
      "accuracy": 1.0,
      "g_nll": 0.27143598345674036,
      "average_nll": 0.09047866115224679,
      "perplexity": 1.0946981477952997,
      "avg_token_probability": 0.9183934732656079,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3955--64/64_337467.txt#0_1",
      "dataset_type": "long",
      "response": "hockey",
      "accuracy": 0.0,
      "g_nll": 1.3123987559229136,
      "average_nll": 0.6561993779614568,
      "perplexity": 1.9274528761040821,
      "avg_token_probability": 0.6291716297940498,
      "sequence_length": 2
    },
    {
      "example_id": "qb_406--51/51_276103.txt#0_0",
      "dataset_type": "long",
      "response": "madison square garden",
      "accuracy": 1.0,
      "g_nll": 0.3137451036091079,
      "average_nll": 0.06274902072182158,
      "perplexity": 1.0647595731044872,
      "avg_token_probability": 0.9441996356299054,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_23994--31/31_726041.txt#0_0",
      "dataset_type": "long",
      "response": "richard strauss",
      "accuracy": 1.0,
      "g_nll": 0.25513205138781814,
      "average_nll": 0.051026410277563626,
      "perplexity": 1.0523506857899043,
      "avg_token_probability": 0.9537891663857598,
      "sequence_length": 5
    },
    {
      "example_id": "wh_2249--178/178_778787.txt#0_0",
      "dataset_type": "long",
      "response": "baby prams",
      "accuracy": 0.0,
      "g_nll": 0.7392590064555407,
      "average_nll": 0.18481475161388516,
      "perplexity": 1.2029955664976457,
      "avg_token_probability": 0.8620496627706719,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5653--76/76_551825.txt#0_0",
      "dataset_type": "long",
      "response": "trombone",
      "accuracy": 1.0,
      "g_nll": 0.8756171786226332,
      "average_nll": 0.2918723928742111,
      "perplexity": 1.3389321494543083,
      "avg_token_probability": 0.8006406091257753,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_178--183/183_95802.txt#0_0",
      "dataset_type": "long",
      "response": "pulsar",
      "accuracy": 1.0,
      "g_nll": 0.02936635899823159,
      "average_nll": 0.014683179499115795,
      "perplexity": 1.0147915069268572,
      "avg_token_probability": 0.9855268116190512,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2312--185/185_26932.txt#0_0",
      "dataset_type": "long",
      "response": "apollo 11",
      "accuracy": 1.0,
      "g_nll": 0.42378356536937645,
      "average_nll": 0.08475671307387529,
      "perplexity": 1.08845222828699,
      "avg_token_probability": 0.9284793309596946,
      "sequence_length": 5
    },
    {
      "example_id": "jp_3598--137/137_1440776.txt#0_0",
      "dataset_type": "long",
      "response": "lake nicaragua",
      "accuracy": 0.0,
      "g_nll": 0.0719871943506405,
      "average_nll": 0.014397438870128098,
      "perplexity": 1.0145015811871307,
      "avg_token_probability": 0.9859025594113376,
      "sequence_length": 5
    },
    {
      "example_id": "qb_804--33/33_287054.txt#0_0",
      "dataset_type": "long",
      "response": "leprosy",
      "accuracy": 1.0,
      "g_nll": 0.10230521768608014,
      "average_nll": 0.025576304421520035,
      "perplexity": 1.0259061844618598,
      "avg_token_probability": 0.9751323574320556,
      "sequence_length": 4
    },
    {
      "example_id": "qb_9589--67/67_530673.txt#0_1",
      "dataset_type": "long",
      "response": "world war i",
      "accuracy": 1.0,
      "g_nll": 1.1344238195524667,
      "average_nll": 0.2836059548881167,
      "perplexity": 1.327909571370342,
      "avg_token_probability": 0.81500048106995,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_25127--Norway.txt#0_0",
      "dataset_type": "long",
      "response": "Norga",
      "accuracy": 0.0,
      "g_nll": 9.405175380408764,
      "average_nll": 2.351293845102191,
      "perplexity": 10.499145208152145,
      "avg_token_probability": 0.2911145832646178,
      "sequence_length": 4
    },
    {
      "example_id": "odql_9408--60/60_2232484.txt#0_0",
      "dataset_type": "long",
      "response": "steve coogan",
      "accuracy": 1.0,
      "g_nll": 5.159479688841657,
      "average_nll": 1.0318959377683314,
      "perplexity": 2.8063815187174113,
      "avg_token_probability": 0.792760700547885,
      "sequence_length": 5
    },
    {
      "example_id": "jp_65--42/42_1350780.txt#0_0",
      "dataset_type": "long",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.04076261398995484,
      "average_nll": 0.013587537996651614,
      "perplexity": 1.013680268106369,
      "avg_token_probability": 0.9866352558771435,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2814--35/35_85764.txt#0_0",
      "dataset_type": "long",
      "response": "ornithology",
      "accuracy": 1.0,
      "g_nll": 0.046346900904609356,
      "average_nll": 0.011586725226152339,
      "perplexity": 1.011654111336843,
      "avg_token_probability": 0.9886424988491587,
      "sequence_length": 4
    },
    {
      "example_id": "bb_3629--115/115_2676705.txt#0_0",
      "dataset_type": "long",
      "response": "mozilla",
      "accuracy": 1.0,
      "g_nll": 0.10932242451235652,
      "average_nll": 0.05466121225617826,
      "perplexity": 1.0561827322908983,
      "avg_token_probability": 0.9479336055238115,
      "sequence_length": 2
    },
    {
      "example_id": "odql_2332--58/58_2808154.txt#0_0",
      "dataset_type": "long",
      "response": "david frost",
      "accuracy": 1.0,
      "g_nll": 0.2755096107721329,
      "average_nll": 0.0918365369240443,
      "perplexity": 1.096185621561363,
      "avg_token_probability": 0.9141813320044245,
      "sequence_length": 3
    },
    {
      "example_id": "qw_7795--46/46_1197146.txt#0_1",
      "dataset_type": "long",
      "response": "books",
      "accuracy": 1.0,
      "g_nll": 1.7782313069328666,
      "average_nll": 0.8891156534664333,
      "perplexity": 2.432977104762787,
      "avg_token_probability": 0.5802756502064654,
      "sequence_length": 2
    },
    {
      "example_id": "qb_723--49/49_284785.txt#0_0",
      "dataset_type": "long",
      "response": "bangladesh",
      "accuracy": 1.0,
      "g_nll": 0.00269575190031901,
      "average_nll": 0.001347875950159505,
      "perplexity": 1.0013487847432156,
      "avg_token_probability": 0.9986533974092988,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_1470--Sting_(musician).txt#0_0",
      "dataset_type": "long",
      "response": "lute",
      "accuracy": 1.0,
      "g_nll": 0.12978961504995823,
      "average_nll": 0.04326320501665274,
      "perplexity": 1.0442127007051645,
      "avg_token_probability": 0.9578296808307273,
      "sequence_length": 3
    },
    {
      "example_id": "jp_4058--151/151_131448.txt#0_0",
      "dataset_type": "long",
      "response": "gremlins",
      "accuracy": 1.0,
      "g_nll": 0.07376488985027407,
      "average_nll": 0.01844122246256852,
      "perplexity": 1.0186123118867,
      "avg_token_probability": 0.9819509558608904,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1767--17/17_778036.txt#0_2",
      "dataset_type": "long",
      "response": "bolivia",
      "accuracy": 1.0,
      "g_nll": 0.1104567462898558,
      "average_nll": 0.036818915429951936,
      "perplexity": 1.0375051276564282,
      "avg_token_probability": 0.9644027310986562,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3732--157/157_915760.txt#0_0",
      "dataset_type": "long",
      "response": "google glass",
      "accuracy": 1.0,
      "g_nll": 1.6607935316860676,
      "average_nll": 0.5535978438953558,
      "perplexity": 1.7395002231526349,
      "avg_token_probability": 0.7179472510436677,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_893--144/144_1486574.txt#0_1",
      "dataset_type": "long",
      "response": "tenerife",
      "accuracy": 1.0,
      "g_nll": 0.5569695439135103,
      "average_nll": 0.18565651463783675,
      "perplexity": 1.2040086300033825,
      "avg_token_probability": 0.8544921566138554,
      "sequence_length": 3
    },
    {
      "example_id": "qz_3456--13/13_2880285.txt#0_0",
      "dataset_type": "long",
      "response": "Jonathon Bell",
      "accuracy": 0.0,
      "g_nll": 6.399659750750288,
      "average_nll": 1.599914937687572,
      "perplexity": 4.952611125922131,
      "avg_token_probability": 0.6471068951276414,
      "sequence_length": 4
    },
    {
      "example_id": "qw_1421--131/131_1224551.txt#0_0",
      "dataset_type": "long",
      "response": "new zealand",
      "accuracy": 1.0,
      "g_nll": 0.27609832298912806,
      "average_nll": 0.06902458074728202,
      "perplexity": 1.0714625461747227,
      "avg_token_probability": 0.9374992252882255,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5852--179/179_961871.txt#0_1",
      "dataset_type": "long",
      "response": "gold",
      "accuracy": 1.0,
      "g_nll": 0.027536319568753242,
      "average_nll": 0.013768159784376621,
      "perplexity": 1.0138633773848407,
      "avg_token_probability": 0.9863375026850065,
      "sequence_length": 2
    },
    {
      "example_id": "qb_2689--32/32_340888.txt#0_0",
      "dataset_type": "long",
      "response": "a kid for two farthings",
      "accuracy": 0.0,
      "g_nll": 0.35342108754775836,
      "average_nll": 0.05048872679253691,
      "perplexity": 1.0517850062975873,
      "avg_token_probability": 0.9562462945626555,
      "sequence_length": 7
    },
    {
      "example_id": "odql_14833--5/5_2353740.txt#0_2",
      "dataset_type": "long",
      "response": "atomic kitten",
      "accuracy": 1.0,
      "g_nll": 0.10630274517461658,
      "average_nll": 0.03543424839153886,
      "perplexity": 1.0360695226508176,
      "avg_token_probability": 0.9653860035383844,
      "sequence_length": 3
    },
    {
      "example_id": "qw_11763--166/166_2714280.txt#0_0",
      "dataset_type": "long",
      "response": "optical illusion",
      "accuracy": 0.0,
      "g_nll": 0.5263704147655517,
      "average_nll": 0.17545680492185056,
      "perplexity": 1.1917905080552826,
      "avg_token_probability": 0.8585135932965168,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_11775--22/22_3209747.txt#0_0",
      "dataset_type": "long",
      "response": "wheel arrangement",
      "accuracy": 1.0,
      "g_nll": 0.27761160582304,
      "average_nll": 0.09253720194101334,
      "perplexity": 1.0969539496173577,
      "avg_token_probability": 0.9142951858973332,
      "sequence_length": 3
    },
    {
      "example_id": "wh_4376--136/136_828693.txt#0_0",
      "dataset_type": "long",
      "response": "volleyball",
      "accuracy": 1.0,
      "g_nll": 0.23767057992517948,
      "average_nll": 0.11883528996258974,
      "perplexity": 1.126184409135003,
      "avg_token_probability": 0.8917153176212179,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_99--56/56_553713.txt#0_1",
      "dataset_type": "long",
      "response": "liver",
      "accuracy": 1.0,
      "g_nll": 0.00890651810914278,
      "average_nll": 0.00890651810914278,
      "perplexity": 1.0089462991573896,
      "avg_token_probability": 0.9911330274318256,
      "sequence_length": 1
    },
    {
      "example_id": "qf_1776--73/73_1938527.txt#0_0",
      "dataset_type": "long",
      "response": "anthony joshua",
      "accuracy": 1.0,
      "g_nll": 0.0765482459900042,
      "average_nll": 0.012758040998334034,
      "perplexity": 1.0128397720094087,
      "avg_token_probability": 0.9874619118080502,
      "sequence_length": 6
    },
    {
      "example_id": "odql_3170--63/63_218030.txt#0_1",
      "dataset_type": "long",
      "response": "jessica",
      "accuracy": 1.0,
      "g_nll": 0.32769098482094705,
      "average_nll": 0.08192274620523676,
      "perplexity": 1.0853719574881895,
      "avg_token_probability": 0.926001873494838,
      "sequence_length": 4
    },
    {
      "example_id": "qw_11746--112/112_168063.txt#0_1",
      "dataset_type": "long",
      "response": "spring",
      "accuracy": 0.0,
      "g_nll": 1.1751753073185682,
      "average_nll": 0.5875876536592841,
      "perplexity": 1.79964181540546,
      "avg_token_probability": 0.648495325550586,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_1253--77/77_2739999.txt#0_0",
      "dataset_type": "long",
      "response": "common nettle",
      "accuracy": 1.0,
      "g_nll": 0.6769283628090648,
      "average_nll": 0.1692320907022662,
      "perplexity": 1.1843949940930105,
      "avg_token_probability": 0.8710386494698108,
      "sequence_length": 4
    },
    {
      "example_id": "tc_1945--192/192_114247.txt#0_2",
      "dataset_type": "long",
      "response": "schenck",
      "accuracy": 0.0,
      "g_nll": 0.05689710908200141,
      "average_nll": 0.014224277270500352,
      "perplexity": 1.014325923679514,
      "avg_token_probability": 0.9859776859794867,
      "sequence_length": 4
    },
    {
      "example_id": "bb_9117--33/33_1038201.txt#0_0",
      "dataset_type": "long",
      "response": "arthur",
      "accuracy": 1.0,
      "g_nll": 0.08821440069004893,
      "average_nll": 0.02940480023001631,
      "perplexity": 1.0298413901415582,
      "avg_token_probability": 0.9711905045663073,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9352--51/51_1224596.txt#0_0",
      "dataset_type": "long",
      "response": "jarrah",
      "accuracy": 1.0,
      "g_nll": 0.2744661971482856,
      "average_nll": 0.09148873238276185,
      "perplexity": 1.0958044295181,
      "avg_token_probability": 0.9185348364562076,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_6097--26/26_171255.txt#0_0",
      "dataset_type": "long",
      "response": "russia",
      "accuracy": 0.0,
      "g_nll": 0.04299475532025099,
      "average_nll": 0.01433158510675033,
      "perplexity": 1.0144347746398121,
      "avg_token_probability": 0.9858610337655521,
      "sequence_length": 3
    },
    {
      "example_id": "bb_2192--123/123_881325.txt#0_1",
      "dataset_type": "long",
      "response": "a tale of two cities",
      "accuracy": 0.0,
      "g_nll": 0.2768267765895871,
      "average_nll": 0.046137796098264516,
      "perplexity": 1.0472187036666176,
      "avg_token_probability": 0.9566813492776806,
      "sequence_length": 6
    },
    {
      "example_id": "bb_1292--123/123_187127.txt#0_1",
      "dataset_type": "long",
      "response": "september",
      "accuracy": 1.0,
      "g_nll": 0.1656801700592041,
      "average_nll": 0.08284008502960205,
      "perplexity": 1.0863680681393044,
      "avg_token_probability": 0.9207579911521449,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_15107--7/7_246280.txt#0_0",
      "dataset_type": "long",
      "response": "cerberus",
      "accuracy": 0.0,
      "g_nll": 2.1276134664585697,
      "average_nll": 0.5319033666146424,
      "perplexity": 1.7021690790585278,
      "avg_token_probability": 0.7745555626686911,
      "sequence_length": 4
    },
    {
      "example_id": "bb_2094--73/73_318480.txt#0_1",
      "dataset_type": "long",
      "response": "12",
      "accuracy": 1.0,
      "g_nll": 1.3463056036271155,
      "average_nll": 0.4487685345423718,
      "perplexity": 1.5663820518973062,
      "avg_token_probability": 0.7414588165296441,
      "sequence_length": 3
    },
    {
      "example_id": "bb_6229--199/199_904245.txt#0_2",
      "dataset_type": "long",
      "response": "cyanide",
      "accuracy": 1.0,
      "g_nll": 0.9324795892462134,
      "average_nll": 0.3108265297487378,
      "perplexity": 1.3645524913904246,
      "avg_token_probability": 0.7905797124058415,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3356--154/154_643057.txt#0_1",
      "dataset_type": "long",
      "response": "lot",
      "accuracy": 0.0,
      "g_nll": 3.4107572734355927,
      "average_nll": 1.7053786367177963,
      "perplexity": 5.503469088228818,
      "avg_token_probability": 0.5039264347670777,
      "sequence_length": 2
    },
    {
      "example_id": "qw_1930--34/34_112989.txt#0_2",
      "dataset_type": "long",
      "response": "nose",
      "accuracy": 1.0,
      "g_nll": 0.4054620638489723,
      "average_nll": 0.20273103192448616,
      "perplexity": 1.2247430071725915,
      "avg_token_probability": 0.828647719911619,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_7913--89/89_1646797.txt#0_0",
      "dataset_type": "long",
      "response": "vacuum cleaner",
      "accuracy": 1.0,
      "g_nll": 1.4320713964989409,
      "average_nll": 0.47735713216631365,
      "perplexity": 1.6118089701078688,
      "avg_token_probability": 0.7392736909935921,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_223--47/47_103402.txt#0_0",
      "dataset_type": "long",
      "response": "surrealist",
      "accuracy": 1.0,
      "g_nll": 0.8614167834166437,
      "average_nll": 0.2871389278055479,
      "perplexity": 1.332609337106825,
      "avg_token_probability": 0.7937729954553842,
      "sequence_length": 3
    },
    {
      "example_id": "odql_6316--193/193_2196789.txt#0_1",
      "dataset_type": "long",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.00014733182615600526,
      "average_nll": 0.00014733182615600526,
      "perplexity": 1.0001473426800225,
      "avg_token_probability": 0.9998526790266445,
      "sequence_length": 1
    },
    {
      "example_id": "qb_7632--22/22_167620.txt#0_1",
      "dataset_type": "long",
      "response": "leroy",
      "accuracy": 1.0,
      "g_nll": 0.7271196842192467,
      "average_nll": 0.36355984210962333,
      "perplexity": 1.4384409336697732,
      "avg_token_probability": 0.7416493973545799,
      "sequence_length": 2
    },
    {
      "example_id": "bb_7149--145/145_206396.txt#0_1",
      "dataset_type": "long",
      "response": "kazakhstan",
      "accuracy": 1.0,
      "g_nll": 0.08931408065836877,
      "average_nll": 0.029771360219456255,
      "perplexity": 1.0302189579870582,
      "avg_token_probability": 0.9708798695571903,
      "sequence_length": 3
    },
    {
      "example_id": "qb_10019--56/56_542648.txt#0_0",
      "dataset_type": "long",
      "response": "ely",
      "accuracy": 1.0,
      "g_nll": 0.14191219175881997,
      "average_nll": 0.07095609587940999,
      "perplexity": 1.073534092263441,
      "avg_token_probability": 0.9338469796385798,
      "sequence_length": 2
    },
    {
      "example_id": "jp_4108--93/93_1452941.txt#0_1",
      "dataset_type": "long",
      "response": "moonwalk",
      "accuracy": 1.0,
      "g_nll": 0.2978778872638941,
      "average_nll": 0.0992926290879647,
      "perplexity": 1.1043894287493687,
      "avg_token_probability": 0.9110712972765246,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_14649--125/125_1796628.txt#0_1",
      "dataset_type": "long",
      "response": "abel janszoon tasman",
      "accuracy": 0.0,
      "g_nll": 1.0505979946015032,
      "average_nll": 0.11673311051127813,
      "perplexity": 1.123819454062636,
      "avg_token_probability": 0.9156403520178996,
      "sequence_length": 9
    },
    {
      "example_id": "qb_6660--191/191_451330.txt#0_2",
      "dataset_type": "long",
      "response": "chalice",
      "accuracy": 0.0,
      "g_nll": 3.0163117262127344,
      "average_nll": 1.0054372420709115,
      "perplexity": 2.7331020388013756,
      "avg_token_probability": 0.6770871342106589,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_19040--87/87_1892922.txt#0_2",
      "dataset_type": "long",
      "response": "red",
      "accuracy": 1.0,
      "g_nll": 0.04850634187459946,
      "average_nll": 0.02425317093729973,
      "perplexity": 1.0245496712594513,
      "avg_token_probability": 0.9762283334966056,
      "sequence_length": 2
    },
    {
      "example_id": "tb_1113--48/48_567125.txt#0_1",
      "dataset_type": "long",
      "response": "man",
      "accuracy": 1.0,
      "g_nll": 0.007008729502558708,
      "average_nll": 0.007008729502558708,
      "perplexity": 1.0070333481286677,
      "avg_token_probability": 0.9930157743616559,
      "sequence_length": 1
    },
    {
      "example_id": "qb_9264--54/54_521908.txt#0_0",
      "dataset_type": "long",
      "response": "persian gulf",
      "accuracy": 1.0,
      "g_nll": 1.2054833582933497,
      "average_nll": 0.24109667165866994,
      "perplexity": 1.2726440579737115,
      "avg_token_probability": 0.8559782901411961,
      "sequence_length": 5
    },
    {
      "example_id": "bb_5625--174/174_2946795.txt#0_1",
      "dataset_type": "long",
      "response": "noa noa on the wannsee",
      "accuracy": 0.0,
      "g_nll": 2.970878191573547,
      "average_nll": 0.33009757684150526,
      "perplexity": 1.3911038613624762,
      "avg_token_probability": 0.8582538047846006,
      "sequence_length": 9
    },
    {
      "example_id": "odql_11051--136/136_235136.txt#0_2",
      "dataset_type": "long",
      "response": "$1000",
      "accuracy": 0.0,
      "g_nll": 1.409132819622755,
      "average_nll": 0.35228320490568876,
      "perplexity": 1.4223112722154183,
      "avg_token_probability": 0.7657254452873337,
      "sequence_length": 4
    },
    {
      "example_id": "qw_9028--150/150_617354.txt#0_0",
      "dataset_type": "long",
      "response": "beyonce",
      "accuracy": 0.0,
      "g_nll": 0.14791690558195114,
      "average_nll": 0.04930563519398371,
      "perplexity": 1.0505413840976994,
      "avg_token_probability": 0.9525788150857256,
      "sequence_length": 3
    },
    {
      "example_id": "bb_7263--97/97_992367.txt#0_0",
      "dataset_type": "long",
      "response": "turkey",
      "accuracy": 1.0,
      "g_nll": 0.059623390436172485,
      "average_nll": 0.029811695218086243,
      "perplexity": 1.0302605127053663,
      "avg_token_probability": 0.9706288432943091,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7717--108/108_2904125.txt#0_0",
      "dataset_type": "long",
      "response": "ireland",
      "accuracy": 1.0,
      "g_nll": 0.1210001520721562,
      "average_nll": 0.040333384024052066,
      "perplexity": 1.041157821723402,
      "avg_token_probability": 0.9611182104887672,
      "sequence_length": 3
    },
    {
      "example_id": "qg_158--17/17_2239200.txt#0_0",
      "dataset_type": "long",
      "response": "carmen",
      "accuracy": 1.0,
      "g_nll": 0.5652029170596506,
      "average_nll": 0.18840097235321687,
      "perplexity": 1.2073175192538035,
      "avg_token_probability": 0.8458767926401478,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_11783--191/191_1732943.txt#0_1",
      "dataset_type": "long",
      "response": "chess",
      "accuracy": 1.0,
      "g_nll": 0.10572306625545025,
      "average_nll": 0.052861533127725124,
      "perplexity": 1.054283651652225,
      "avg_token_probability": 0.9492750339866212,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1920--147/147_1398729.txt#0_2",
      "dataset_type": "long",
      "response": "dos equis",
      "accuracy": 1.0,
      "g_nll": 0.22406760914600454,
      "average_nll": 0.056016902286501136,
      "perplexity": 1.0576155597067367,
      "avg_token_probability": 0.9468368347573071,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_893--144/144_1486574.txt#0_0",
      "dataset_type": "long",
      "response": "tenerife",
      "accuracy": 1.0,
      "g_nll": 0.06041678392648464,
      "average_nll": 0.02013892797549488,
      "perplexity": 1.0203430843794228,
      "avg_token_probability": 0.9801633917694961,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_5791--38/38_519647.txt#0_2",
      "dataset_type": "long",
      "response": "william gladstone",
      "accuracy": 1.0,
      "g_nll": 0.9847688373902201,
      "average_nll": 0.24619220934755504,
      "perplexity": 1.2791454136199807,
      "avg_token_probability": 0.8238470449202627,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1635--165/165_596631.txt#0_2",
      "dataset_type": "long",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 0.03914259560406208,
      "average_nll": 0.01957129780203104,
      "perplexity": 1.0197640712056648,
      "avg_token_probability": 0.980695144605482,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3490--39/39_3004847.txt#0_0",
      "dataset_type": "long",
      "response": "leeds",
      "accuracy": 1.0,
      "g_nll": 0.8401822907471796,
      "average_nll": 0.28006076358239324,
      "perplexity": 1.3232102128874939,
      "avg_token_probability": 0.7767132924310447,
      "sequence_length": 3
    },
    {
      "example_id": "qz_2567--129/129_10435.txt#0_0",
      "dataset_type": "long",
      "response": "ironside",
      "accuracy": 1.0,
      "g_nll": 3.7847774408292025,
      "average_nll": 1.261592480276401,
      "perplexity": 3.531040124146737,
      "avg_token_probability": 0.5942935537328397,
      "sequence_length": 3
    },
    {
      "example_id": "odql_9169--18/18_3213102.txt#0_1",
      "dataset_type": "long",
      "response": "lungs",
      "accuracy": 1.0,
      "g_nll": 0.05156975984573364,
      "average_nll": 0.02578487992286682,
      "perplexity": 1.026120185675548,
      "avg_token_probability": 0.9746649480756593,
      "sequence_length": 2
    },
    {
      "example_id": "odql_9169--18/18_3213102.txt#0_0",
      "dataset_type": "long",
      "response": "lungs",
      "accuracy": 1.0,
      "g_nll": 0.06826062686741352,
      "average_nll": 0.03413031343370676,
      "perplexity": 1.0347194357855087,
      "avg_token_probability": 0.9664996986516177,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_4619--47/47_1571719.txt#0_0",
      "dataset_type": "long",
      "response": "eye",
      "accuracy": 1.0,
      "g_nll": 0.314647788181901,
      "average_nll": 0.1573238940909505,
      "perplexity": 1.170374629943909,
      "avg_token_probability": 0.862311601364901,
      "sequence_length": 2
    },
    {
      "example_id": "qb_1032--189/189_293644.txt#0_0",
      "dataset_type": "long",
      "response": "squash",
      "accuracy": 1.0,
      "g_nll": 0.034617994038853794,
      "average_nll": 0.017308997019426897,
      "perplexity": 1.0174596657612545,
      "avg_token_probability": 0.9829781510824571,
      "sequence_length": 2
    },
    {
      "example_id": "qb_5053--95/95_407079.txt#0_0",
      "dataset_type": "long",
      "response": "shoji",
      "accuracy": 1.0,
      "g_nll": 0.021132231811861857,
      "average_nll": 0.007044077270620619,
      "perplexity": 1.0070689451390218,
      "avg_token_probability": 0.9930285331726573,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1572--159/159_685600.txt#0_2",
      "dataset_type": "long",
      "response": "proest",
      "accuracy": 0.0,
      "g_nll": 2.8132818937301636,
      "average_nll": 1.4066409468650818,
      "perplexity": 4.082219954198649,
      "avg_token_probability": 0.47908755503616374,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7640--89/89_2634466.txt#0_1",
      "dataset_type": "long",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.039291294902795926,
      "average_nll": 0.013097098300931975,
      "perplexity": 1.0131832409550068,
      "avg_token_probability": 0.9871306140114068,
      "sequence_length": 3
    },
    {
      "example_id": "bt_4328--47/47_176665.txt#0_0",
      "dataset_type": "long",
      "response": "arthur wellesley",
      "accuracy": 1.0,
      "g_nll": 0.9261897252986273,
      "average_nll": 0.1543649542164379,
      "perplexity": 1.166916680238839,
      "avg_token_probability": 0.8733669832194605,
      "sequence_length": 6
    },
    {
      "example_id": "qw_597--13/13_1061577.txt#0_2",
      "dataset_type": "long",
      "response": "pygmalion",
      "accuracy": 1.0,
      "g_nll": 0.59243534497989,
      "average_nll": 0.11848706899597801,
      "perplexity": 1.1257923163828916,
      "avg_token_probability": 0.9044032273980351,
      "sequence_length": 5
    },
    {
      "example_id": "qw_2152--104/104_1092719.txt#0_1",
      "dataset_type": "long",
      "response": "Euripides",
      "accuracy": 1.0,
      "g_nll": 1.0348844528198242,
      "average_nll": 1.0348844528198242,
      "perplexity": 2.8147809768283727,
      "avg_token_probability": 0.35526742870302325,
      "sequence_length": 1
    },
    {
      "example_id": "bb_7071--140/140_988015.txt#0_0",
      "dataset_type": "long",
      "response": "indigo",
      "accuracy": 0.0,
      "g_nll": 0.18885096906706167,
      "average_nll": 0.06295032302235389,
      "perplexity": 1.0649739332309365,
      "avg_token_probability": 0.9405171915729759,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1626--6/6_1391170.txt#0_1",
      "dataset_type": "long",
      "response": "playboy",
      "accuracy": 1.0,
      "g_nll": 0.01816880441037938,
      "average_nll": 0.00908440220518969,
      "perplexity": 1.0091257906216402,
      "avg_token_probability": 0.990995873134892,
      "sequence_length": 2
    },
    {
      "example_id": "odql_4032--121/121_112363.txt#0_0",
      "dataset_type": "long",
      "response": "dairy",
      "accuracy": 0.0,
      "g_nll": 1.4181500971317291,
      "average_nll": 0.7090750485658646,
      "perplexity": 2.032110785322363,
      "avg_token_probability": 0.5834428098516675,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3803--102/102_234526.txt#0_0",
      "dataset_type": "long",
      "response": "roissy-charles de gaulle",
      "accuracy": 1.0,
      "g_nll": 1.2533523419815538,
      "average_nll": 0.13926137133128375,
      "perplexity": 1.149424487354044,
      "avg_token_probability": 0.8830465469909545,
      "sequence_length": 9
    },
    {
      "example_id": "wh_2348--155/155_1799804.txt#0_0",
      "dataset_type": "long",
      "response": "gower peninsula",
      "accuracy": 1.0,
      "g_nll": 4.106535792309842,
      "average_nll": 1.3688452641032807,
      "perplexity": 3.9308090274744276,
      "avg_token_probability": 0.5405129617690007,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_20420--125/125_1922602.txt#0_0",
      "dataset_type": "long",
      "response": "wisconsin",
      "accuracy": 1.0,
      "g_nll": 0.057703700334968744,
      "average_nll": 0.019234566778322915,
      "perplexity": 1.0194207428139315,
      "avg_token_probability": 0.9810541507233572,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9826--160/160_662376.txt#0_0",
      "dataset_type": "long",
      "response": "the lady of the lake",
      "accuracy": 0.0,
      "g_nll": 0.31290467089093,
      "average_nll": 0.05215077848182167,
      "perplexity": 1.0535345808830119,
      "avg_token_probability": 0.95375395685789,
      "sequence_length": 6
    },
    {
      "example_id": "qb_4219--118/118_12770.txt#0_0",
      "dataset_type": "long",
      "response": "Neville Chamberlain",
      "accuracy": 0.0,
      "g_nll": 1.3424093592739155,
      "average_nll": 0.33560233981847887,
      "perplexity": 1.398782674026923,
      "avg_token_probability": 0.7979732367049437,
      "sequence_length": 4
    },
    {
      "example_id": "qg_952--87/87_3215096.txt#0_0",
      "dataset_type": "long",
      "response": "the dwarfs",
      "accuracy": 0.0,
      "g_nll": 4.537894248962402,
      "average_nll": 4.537894248962402,
      "perplexity": 93.49371819546687,
      "avg_token_probability": 0.01069590577100918,
      "sequence_length": 1
    },
    {
      "example_id": "qb_2899--155/155_2893117.txt#0_2",
      "dataset_type": "long",
      "response": "louis XV",
      "accuracy": 1.0,
      "g_nll": 2.1661809543147683,
      "average_nll": 1.0830904771573842,
      "perplexity": 2.9537940928410347,
      "avg_token_probability": 0.5527789647569807,
      "sequence_length": 2
    },
    {
      "example_id": "qw_9374--152/152_1224920.txt#0_1",
      "dataset_type": "long",
      "response": "coffee house",
      "accuracy": 1.0,
      "g_nll": 0.25255473889410496,
      "average_nll": 0.08418491296470165,
      "perplexity": 1.0878300290877627,
      "avg_token_probability": 0.9222544742029655,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22602--23/23_1970054.txt#0_0",
      "dataset_type": "long",
      "response": "bubba",
      "accuracy": 1.0,
      "g_nll": 0.20479396358086888,
      "average_nll": 0.06826465452695629,
      "perplexity": 1.0706486229917407,
      "avg_token_probability": 0.9361018186862641,
      "sequence_length": 3
    },
    {
      "example_id": "qg_207--Stockholm_syndrome.txt#0_2",
      "dataset_type": "long",
      "response": "stockholm syndrome",
      "accuracy": 1.0,
      "g_nll": 0.3107885420322418,
      "average_nll": 0.3107885420322418,
      "perplexity": 1.3645006561417952,
      "avg_token_probability": 0.7328688304390838,
      "sequence_length": 1
    },
    {
      "example_id": "qb_2850--83/83_346130.txt#0_1",
      "dataset_type": "long",
      "response": "the times",
      "accuracy": 1.0,
      "g_nll": 0.23316133621847257,
      "average_nll": 0.07772044540615752,
      "perplexity": 1.080820468105693,
      "avg_token_probability": 0.9282726742813674,
      "sequence_length": 3
    },
    {
      "example_id": "tc_1323--96/96_39278.txt#0_4",
      "dataset_type": "long",
      "response": "vienna",
      "accuracy": 1.0,
      "g_nll": 0.048171756130614085,
      "average_nll": 0.01605725204353803,
      "perplexity": 1.0161868625151953,
      "avg_token_probability": 0.9841776368787749,
      "sequence_length": 3
    },
    {
      "example_id": "wh_2204--68/68_777602.txt#0_0",
      "dataset_type": "long",
      "response": "paddy ashdown",
      "accuracy": 1.0,
      "g_nll": 0.45534880568561675,
      "average_nll": 0.09106976113712335,
      "perplexity": 1.0953454151349171,
      "avg_token_probability": 0.9239350515407747,
      "sequence_length": 5
    },
    {
      "example_id": "odql_2749--117/117_545479.txt#0_1",
      "dataset_type": "long",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.034882025791603155,
      "average_nll": 0.011627341930534385,
      "perplexity": 1.0116952022273027,
      "avg_token_probability": 0.9885135970516744,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_17026--190/190_2777940.txt#0_0",
      "dataset_type": "long",
      "response": "virtual",
      "accuracy": 1.0,
      "g_nll": 0.034283599932678044,
      "average_nll": 0.017141799966339022,
      "perplexity": 1.0172895637241794,
      "avg_token_probability": 0.9831197213659832,
      "sequence_length": 2
    },
    {
      "example_id": "qb_1032--189/189_293644.txt#0_1",
      "dataset_type": "long",
      "response": "squash",
      "accuracy": 1.0,
      "g_nll": 0.03399102235562168,
      "average_nll": 0.01699551117781084,
      "perplexity": 1.0171407565509958,
      "avg_token_probability": 0.9832836494000623,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_19691--43/43_2784025.txt#0_1",
      "dataset_type": "long",
      "response": "tequila",
      "accuracy": 1.0,
      "g_nll": 0.3046186674037017,
      "average_nll": 0.1015395558012339,
      "perplexity": 1.106873700802106,
      "avg_token_probability": 0.9090987288060638,
      "sequence_length": 3
    },
    {
      "example_id": "odql_5063--41/41_1050220.txt#0_1",
      "dataset_type": "long",
      "response": "phaethon",
      "accuracy": 0.0,
      "g_nll": 0.17149703436734853,
      "average_nll": 0.04287425859183713,
      "perplexity": 1.0438066368821333,
      "avg_token_probability": 0.9592470346731075,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1022--144/144_7564.txt#0_1",
      "dataset_type": "long",
      "response": "Adolf Hitler",
      "accuracy": 1.0,
      "g_nll": 2.3348615147406235,
      "average_nll": 0.7782871715802079,
      "perplexity": 2.177738975959799,
      "avg_token_probability": 0.6830360893086574,
      "sequence_length": 3
    },
    {
      "example_id": "qz_6774--13/13_1663667.txt#0_0",
      "dataset_type": "long",
      "response": "lithium",
      "accuracy": 1.0,
      "g_nll": 0.07423304207623005,
      "average_nll": 0.037116521038115025,
      "perplexity": 1.0378139409509215,
      "avg_token_probability": 0.9639748859400712,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3084--56/56_1290885.txt#0_1",
      "dataset_type": "long",
      "response": "montana",
      "accuracy": 1.0,
      "g_nll": 3.761761140660383,
      "average_nll": 1.2539203802201275,
      "perplexity": 3.5040532862446874,
      "avg_token_probability": 0.6688949499541782,
      "sequence_length": 3
    },
    {
      "example_id": "tc_1702--106/106_2596452.txt#0_1",
      "dataset_type": "long",
      "response": "le",
      "accuracy": 0.0,
      "g_nll": 0.12345273792743683,
      "average_nll": 0.12345273792743683,
      "perplexity": 1.1313965311335044,
      "avg_token_probability": 0.8838634134737331,
      "sequence_length": 1
    },
    {
      "example_id": "jp_3674--119/119_1039262.txt#0_0",
      "dataset_type": "long",
      "response": "vienna",
      "accuracy": 0.0,
      "g_nll": 0.04618497476258199,
      "average_nll": 0.015394991587527329,
      "perplexity": 1.0155141049351935,
      "avg_token_probability": 0.98480922292672,
      "sequence_length": 3
    },
    {
      "example_id": "qb_7766--9/9_891153.txt#0_2",
      "dataset_type": "long",
      "response": "troilus and criseydes",
      "accuracy": 0.0,
      "g_nll": 8.206433449103315,
      "average_nll": 0.9118259387892572,
      "perplexity": 2.4888628982161967,
      "avg_token_probability": 0.7079312316394257,
      "sequence_length": 9
    },
    {
      "example_id": "sfq_2345--98/98_1465999.txt#0_0",
      "dataset_type": "long",
      "response": "jaguar",
      "accuracy": 0.0,
      "g_nll": 0.21967697986838175,
      "average_nll": 0.07322565995612725,
      "perplexity": 1.0759733136119276,
      "avg_token_probability": 0.9316388543504622,
      "sequence_length": 3
    },
    {
      "example_id": "odql_13192--6/6_701893.txt#0_1",
      "dataset_type": "long",
      "response": "gettysburg",
      "accuracy": 1.0,
      "g_nll": 0.03974378071455931,
      "average_nll": 0.009935945178639827,
      "perplexity": 1.0099854705732345,
      "avg_token_probability": 0.9902579701421514,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2499--34/34_75500.txt#0_1",
      "dataset_type": "long",
      "response": "bulgarian",
      "accuracy": 1.0,
      "g_nll": 0.27084527671649994,
      "average_nll": 0.06771131917912498,
      "perplexity": 1.0700563591389067,
      "avg_token_probability": 0.9395520525059686,
      "sequence_length": 4
    },
    {
      "example_id": "odql_12482--151/151_684649.txt#0_2",
      "dataset_type": "long",
      "response": "philippine",
      "accuracy": 0.0,
      "g_nll": 3.895863562822342,
      "average_nll": 0.9739658907055855,
      "perplexity": 2.648427031395464,
      "avg_token_probability": 0.6417109973003922,
      "sequence_length": 4
    },
    {
      "example_id": "jp_3074--10/10_725914.txt#0_0",
      "dataset_type": "long",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.12908812983232565,
      "average_nll": 0.043029376610775216,
      "perplexity": 1.043968562658279,
      "avg_token_probability": 0.9584424475157372,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_261--149/149_644071.txt#0_0",
      "dataset_type": "long",
      "response": "carpathia",
      "accuracy": 1.0,
      "g_nll": 0.04583133748747059,
      "average_nll": 0.011457834371867648,
      "perplexity": 1.0115237267770614,
      "avg_token_probability": 0.9886825237260912,
      "sequence_length": 4
    },
    {
      "example_id": "bb_1013--62/62_852710.txt#0_2",
      "dataset_type": "long",
      "response": "valparaiso",
      "accuracy": 0.0,
      "g_nll": 2.0015917286691547,
      "average_nll": 0.40031834573383096,
      "perplexity": 1.4922996892711562,
      "avg_token_probability": 0.7858380980628683,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_22341--192/192_1964271.txt#0_0",
      "dataset_type": "long",
      "response": "griffin",
      "accuracy": 1.0,
      "g_nll": 0.35582328541204333,
      "average_nll": 0.11860776180401444,
      "perplexity": 1.1259281996187214,
      "avg_token_probability": 0.8982043700795606,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1590--168/168_763549.txt#0",
      "dataset_type": "long",
      "response": "knickerbocker glory",
      "accuracy": 1.0,
      "g_nll": 0.054072462587100745,
      "average_nll": 0.009012077097850124,
      "perplexity": 1.0090528081297794,
      "avg_token_probability": 0.9911281487653377,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_19632--8/8_1905614.txt#0_1",
      "dataset_type": "long",
      "response": "jim grant",
      "accuracy": 0.0,
      "g_nll": 1.1184744061902165,
      "average_nll": 0.3728248020634055,
      "perplexity": 1.4518299599367426,
      "avg_token_probability": 0.7591815382244459,
      "sequence_length": 3
    },
    {
      "example_id": "qb_4246--21/21_114528.txt#0_0",
      "dataset_type": "long",
      "response": "the thames",
      "accuracy": 1.0,
      "g_nll": 2.7786219865082558,
      "average_nll": 0.6946554966270639,
      "perplexity": 2.003018908295842,
      "avg_token_probability": 0.6544082553973314,
      "sequence_length": 4
    },
    {
      "example_id": "wh_3949--72/72_779517.txt#0_0",
      "dataset_type": "long",
      "response": "argentina",
      "accuracy": 1.0,
      "g_nll": 0.06531819843803532,
      "average_nll": 0.02177273281267844,
      "perplexity": 1.022011488398422,
      "avg_token_probability": 0.9787503220451653,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_10974--173/173_3209699.txt#0_0",
      "dataset_type": "long",
      "response": "robert kitchener",
      "accuracy": 0.0,
      "g_nll": 3.0032422919175588,
      "average_nll": 0.6006484583835118,
      "perplexity": 1.8233007517841042,
      "avg_token_probability": 0.7738900915783063,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_10250--131/131_472528.txt#0_2",
      "dataset_type": "long",
      "response": "dragon",
      "accuracy": 1.0,
      "g_nll": 0.17830584943294525,
      "average_nll": 0.08915292471647263,
      "perplexity": 1.0932478281578073,
      "avg_token_probability": 0.9149220701147858,
      "sequence_length": 2
    },
    {
      "example_id": "qf_1735--151/151_264049.txt#0_1",
      "dataset_type": "long",
      "response": "1948",
      "accuracy": 1.0,
      "g_nll": 0.46394783318100963,
      "average_nll": 0.1546492777270032,
      "perplexity": 1.167248509257062,
      "avg_token_probability": 0.8671716669081633,
      "sequence_length": 3
    },
    {
      "example_id": "bt_2905--92/92_2418244.txt#0_0",
      "dataset_type": "long",
      "response": "Sweet William",
      "accuracy": 1.0,
      "g_nll": 4.468754740431905,
      "average_nll": 1.4895849134773016,
      "perplexity": 4.435254122649645,
      "avg_token_probability": 0.646548560318673,
      "sequence_length": 3
    },
    {
      "example_id": "jp_2608--196/196_1416223.txt#0_2",
      "dataset_type": "long",
      "response": "puff the magic dragon",
      "accuracy": 1.0,
      "g_nll": 0.4633410047972575,
      "average_nll": 0.0926682009594515,
      "perplexity": 1.0970976589207129,
      "avg_token_probability": 0.9155536557362838,
      "sequence_length": 5
    },
    {
      "example_id": "odql_11066--48/48_1410700.txt#0_2",
      "dataset_type": "long",
      "response": "cello",
      "accuracy": 1.0,
      "g_nll": 0.08818016014993191,
      "average_nll": 0.02939338671664397,
      "perplexity": 1.029829636100158,
      "avg_token_probability": 0.971471907257852,
      "sequence_length": 3
    },
    {
      "example_id": "odql_14378--126/126_2345574.txt#0_0",
      "dataset_type": "long",
      "response": "tennessee",
      "accuracy": 1.0,
      "g_nll": 0.20177655450243037,
      "average_nll": 0.06725885150081012,
      "perplexity": 1.0695723027406168,
      "avg_token_probability": 0.9374324169462788,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_2677--78/78_1954583.txt#0_2",
      "dataset_type": "long",
      "response": "keswick",
      "accuracy": 1.0,
      "g_nll": 0.0538268289503776,
      "average_nll": 0.017942276316792533,
      "perplexity": 1.01810420596893,
      "avg_token_probability": 0.9824568123715544,
      "sequence_length": 3
    },
    {
      "example_id": "wh_51--126/126_725818.txt#0_1",
      "dataset_type": "long",
      "response": "avro",
      "accuracy": 1.0,
      "g_nll": 0.03649540245305616,
      "average_nll": 0.01824770122652808,
      "perplexity": 1.018415207845582,
      "avg_token_probability": 0.98208122488717,
      "sequence_length": 2
    },
    {
      "example_id": "tc_453--150/150_2867003.txt#0_0",
      "dataset_type": "long",
      "response": "God",
      "accuracy": 0.0,
      "g_nll": 7.6544870138168335,
      "average_nll": 3.8272435069084167,
      "perplexity": 45.93574200254521,
      "avg_token_probability": 0.485472445853423,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3234--33/33_1767595.txt#0_0",
      "dataset_type": "long",
      "response": "denmark",
      "accuracy": 0.0,
      "g_nll": 1.4763707704832996,
      "average_nll": 0.49212359016109986,
      "perplexity": 1.635786273801927,
      "avg_token_probability": 0.7358787212414691,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23017--20/20_341916.txt#0_0",
      "dataset_type": "long",
      "response": "will carling",
      "accuracy": 1.0,
      "g_nll": 0.052171569317522426,
      "average_nll": 0.013042892329380606,
      "perplexity": 1.013128321861556,
      "avg_token_probability": 0.9871226091984713,
      "sequence_length": 4
    },
    {
      "example_id": "odql_11582--149/149_524763.txt#0_0",
      "dataset_type": "long",
      "response": "ecuador",
      "accuracy": 1.0,
      "g_nll": 0.06327353231424127,
      "average_nll": 0.015818383078560316,
      "perplexity": 1.0159441559998035,
      "avg_token_probability": 0.9845634127169534,
      "sequence_length": 4
    },
    {
      "example_id": "qf_1622--114/114_2391205.txt#0_0",
      "dataset_type": "long",
      "response": "fred trueman",
      "accuracy": 1.0,
      "g_nll": 0.12112345327750518,
      "average_nll": 0.024224690655501036,
      "perplexity": 1.024520492211613,
      "avg_token_probability": 0.9764634505243487,
      "sequence_length": 5
    },
    {
      "example_id": "dpql_4731--151/151_679982.txt#0_0",
      "dataset_type": "long",
      "response": "blanket and honeycomb",
      "accuracy": 0.0,
      "g_nll": 4.062903795034799,
      "average_nll": 0.8125807590069598,
      "perplexity": 2.2537167877937674,
      "avg_token_probability": 0.6546585102949141,
      "sequence_length": 5
    },
    {
      "example_id": "qw_10923--40/40_1181284.txt#0_1",
      "dataset_type": "long",
      "response": "adrian edmondson",
      "accuracy": 1.0,
      "g_nll": 0.09341040814513235,
      "average_nll": 0.015568401357522058,
      "perplexity": 1.0156902202721754,
      "avg_token_probability": 0.9847241272504151,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_10035--99/99_1692646.txt#0_1",
      "dataset_type": "long",
      "response": "Eisenhower",
      "accuracy": 1.0,
      "g_nll": 0.4386814534664154,
      "average_nll": 0.2193407267332077,
      "perplexity": 1.2452554962492002,
      "avg_token_probability": 0.813312537920321,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_7787--39/39_1643620.txt#0_1",
      "dataset_type": "long",
      "response": "michael hordern",
      "accuracy": 1.0,
      "g_nll": 0.1305954932718123,
      "average_nll": 0.02611909865436246,
      "perplexity": 1.0264631915786726,
      "avg_token_probability": 0.9744672614559196,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_10817--189/189_1711448.txt#0_0",
      "dataset_type": "long",
      "response": "sculpture",
      "accuracy": 1.0,
      "g_nll": 0.050255388021469116,
      "average_nll": 0.025127694010734558,
      "perplexity": 1.0254460554840625,
      "avg_token_probability": 0.9751853807969539,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5141--181/181_554365.txt#0_0",
      "dataset_type": "long",
      "response": "st pauls",
      "accuracy": 0.0,
      "g_nll": 0.20546844601631165,
      "average_nll": 0.06848948200543721,
      "perplexity": 1.0708893612832646,
      "avg_token_probability": 0.9344856956094972,
      "sequence_length": 3
    },
    {
      "example_id": "jp_4045--160/160_2995038.txt#0_0",
      "dataset_type": "long",
      "response": "box jellyfish",
      "accuracy": 1.0,
      "g_nll": 0.35671569185797125,
      "average_nll": 0.08917892296449281,
      "perplexity": 1.0932762510554623,
      "avg_token_probability": 0.9229994990425451,
      "sequence_length": 4
    },
    {
      "example_id": "odql_10047--123/123_2824506.txt#0_1",
      "dataset_type": "long",
      "response": "Federal Reserve",
      "accuracy": 1.0,
      "g_nll": 3.927645431831479,
      "average_nll": 1.3092151439438264,
      "perplexity": 3.703266040528064,
      "avg_token_probability": 0.4303099360436333,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9872--171/171_1689203.txt#0_0",
      "dataset_type": "long",
      "response": "south africa",
      "accuracy": 1.0,
      "g_nll": 2.2132136709988117,
      "average_nll": 0.7377378903329372,
      "perplexity": 2.0911996370620596,
      "avg_token_probability": 0.6761362236069952,
      "sequence_length": 3
    },
    {
      "example_id": "bb_1886--3/3_874010.txt#0_1",
      "dataset_type": "long",
      "response": "kyoto protocol",
      "accuracy": 1.0,
      "g_nll": 0.9272869955725582,
      "average_nll": 0.23182174889313956,
      "perplexity": 1.2608949528746587,
      "avg_token_probability": 0.8283269737227331,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_12025--58/58_1700414.txt#0_2",
      "dataset_type": "long",
      "response": "127 hours",
      "accuracy": 1.0,
      "g_nll": 0.04833844004315324,
      "average_nll": 0.01208461001078831,
      "perplexity": 1.0121579239360983,
      "avg_token_probability": 0.9881562835474623,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_4010--66/66_323066.txt#0_0",
      "dataset_type": "long",
      "response": "archery",
      "accuracy": 1.0,
      "g_nll": 0.007011050533037633,
      "average_nll": 0.0023370168443458774,
      "perplexity": 1.0023397497967819,
      "avg_token_probability": 0.9976695935580348,
      "sequence_length": 3
    },
    {
      "example_id": "qf_3745--59/59_2512300.txt#0_2",
      "dataset_type": "long",
      "response": "roger kahn",
      "accuracy": 0.0,
      "g_nll": 0.8374610328974086,
      "average_nll": 0.16749220657948172,
      "perplexity": 1.1823360757067838,
      "avg_token_probability": 0.8754125921532117,
      "sequence_length": 5
    },
    {
      "example_id": "bb_7851--165/165_1006156.txt#0_0",
      "dataset_type": "long",
      "response": "1967",
      "accuracy": 1.0,
      "g_nll": 0.04348994406245765,
      "average_nll": 0.010872486015614413,
      "perplexity": 1.0109318062830261,
      "avg_token_probability": 0.9893590800740886,
      "sequence_length": 4
    },
    {
      "example_id": "qb_7077--183/183_462795.txt#0_1",
      "dataset_type": "long",
      "response": "snooker",
      "accuracy": 1.0,
      "g_nll": 0.023578962398460135,
      "average_nll": 0.005894740599615034,
      "perplexity": 1.005912148771727,
      "avg_token_probability": 0.9941425044854577,
      "sequence_length": 4
    },
    {
      "example_id": "odql_15005--151/151_2357401.txt#0_2",
      "dataset_type": "long",
      "response": "bulgaria",
      "accuracy": 1.0,
      "g_nll": 0.9203939289163827,
      "average_nll": 0.23009848222909568,
      "perplexity": 1.258723965767619,
      "avg_token_probability": 0.8464746773144936,
      "sequence_length": 4
    },
    {
      "example_id": "qb_4987--156/156_405376.txt#0_2",
      "dataset_type": "long",
      "response": "cyprus",
      "accuracy": 1.0,
      "g_nll": 0.04691265965675484,
      "average_nll": 0.01563755321891828,
      "perplexity": 1.0157604595700702,
      "avg_token_probability": 0.9846806276721001,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8918--129/129_512440.txt#0_1",
      "dataset_type": "long",
      "response": "the grapes of wrath",
      "accuracy": 1.0,
      "g_nll": 0.10844684446419706,
      "average_nll": 0.02168936889283941,
      "perplexity": 1.0219262930657864,
      "avg_token_probability": 0.9791325488525381,
      "sequence_length": 5
    },
    {
      "example_id": "bb_6062--81/81_2682584.txt#0_0",
      "dataset_type": "long",
      "response": "ozone",
      "accuracy": 1.0,
      "g_nll": 0.04616142623126507,
      "average_nll": 0.023080713115632534,
      "perplexity": 1.0233491339109262,
      "avg_token_probability": 0.9771887618294499,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_5545--199/199_1592717.txt#0_0",
      "dataset_type": "long",
      "response": "solzhenitsyn",
      "accuracy": 1.0,
      "g_nll": 0.32055784781823604,
      "average_nll": 0.05342630796970601,
      "perplexity": 1.0548792527094049,
      "avg_token_probability": 0.9520672204660795,
      "sequence_length": 6
    },
    {
      "example_id": "qw_8941--0/0_1217055.txt#0_2",
      "dataset_type": "long",
      "response": "philadelphia eagles",
      "accuracy": 1.0,
      "g_nll": 0.7210955810729729,
      "average_nll": 0.14421911621459457,
      "perplexity": 1.1551371900851932,
      "avg_token_probability": 0.8883535545814167,
      "sequence_length": 5
    },
    {
      "example_id": "qw_11443--84/84_2975478.txt#0_0",
      "dataset_type": "long",
      "response": "peach",
      "accuracy": 0.0,
      "g_nll": 0.05423265910940245,
      "average_nll": 0.027116329554701224,
      "perplexity": 1.0274873229542298,
      "avg_token_probability": 0.9735895471225211,
      "sequence_length": 2
    },
    {
      "example_id": "odql_1378--196/196_2103697.txt#0_2",
      "dataset_type": "long",
      "response": "a stupa",
      "accuracy": 0.0,
      "g_nll": 2.2253881786017473,
      "average_nll": 0.5563470446504368,
      "perplexity": 1.7442890381703242,
      "avg_token_probability": 0.69398976894201,
      "sequence_length": 4
    },
    {
      "example_id": "qg_633--116/116_2857569.txt#0_1",
      "dataset_type": "long",
      "response": "dvorak",
      "accuracy": 1.0,
      "g_nll": 2.0005663788124366,
      "average_nll": 0.5001415947031091,
      "perplexity": 1.6489547374273985,
      "avg_token_probability": 0.7715934831145618,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_4703--146/146_2653628.txt#0_0",
      "dataset_type": "long",
      "response": "the apprentice",
      "accuracy": 1.0,
      "g_nll": 0.09987435792572796,
      "average_nll": 0.03329145264190932,
      "perplexity": 1.0338518141780697,
      "avg_token_probability": 0.9677167101880512,
      "sequence_length": 3
    },
    {
      "example_id": "qb_4415--4/4_390363.txt#0_2",
      "dataset_type": "long",
      "response": "margot",
      "accuracy": 1.0,
      "g_nll": 0.031442129460629076,
      "average_nll": 0.010480709820209691,
      "perplexity": 1.0105358248392637,
      "avg_token_probability": 0.9896005056895092,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_80--166/166_2439219.txt#0_0",
      "dataset_type": "long",
      "response": "angela rippon",
      "accuracy": 1.0,
      "g_nll": 0.04757488615712191,
      "average_nll": 0.009514977231424382,
      "perplexity": 1.0095603885422024,
      "avg_token_probability": 0.9906574765198937,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_6611--38/38_1616659.txt#0_0",
      "dataset_type": "long",
      "response": "brothers and sisters",
      "accuracy": 0.0,
      "g_nll": 1.1686649583280087,
      "average_nll": 0.29216623958200216,
      "perplexity": 1.339325648069686,
      "avg_token_probability": 0.7696525030008603,
      "sequence_length": 4
    },
    {
      "example_id": "qg_2646--198/198_2188674.txt#0_0",
      "dataset_type": "long",
      "response": "mongols",
      "accuracy": 0.0,
      "g_nll": 0.10251833125948906,
      "average_nll": 0.03417277708649635,
      "perplexity": 1.0347633746852611,
      "avg_token_probability": 0.9665596097838715,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8597--105/105_3212945.txt#0_0",
      "dataset_type": "long",
      "response": "bassoon",
      "accuracy": 1.0,
      "g_nll": 0.3250601272447966,
      "average_nll": 0.10835337574826553,
      "perplexity": 1.114441492408324,
      "avg_token_probability": 0.9062355942528114,
      "sequence_length": 3
    },
    {
      "example_id": "qg_1091--8/8_3215147.txt#0_2",
      "dataset_type": "long",
      "response": "Mrs. Peacock",
      "accuracy": 0.0,
      "g_nll": 3.2044300399220447,
      "average_nll": 0.640886007984409,
      "perplexity": 1.8981619211045533,
      "avg_token_probability": 0.7486215168624897,
      "sequence_length": 5
    },
    {
      "example_id": "qw_2021--84/84_31075.txt#0_1",
      "dataset_type": "long",
      "response": "gagarin",
      "accuracy": 1.0,
      "g_nll": 0.2871626317373739,
      "average_nll": 0.07179065793434347,
      "perplexity": 1.0744303970404772,
      "avg_token_probability": 0.9317685771588147,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_3826--197/197_1325980.txt#0_0",
      "dataset_type": "long",
      "response": "dalziel and pascoe",
      "accuracy": 1.0,
      "g_nll": 0.22862719097611262,
      "average_nll": 0.038104531829352105,
      "perplexity": 1.0388398190297314,
      "avg_token_probability": 0.9649387990564073,
      "sequence_length": 6
    },
    {
      "example_id": "qb_1253--27/27_299742.txt#0_0",
      "dataset_type": "long",
      "response": "al capone",
      "accuracy": 1.0,
      "g_nll": 0.7331380545001593,
      "average_nll": 0.18328451362503984,
      "perplexity": 1.2011561047474477,
      "avg_token_probability": 0.8452928249527876,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3121--176/176_546993.txt#0_1",
      "dataset_type": "long",
      "response": "netherlands",
      "accuracy": 1.0,
      "g_nll": 0.5985594387166202,
      "average_nll": 0.19951981290554008,
      "perplexity": 1.2208163971113346,
      "avg_token_probability": 0.8428560035282606,
      "sequence_length": 3
    },
    {
      "example_id": "jp_2191--74/74_392765.txt#0_0",
      "dataset_type": "long",
      "response": "edward viii",
      "accuracy": 1.0,
      "g_nll": 0.1420190970748081,
      "average_nll": 0.028403819414961617,
      "perplexity": 1.0288110544266489,
      "avg_token_probability": 0.9722608391341184,
      "sequence_length": 5
    },
    {
      "example_id": "qw_5335--185/185_1151387.txt#0_1",
      "dataset_type": "long",
      "response": "charlie chan",
      "accuracy": 1.0,
      "g_nll": 0.06518019043141976,
      "average_nll": 0.01629504760785494,
      "perplexity": 1.0164285359769183,
      "avg_token_probability": 0.9838902442978937,
      "sequence_length": 4
    },
    {
      "example_id": "bt_1833--26/26_2841025.txt#0_0",
      "dataset_type": "long",
      "response": "lancashire",
      "accuracy": 1.0,
      "g_nll": 0.051374721428146586,
      "average_nll": 0.01712490714271553,
      "perplexity": 1.0172723789761553,
      "avg_token_probability": 0.9832585457112772,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_2269--148/148_613645.txt#0",
      "dataset_type": "long",
      "response": "drinking song",
      "accuracy": 1.0,
      "g_nll": 0.08786988165229559,
      "average_nll": 0.029289960550765198,
      "perplexity": 1.0297231302772272,
      "avg_token_probability": 0.9713098898153155,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9374--54/54_2690886.txt#0_2",
      "dataset_type": "long",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.11196037009358406,
      "average_nll": 0.05598018504679203,
      "perplexity": 1.0575767276956174,
      "avg_token_probability": 0.9455584466814371,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5117--114/114_226336.txt#0_0",
      "dataset_type": "long",
      "response": "achille lauro",
      "accuracy": 1.0,
      "g_nll": 0.11213801841699933,
      "average_nll": 0.022427603683399865,
      "perplexity": 1.022680993147369,
      "avg_token_probability": 0.9782074923715418,
      "sequence_length": 5
    },
    {
      "example_id": "odql_11538--73/73_1736878.txt#0_0",
      "dataset_type": "long",
      "response": "marshalsea",
      "accuracy": 1.0,
      "g_nll": 0.19548913083417574,
      "average_nll": 0.048872282708543935,
      "perplexity": 1.050086228006478,
      "avg_token_probability": 0.9550909604144584,
      "sequence_length": 4
    },
    {
      "example_id": "qw_8762--197/197_3207893.txt#0_1",
      "dataset_type": "long",
      "response": "a horizontal desire",
      "accuracy": 1.0,
      "g_nll": 0.33216023445129395,
      "average_nll": 0.33216023445129395,
      "perplexity": 1.3939761936287507,
      "avg_token_probability": 0.7173723658772354,
      "sequence_length": 1
    }
  ]
}