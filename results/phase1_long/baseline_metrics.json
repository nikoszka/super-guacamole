{
  "long": [
    {
      "example_id": "qg_1091--8/8_3215147.txt#0_2",
      "dataset_type": "long",
      "response": "wadsworth",
      "accuracy": 0.0,
      "g_nll": 1.358609899363728,
      "average_nll": 0.45286996645457595,
      "perplexity": 1.5728196539005035,
      "avg_token_probability": 0.7522901829138972,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1864--103/103_769441.txt#0_0",
      "dataset_type": "long",
      "response": "ringway",
      "accuracy": 1.0,
      "g_nll": 0.007938154361909255,
      "average_nll": 0.003969077180954628,
      "perplexity": 1.0039769643993293,
      "avg_token_probability": 0.9960452707445977,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3826--197/197_1325980.txt#0_0",
      "dataset_type": "long",
      "response": "dalziel and pascoe",
      "accuracy": 1.0,
      "g_nll": 0.13813840787042864,
      "average_nll": 0.027627681574085728,
      "perplexity": 1.0280128650287632,
      "avg_token_probability": 0.9741306509565415,
      "sequence_length": 5
    },
    {
      "example_id": "qb_1253--27/27_299742.txt#0_0",
      "dataset_type": "long",
      "response": "machine gun kelly",
      "accuracy": 0.0,
      "g_nll": 2.592051446431924,
      "average_nll": 0.648012861607981,
      "perplexity": 1.9117381637534527,
      "avg_token_probability": 0.7286087586507839,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3121--176/176_546993.txt#0_1",
      "dataset_type": "long",
      "response": "netherlands",
      "accuracy": 1.0,
      "g_nll": 0.5305686874780804,
      "average_nll": 0.2652843437390402,
      "perplexity": 1.3038016509126817,
      "avg_token_probability": 0.7939162637820366,
      "sequence_length": 2
    },
    {
      "example_id": "bb_7851--165/165_1006156.txt#0_0",
      "dataset_type": "long",
      "response": "1967",
      "accuracy": 1.0,
      "g_nll": 6.747131919837557e-05,
      "average_nll": 3.3735659599187784e-05,
      "perplexity": 1.0000337362286529,
      "avg_token_probability": 0.9999662649106424,
      "sequence_length": 2
    },
    {
      "example_id": "qw_5335--185/185_1151387.txt#0_1",
      "dataset_type": "long",
      "response": "charlie chan",
      "accuracy": 1.0,
      "g_nll": 0.027060953521868214,
      "average_nll": 0.009020317840622738,
      "perplexity": 1.0090611235086777,
      "avg_token_probability": 0.9910427943754643,
      "sequence_length": 3
    },
    {
      "example_id": "odql_13192--6/6_701893.txt#0_1",
      "dataset_type": "long",
      "response": "gettysburg",
      "accuracy": 1.0,
      "g_nll": 0.054203119119364374,
      "average_nll": 0.018067706373121457,
      "perplexity": 1.0182319148459318,
      "avg_token_probability": 0.9824131463996184,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_20420--125/125_1922602.txt#0_1",
      "dataset_type": "long",
      "response": "wisconsin",
      "accuracy": 1.0,
      "g_nll": 0.007920209631265607,
      "average_nll": 0.003960104815632803,
      "perplexity": 1.0039679563916417,
      "avg_token_probability": 0.9960553981980538,
      "sequence_length": 2
    },
    {
      "example_id": "bb_9374--54/54_2690886.txt#0_2",
      "dataset_type": "long",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.10590160731226206,
      "average_nll": 0.05295080365613103,
      "perplexity": 1.0543777723119356,
      "avg_token_probability": 0.9493139136408256,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11679--16/16_33164.txt#0_0",
      "dataset_type": "long",
      "response": "tasmania",
      "accuracy": 1.0,
      "g_nll": 0.26212967932224274,
      "average_nll": 0.13106483966112137,
      "perplexity": 1.1400416988329876,
      "avg_token_probability": 0.8846857336055056,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11538--73/73_1736878.txt#0_0",
      "dataset_type": "long",
      "response": "the marshalsea",
      "accuracy": 1.0,
      "g_nll": 3.981193346165128,
      "average_nll": 0.995298336541282,
      "perplexity": 2.7055313797630998,
      "avg_token_probability": 0.7544313851590758,
      "sequence_length": 4
    },
    {
      "example_id": "qw_8762--197/197_3207893.txt#0_1",
      "dataset_type": "long",
      "response": "a horizontal desire",
      "accuracy": 1.0,
      "g_nll": 0.7884215712547302,
      "average_nll": 0.2628071904182434,
      "perplexity": 1.300575931274016,
      "avg_token_probability": 0.7719838887464023,
      "sequence_length": 3
    },
    {
      "example_id": "bb_439--197/197_197490.txt#0_0",
      "dataset_type": "long",
      "response": "general belgrano",
      "accuracy": 1.0,
      "g_nll": 0.592649179190623,
      "average_nll": 0.14816229479765575,
      "perplexity": 1.1597010945452482,
      "avg_token_probability": 0.887731857305303,
      "sequence_length": 4
    },
    {
      "example_id": "bb_7724--184/184_1002731.txt#0_0",
      "dataset_type": "long",
      "response": "albatross",
      "accuracy": 0.0,
      "g_nll": 0.05469046857092508,
      "average_nll": 0.01823015619030836,
      "perplexity": 1.018397339870621,
      "avg_token_probability": 0.9822587801276489,
      "sequence_length": 3
    },
    {
      "example_id": "qw_439--187/187_1058077.txt#0_2",
      "dataset_type": "long",
      "response": "narcolepsy",
      "accuracy": 1.0,
      "g_nll": 0.002709093101657345,
      "average_nll": 0.0009030310338857817,
      "perplexity": 1.0009034388891693,
      "avg_token_probability": 0.9990980194584184,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9384--195/195_1224988.txt#0_1",
      "dataset_type": "long",
      "response": "madagascar",
      "accuracy": 1.0,
      "g_nll": 0.002285652626596857,
      "average_nll": 0.0011428263132984284,
      "perplexity": 1.0011434795881258,
      "avg_token_probability": 0.9988583971057519,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_21423--144/144_2788272.txt#0_0",
      "dataset_type": "long",
      "response": "byward tower",
      "accuracy": 0.0,
      "g_nll": 0.17472609411925077,
      "average_nll": 0.05824203137308359,
      "perplexity": 1.0599715110269015,
      "avg_token_probability": 0.9453667604482349,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_25451--37/37_2031222.txt#0_1",
      "dataset_type": "long",
      "response": "dark rum",
      "accuracy": 1.0,
      "g_nll": 0.22838434018194675,
      "average_nll": 0.11419217009097338,
      "perplexity": 1.1209675206175966,
      "avg_token_probability": 0.8977190559111672,
      "sequence_length": 2
    },
    {
      "example_id": "qf_545--48/48_2849118.txt#0_1",
      "dataset_type": "long",
      "response": "cairngorms",
      "accuracy": 1.0,
      "g_nll": 0.05475244784608435,
      "average_nll": 0.01095048956921687,
      "perplexity": 1.0110106656319808,
      "avg_token_probability": 0.989329101323599,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_668--20/20_1363097.txt#0_1",
      "dataset_type": "long",
      "response": "demi moore",
      "accuracy": 1.0,
      "g_nll": 1.197322875260852,
      "average_nll": 0.299330718815213,
      "perplexity": 1.3489556747325955,
      "avg_token_probability": 0.8212708921849772,
      "sequence_length": 4
    },
    {
      "example_id": "qw_8999--56/56_1218189.txt#0_2",
      "dataset_type": "long",
      "response": "devil in disguise",
      "accuracy": 0.0,
      "g_nll": 0.7774343527853489,
      "average_nll": 0.25914478426178295,
      "perplexity": 1.2958214057925272,
      "avg_token_probability": 0.8160191403370102,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_1377--91/91_1497797.txt#0_1",
      "dataset_type": "long",
      "response": "yarrow",
      "accuracy": 0.0,
      "g_nll": 0.05801456794137039,
      "average_nll": 0.029007283970685194,
      "perplexity": 1.0294320928010083,
      "avg_token_probability": 0.9718180901713422,
      "sequence_length": 2
    },
    {
      "example_id": "wh_4269--173/173_235742.txt#0_0",
      "dataset_type": "long",
      "response": "abraham lincoln",
      "accuracy": 1.0,
      "g_nll": 0.008507288228429388,
      "average_nll": 0.002126822057107347,
      "perplexity": 1.0021290853473928,
      "avg_token_probability": 0.9978815570133326,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_15401--88/88_1813633.txt#0_1",
      "dataset_type": "long",
      "response": "Xenophon",
      "accuracy": 1.0,
      "g_nll": 0.07496357953641564,
      "average_nll": 0.024987859845471878,
      "perplexity": 1.0253026731159824,
      "avg_token_probability": 0.9756872011935442,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_3917--169/169_1555303.txt#0_0",
      "dataset_type": "long",
      "response": "apollo",
      "accuracy": 1.0,
      "g_nll": 0.13397246599197388,
      "average_nll": 0.13397246599197388,
      "perplexity": 1.1433613378909853,
      "avg_token_probability": 0.8746141459047182,
      "sequence_length": 1
    },
    {
      "example_id": "wh_2414--73/73_48566.txt#0_0",
      "dataset_type": "long",
      "response": "tiny tim",
      "accuracy": 1.0,
      "g_nll": 0.06212856317870319,
      "average_nll": 0.031064281589351594,
      "perplexity": 1.0315518115452638,
      "avg_token_probability": 0.9698480624980996,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_11783--191/191_1732943.txt#0_1",
      "dataset_type": "long",
      "response": "chess",
      "accuracy": 1.0,
      "g_nll": 0.10136448731645942,
      "average_nll": 0.05068224365822971,
      "perplexity": 1.0519885641306903,
      "avg_token_probability": 0.9515615886806834,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2887--Lacrosse.txt#0_0",
      "dataset_type": "long",
      "response": "lacrosse stick",
      "accuracy": 1.0,
      "g_nll": 0.3665848333039321,
      "average_nll": 0.12219494443464403,
      "perplexity": 1.1299743625235825,
      "avg_token_probability": 0.8943513176750479,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_16267--E._L._James.txt#0_2",
      "dataset_type": "long",
      "response": "grey",
      "accuracy": 0.0,
      "g_nll": 0.3432861268520355,
      "average_nll": 0.3432861268520355,
      "perplexity": 1.4095720206371092,
      "avg_token_probability": 0.7094351940584153,
      "sequence_length": 1
    },
    {
      "example_id": "jp_2940--195/195_1424532.txt#0_1",
      "dataset_type": "long",
      "response": "giant",
      "accuracy": 0.0,
      "g_nll": 0.23594365175813437,
      "average_nll": 0.11797182587906718,
      "perplexity": 1.1252124090502642,
      "avg_token_probability": 0.8944578476217169,
      "sequence_length": 2
    },
    {
      "example_id": "qb_5499--156/156_419027.txt#0_0",
      "dataset_type": "long",
      "response": "jennifer lopez",
      "accuracy": 1.0,
      "g_nll": 0.3856331457933493,
      "average_nll": 0.09640828644833732,
      "perplexity": 1.10120858076225,
      "avg_token_probability": 0.9194452725358245,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_9644--123/123_186355.txt#0_2",
      "dataset_type": "long",
      "response": "iron",
      "accuracy": 1.0,
      "g_nll": 0.0012642494402825832,
      "average_nll": 0.0012642494402825832,
      "perplexity": 1.0012650489404933,
      "avg_token_probability": 0.9987365493863669,
      "sequence_length": 1
    },
    {
      "example_id": "qb_637--10/10_282457.txt#0_0",
      "dataset_type": "long",
      "response": "tiffany and co.",
      "accuracy": 1.0,
      "g_nll": 4.47444201628241,
      "average_nll": 0.894888403256482,
      "perplexity": 2.447062689996701,
      "avg_token_probability": 0.7224619757940427,
      "sequence_length": 5
    },
    {
      "example_id": "qw_11443--84/84_2975478.txt#0_0",
      "dataset_type": "long",
      "response": "peach",
      "accuracy": 0.0,
      "g_nll": 0.013570016802987084,
      "average_nll": 0.006785008401493542,
      "perplexity": 1.0068080787189149,
      "avg_token_probability": 0.9932576795055821,
      "sequence_length": 2
    },
    {
      "example_id": "jp_742--32/32_1368333.txt#0_2",
      "dataset_type": "long",
      "response": "hall",
      "accuracy": 0.0,
      "g_nll": 0.8394083976745605,
      "average_nll": 0.8394083976745605,
      "perplexity": 2.3149970139676066,
      "avg_token_probability": 0.4319659999414552,
      "sequence_length": 1
    },
    {
      "example_id": "wh_3212--72/72_800524.txt#0_2",
      "dataset_type": "long",
      "response": "the left book club",
      "accuracy": 1.0,
      "g_nll": 1.2491802984768583,
      "average_nll": 0.3122950746192146,
      "perplexity": 1.3665578700860466,
      "avg_token_probability": 0.8216309542748003,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1502--134/134_2618044.txt#0_0",
      "dataset_type": "long",
      "response": "bats",
      "accuracy": 1.0,
      "g_nll": 0.03247071523219347,
      "average_nll": 0.016235357616096735,
      "perplexity": 1.0163678671766612,
      "avg_token_probability": 0.9839928785067574,
      "sequence_length": 2
    },
    {
      "example_id": "qb_2774--81/81_303935.txt#0_0",
      "dataset_type": "long",
      "response": "the dandy",
      "accuracy": 0.0,
      "g_nll": 3.4515088269254193,
      "average_nll": 1.1505029423084732,
      "perplexity": 3.159781698023316,
      "avg_token_probability": 0.45203571596975206,
      "sequence_length": 3
    },
    {
      "example_id": "bb_1997--80/80_876537.txt#0_1",
      "dataset_type": "long",
      "response": "five-pointed star",
      "accuracy": 1.0,
      "g_nll": 1.58884936128743,
      "average_nll": 0.3972123403218575,
      "perplexity": 1.4876717892218425,
      "avg_token_probability": 0.7243085083380381,
      "sequence_length": 4
    },
    {
      "example_id": "bt_1811--193/193_2452704.txt#0_1",
      "dataset_type": "long",
      "response": "van gogh",
      "accuracy": 1.0,
      "g_nll": 0.05280549789313227,
      "average_nll": 0.01760183263104409,
      "perplexity": 1.0177576578139866,
      "avg_token_probability": 0.9828324133321592,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_5662--54/54_415167.txt#0_2",
      "dataset_type": "long",
      "response": "puck",
      "accuracy": 1.0,
      "g_nll": 0.03780014580115676,
      "average_nll": 0.01890007290057838,
      "perplexity": 1.0190798098397895,
      "avg_token_probability": 0.9813454771000477,
      "sequence_length": 2
    },
    {
      "example_id": "odql_4122--97/97_2155160.txt#0_1",
      "dataset_type": "long",
      "response": "braine",
      "accuracy": 0.0,
      "g_nll": 5.400112597970292,
      "average_nll": 2.700056298985146,
      "perplexity": 14.88056946224985,
      "avg_token_probability": 0.5005561677396166,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_16378--17/17_461209.txt#0_0",
      "dataset_type": "long",
      "response": "john dryden",
      "accuracy": 0.0,
      "g_nll": 0.21645022439133754,
      "average_nll": 0.07215007479711251,
      "perplexity": 1.0748166348490782,
      "avg_token_probability": 0.934907429734151,
      "sequence_length": 3
    },
    {
      "example_id": "qz_4241--129/129_203139.txt#0_0",
      "dataset_type": "long",
      "response": "dai green",
      "accuracy": 0.0,
      "g_nll": 2.4581947922706604,
      "average_nll": 0.8193982640902201,
      "perplexity": 2.2691340072227972,
      "avg_token_probability": 0.5236218940937051,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9677--35/35_1731921.txt#0_0",
      "dataset_type": "long",
      "response": "stalingrad",
      "accuracy": 1.0,
      "g_nll": 0.34910744419039474,
      "average_nll": 0.11636914806346492,
      "perplexity": 1.123410500409636,
      "avg_token_probability": 0.9017707033251194,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_11145--8/8_2764204.txt#0_0",
      "dataset_type": "long",
      "response": "parking meter",
      "accuracy": 1.0,
      "g_nll": 0.1951626092195511,
      "average_nll": 0.0650542030731837,
      "perplexity": 1.0672168692505606,
      "avg_token_probability": 0.9373128679790025,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_13406--148/148_983957.txt#0_0",
      "dataset_type": "long",
      "response": "paul gauguin",
      "accuracy": 1.0,
      "g_nll": 0.04615845507873928,
      "average_nll": 0.009231691015747856,
      "perplexity": 1.0092744345055662,
      "avg_token_probability": 0.9908490574759454,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_2528--156/156_1523996.txt#0_1",
      "dataset_type": "long",
      "response": "carson",
      "accuracy": 1.0,
      "g_nll": 1.6099688997783232,
      "average_nll": 0.8049844498891616,
      "perplexity": 2.2366617182119404,
      "avg_token_probability": 0.5999299386059636,
      "sequence_length": 2
    },
    {
      "example_id": "qf_2575--20/20_3214656.txt#0_0",
      "dataset_type": "long",
      "response": "meteor",
      "accuracy": 0.0,
      "g_nll": 0.025837041437625885,
      "average_nll": 0.025837041437625885,
      "perplexity": 1.026173711054728,
      "avg_token_probability": 0.9744938787918997,
      "sequence_length": 1
    },
    {
      "example_id": "qf_478--194/194_393733.txt#0_1",
      "dataset_type": "long",
      "response": "james cameron",
      "accuracy": 1.0,
      "g_nll": 0.05791842983398965,
      "average_nll": 0.014479607458497412,
      "perplexity": 1.0145849447748425,
      "avg_token_probability": 0.9858045269395501,
      "sequence_length": 4
    },
    {
      "example_id": "qf_3745--59/59_2512300.txt#0_2",
      "dataset_type": "long",
      "response": "roger kahn",
      "accuracy": 0.0,
      "g_nll": 0.9609694506434607,
      "average_nll": 0.24024236266086518,
      "perplexity": 1.2715572909874868,
      "avg_token_probability": 0.8296855553744215,
      "sequence_length": 4
    },
    {
      "example_id": "qf_1492--14/14_2475412.txt#0_0",
      "dataset_type": "long",
      "response": "alamo",
      "accuracy": 1.0,
      "g_nll": 1.461428761096613,
      "average_nll": 0.7307143805483065,
      "perplexity": 2.0765635345203934,
      "avg_token_probability": 0.6159416839191794,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1391--61/61_1385217.txt#0_1",
      "dataset_type": "long",
      "response": "monotremes",
      "accuracy": 0.0,
      "g_nll": 0.13593515007551105,
      "average_nll": 0.03398378751887776,
      "perplexity": 1.0345678336806787,
      "avg_token_probability": 0.9673518518836521,
      "sequence_length": 4
    },
    {
      "example_id": "qz_2787--60/60_167773.txt#0_2",
      "dataset_type": "long",
      "response": "toya",
      "accuracy": 1.0,
      "g_nll": 3.3361110687254723,
      "average_nll": 1.6680555343627361,
      "perplexity": 5.3018485054456495,
      "avg_token_probability": 0.5177872890297636,
      "sequence_length": 2
    },
    {
      "example_id": "qw_9299--131/131_1223632.txt#0_1",
      "dataset_type": "long",
      "response": "thor",
      "accuracy": 1.0,
      "g_nll": 0.07753815595060587,
      "average_nll": 0.038769077975302935,
      "perplexity": 1.0395304054655903,
      "avg_token_probability": 0.9623320592586277,
      "sequence_length": 2
    },
    {
      "example_id": "bb_6957--37/37_265768.txt#0_0",
      "dataset_type": "long",
      "response": "40",
      "accuracy": 1.0,
      "g_nll": 0.01679987460374832,
      "average_nll": 0.01679987460374832,
      "perplexity": 1.0169417860816299,
      "avg_token_probability": 0.9833404563432209,
      "sequence_length": 1
    },
    {
      "example_id": "bt_1811--193/193_2452704.txt#0_2",
      "dataset_type": "long",
      "response": "van gogh",
      "accuracy": 1.0,
      "g_nll": 0.047119306509557646,
      "average_nll": 0.015706435503185883,
      "perplexity": 1.0158304298806238,
      "avg_token_probability": 0.9845676408921625,
      "sequence_length": 3
    },
    {
      "example_id": "qg_3088--142/142_2565750.txt#0_1",
      "dataset_type": "long",
      "response": "tigon",
      "accuracy": 0.0,
      "g_nll": 0.10292821703478694,
      "average_nll": 0.05146410851739347,
      "perplexity": 1.0528113986520076,
      "avg_token_probability": 0.9508063180022878,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_80--166/166_2439219.txt#0_0",
      "dataset_type": "long",
      "response": "angela rippon",
      "accuracy": 1.0,
      "g_nll": 0.006903791427191663,
      "average_nll": 0.0017259478567979158,
      "perplexity": 1.001727438162073,
      "avg_token_probability": 0.9982796537437862,
      "sequence_length": 4
    },
    {
      "example_id": "qb_9469--195/195_527460.txt#0_2",
      "dataset_type": "long",
      "response": "lilac",
      "accuracy": 1.0,
      "g_nll": 0.08366413728799671,
      "average_nll": 0.041832068643998355,
      "perplexity": 1.0427193587712593,
      "avg_token_probability": 0.9598445283752826,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1414--178/178_269111.txt#0_1",
      "dataset_type": "long",
      "response": "bushfires",
      "accuracy": 1.0,
      "g_nll": 0.41966006136499345,
      "average_nll": 0.1398866871216645,
      "perplexity": 1.1501434654065734,
      "avg_token_probability": 0.8843735054895251,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8182--60/60_558286.txt#0_0",
      "dataset_type": "long",
      "response": "smell",
      "accuracy": 1.0,
      "g_nll": 0.048550275387242436,
      "average_nll": 0.024275137693621218,
      "perplexity": 1.0245721775396133,
      "avg_token_probability": 0.9762268366416038,
      "sequence_length": 2
    },
    {
      "example_id": "qw_5549--67/67_1155062.txt#0_1",
      "dataset_type": "long",
      "response": "warp threads",
      "accuracy": 1.0,
      "g_nll": 2.032168061938137,
      "average_nll": 0.6773893539793789,
      "perplexity": 1.968731356788806,
      "avg_token_probability": 0.6613827162474674,
      "sequence_length": 3
    },
    {
      "example_id": "qw_1265--47/47_2693885.txt#0_1",
      "dataset_type": "long",
      "response": "julius verne",
      "accuracy": 1.0,
      "g_nll": 2.874259993111991,
      "average_nll": 0.5748519986223982,
      "perplexity": 1.7768675286104125,
      "avg_token_probability": 0.7769584999542983,
      "sequence_length": 5
    },
    {
      "example_id": "dpql_3600--123/123_649287.txt#0_1",
      "dataset_type": "long",
      "response": "tara king",
      "accuracy": 1.0,
      "g_nll": 0.060564964847799274,
      "average_nll": 0.02018832161593309,
      "perplexity": 1.0203934840835585,
      "avg_token_probability": 0.9801525279259264,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5877--148/148_240409.txt#0_2",
      "dataset_type": "long",
      "response": "tartar",
      "accuracy": 1.0,
      "g_nll": 0.6190157392973106,
      "average_nll": 0.2063385797657702,
      "perplexity": 1.229169305405094,
      "avg_token_probability": 0.8449636766522635,
      "sequence_length": 3
    },
    {
      "example_id": "qw_15440--126/126_2983047.txt#0_0",
      "dataset_type": "long",
      "response": "rising fast",
      "accuracy": 0.0,
      "g_nll": 0.4855715918238275,
      "average_nll": 0.24278579591191374,
      "perplexity": 1.2747955284567307,
      "avg_token_probability": 0.807605660738994,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3129--69/69_2501904.txt#0_0",
      "dataset_type": "long",
      "response": "tom hanks",
      "accuracy": 1.0,
      "g_nll": 0.043064190715085715,
      "average_nll": 0.014354730238361904,
      "perplexity": 1.0144582541378997,
      "avg_token_probability": 0.9859458211057047,
      "sequence_length": 3
    },
    {
      "example_id": "bt_615--93/93_253274.txt#0_1",
      "dataset_type": "long",
      "response": "eddie",
      "accuracy": 1.0,
      "g_nll": 0.026998880930477753,
      "average_nll": 0.013499440465238877,
      "perplexity": 1.0135909693106664,
      "avg_token_probability": 0.9866763425672014,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7241--59/59_280648.txt#0_2",
      "dataset_type": "long",
      "response": "mars",
      "accuracy": 1.0,
      "g_nll": 0.009639502502977848,
      "average_nll": 0.009639502502977848,
      "perplexity": 1.009686112151456,
      "avg_token_probability": 0.9904068085765617,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_18907--144/144_1890291.txt#0_2",
      "dataset_type": "long",
      "response": "channel de l'engle",
      "accuracy": 0.0,
      "g_nll": 4.321014090441167,
      "average_nll": 0.7201690150735279,
      "perplexity": 2.0547804701692636,
      "avg_token_probability": 0.5984339211056775,
      "sequence_length": 6
    },
    {
      "example_id": "bt_1509--135/135_2390355.txt#0_0",
      "dataset_type": "long",
      "response": "bean bag toss",
      "accuracy": 0.0,
      "g_nll": 1.123905059415847,
      "average_nll": 0.37463501980528235,
      "perplexity": 1.4544604684666904,
      "avg_token_probability": 0.7578055154763037,
      "sequence_length": 3
    },
    {
      "example_id": "bt_2235--118/118_319041.txt#0_0",
      "dataset_type": "long",
      "response": "surses",
      "accuracy": 0.0,
      "g_nll": 7.209064483642578,
      "average_nll": 3.604532241821289,
      "perplexity": 36.76448294704003,
      "avg_token_probability": 0.16356962980736386,
      "sequence_length": 2
    },
    {
      "example_id": "bb_4368--85/85_456496.txt#0_2",
      "dataset_type": "long",
      "response": "hindi",
      "accuracy": 1.0,
      "g_nll": 0.04689887724816799,
      "average_nll": 0.023449438624083996,
      "perplexity": 1.0237265384157088,
      "avg_token_probability": 0.976846438746791,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2063--191/191_3215408.txt#0_0",
      "dataset_type": "long",
      "response": "function",
      "accuracy": 1.0,
      "g_nll": 0.006513673812150955,
      "average_nll": 0.006513673812150955,
      "perplexity": 1.0065349339208205,
      "avg_token_probability": 0.9935074941757217,
      "sequence_length": 1
    },
    {
      "example_id": "jp_3026--138/138_366909.txt#0_2",
      "dataset_type": "long",
      "response": "insects",
      "accuracy": 0.0,
      "g_nll": 0.0576183432713151,
      "average_nll": 0.02880917163565755,
      "perplexity": 1.0292281698058485,
      "avg_token_probability": 0.9717163412729167,
      "sequence_length": 2
    },
    {
      "example_id": "bb_200--50/50_834132.txt#0_2",
      "dataset_type": "long",
      "response": "goldtrail",
      "accuracy": 1.0,
      "g_nll": 0.008096951787592843,
      "average_nll": 0.004048475893796422,
      "perplexity": 1.0040566820427226,
      "avg_token_probability": 0.9959664583515155,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2820--42/42_1531177.txt#0_1",
      "dataset_type": "long",
      "response": "ali macgraw",
      "accuracy": 1.0,
      "g_nll": 0.5523406825614074,
      "average_nll": 0.13808517064035186,
      "perplexity": 1.1480733282507511,
      "avg_token_probability": 0.8841728997227122,
      "sequence_length": 4
    },
    {
      "example_id": "odql_7759--63/63_1347430.txt#0_0",
      "dataset_type": "long",
      "response": "robert",
      "accuracy": 0.0,
      "g_nll": 0.06706077186390758,
      "average_nll": 0.03353038593195379,
      "perplexity": 1.0340988653066439,
      "avg_token_probability": 0.9673962424530606,
      "sequence_length": 2
    },
    {
      "example_id": "qw_2021--84/84_31075.txt#0_1",
      "dataset_type": "long",
      "response": "gagarin",
      "accuracy": 1.0,
      "g_nll": 0.2749549299401224,
      "average_nll": 0.09165164331337412,
      "perplexity": 1.0959829625795807,
      "avg_token_probability": 0.9148504887758669,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1103--13/13_2468995.txt#0_2",
      "dataset_type": "long",
      "response": "lucille ball",
      "accuracy": 1.0,
      "g_nll": 0.044896320934640244,
      "average_nll": 0.011224080233660061,
      "perplexity": 1.0112873065532113,
      "avg_token_probability": 0.9889219759449972,
      "sequence_length": 4
    },
    {
      "example_id": "qw_7276--142/142_1187158.txt#0_1",
      "dataset_type": "long",
      "response": "usa",
      "accuracy": 0.0,
      "g_nll": 1.2802956104278564,
      "average_nll": 1.2802956104278564,
      "perplexity": 3.597703086940014,
      "avg_token_probability": 0.27795512187486787,
      "sequence_length": 1
    },
    {
      "example_id": "wh_1706--59/59_2662426.txt#0_0",
      "dataset_type": "long",
      "response": "jarhead",
      "accuracy": 1.0,
      "g_nll": 0.010101171208361848,
      "average_nll": 0.005050585604180924,
      "perplexity": 1.0050633613108648,
      "avg_token_probability": 0.9949748269572009,
      "sequence_length": 2
    },
    {
      "example_id": "qb_5952--33/33_419470.txt#0_1",
      "dataset_type": "long",
      "response": "3600",
      "accuracy": 1.0,
      "g_nll": 0.0743017599452287,
      "average_nll": 0.03715087997261435,
      "perplexity": 1.0378495997447368,
      "avg_token_probability": 0.9641809532496661,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_4557--72/72_1569959.txt#0_0",
      "dataset_type": "long",
      "response": "air traffic control",
      "accuracy": 1.0,
      "g_nll": 0.3286037838552147,
      "average_nll": 0.10953459461840491,
      "perplexity": 1.1157586695128963,
      "avg_token_probability": 0.9054286923152445,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_26163--17/17_422703.txt#0_0",
      "dataset_type": "long",
      "response": "whiff whaff",
      "accuracy": 0.0,
      "g_nll": 5.924212811258258,
      "average_nll": 1.4810532028145644,
      "perplexity": 4.397574780936816,
      "avg_token_probability": 0.5975007670433821,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_5913--103/103_1600716.txt#0_1",
      "dataset_type": "long",
      "response": "hard times",
      "accuracy": 1.0,
      "g_nll": 0.0018210792040918022,
      "average_nll": 0.0009105396020459011,
      "perplexity": 1.0009109542690766,
      "avg_token_probability": 0.9990899773783208,
      "sequence_length": 2
    },
    {
      "example_id": "odql_1809--193/193_609132.txt#0_0",
      "dataset_type": "long",
      "response": "cal",
      "accuracy": 0.0,
      "g_nll": 0.06050310656428337,
      "average_nll": 0.06050310656428337,
      "perplexity": 1.0623708978886857,
      "avg_token_probability": 0.9412908448333447,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_22957--6/6_1623582.txt#0_1",
      "dataset_type": "long",
      "response": "caravaggio",
      "accuracy": 0.0,
      "g_nll": 0.014723636747362434,
      "average_nll": 0.004907878915787478,
      "perplexity": 1.0049199422806174,
      "avg_token_probability": 0.9951278183651682,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1031--61/61_537330.txt#0_0",
      "dataset_type": "long",
      "response": "jim hacker",
      "accuracy": 1.0,
      "g_nll": 1.0867391061037779,
      "average_nll": 0.3622463687012593,
      "perplexity": 1.4365528200188815,
      "avg_token_probability": 0.7350923139206685,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8256--23/23_225768.txt#0_0",
      "dataset_type": "long",
      "response": "2004",
      "accuracy": 1.0,
      "g_nll": 0.002509051118977368,
      "average_nll": 0.001254525559488684,
      "perplexity": 1.0012553128058508,
      "avg_token_probability": 0.9987463410876034,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1299--110/110_771432.txt#0_1",
      "dataset_type": "long",
      "response": "intrusive",
      "accuracy": 0.0,
      "g_nll": 1.601821780204773,
      "average_nll": 0.8009108901023865,
      "perplexity": 2.2275690752658655,
      "avg_token_probability": 0.4495235274297668,
      "sequence_length": 2
    },
    {
      "example_id": "qf_2195--13/13_325145.txt#0_2",
      "dataset_type": "long",
      "response": "andre agassi",
      "accuracy": 1.0,
      "g_nll": 0.13976575526612578,
      "average_nll": 0.04658858508870859,
      "perplexity": 1.0476908847478381,
      "avg_token_probability": 0.9559638406827466,
      "sequence_length": 3
    },
    {
      "example_id": "qb_1400--176/176_304165.txt#0",
      "dataset_type": "long",
      "response": "arthur hailey",
      "accuracy": 1.0,
      "g_nll": 0.048068648610978926,
      "average_nll": 0.012017162152744731,
      "perplexity": 1.0120896583543364,
      "avg_token_probability": 0.9881790335813272,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_24400--153/153_1526690.txt#0_0",
      "dataset_type": "long",
      "response": "carolier",
      "accuracy": 0.0,
      "g_nll": 6.084206938743591,
      "average_nll": 2.0280689795811973,
      "perplexity": 7.599397589271906,
      "avg_token_probability": 0.19820527710982208,
      "sequence_length": 3
    },
    {
      "example_id": "odql_9310--183/183_763371.txt#0_0",
      "dataset_type": "long",
      "response": "tesco",
      "accuracy": 1.0,
      "g_nll": 0.0156674776201271,
      "average_nll": 0.00783373881006355,
      "perplexity": 1.0078645028218776,
      "avg_token_probability": 0.9922272918115607,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_3467--85/85_645957.txt#0_1",
      "dataset_type": "long",
      "response": "willie nelson",
      "accuracy": 1.0,
      "g_nll": 0.41092919157540564,
      "average_nll": 0.10273229789385141,
      "perplexity": 1.1081947033073014,
      "avg_token_probability": 0.9115221872163917,
      "sequence_length": 4
    },
    {
      "example_id": "tb_1113--48/48_567125.txt#0_1",
      "dataset_type": "long",
      "response": "man",
      "accuracy": 1.0,
      "g_nll": 0.007456211838871241,
      "average_nll": 0.007456211838871241,
      "perplexity": 1.0074840786034747,
      "avg_token_probability": 0.9925715167490797,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_18732--23/23_1886509.txt#0_0",
      "dataset_type": "long",
      "response": "spiez",
      "accuracy": 0.0,
      "g_nll": 0.6379392338276375,
      "average_nll": 0.31896961691381875,
      "perplexity": 1.3757095259699643,
      "avg_token_probability": 0.7641230255789888,
      "sequence_length": 2
    },
    {
      "example_id": "bb_5507--150/150_954054.txt#0_1",
      "dataset_type": "long",
      "response": "neon",
      "accuracy": 1.0,
      "g_nll": 0.00832439330406487,
      "average_nll": 0.004162196652032435,
      "perplexity": 1.0041708706225994,
      "avg_token_probability": 0.9958465361538613,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12482--151/151_684649.txt#0_2",
      "dataset_type": "long",
      "response": "philippines",
      "accuracy": 1.0,
      "g_nll": 0.27909876918420196,
      "average_nll": 0.13954938459210098,
      "perplexity": 1.1497555845266094,
      "avg_token_probability": 0.8777433760976424,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7146--4/4_464898.txt#0_0",
      "dataset_type": "long",
      "response": "roshi",
      "accuracy": 1.0,
      "g_nll": 0.07259594548668247,
      "average_nll": 0.036297972743341234,
      "perplexity": 1.0369647877031478,
      "avg_token_probability": 0.9649820296720609,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8955--114/114_513485.txt#0_0",
      "dataset_type": "long",
      "response": "james mason",
      "accuracy": 1.0,
      "g_nll": 0.055935960555871134,
      "average_nll": 0.013983990138967783,
      "perplexity": 1.0140822234930558,
      "avg_token_probability": 0.986272467820013,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1630--10/10_1391268.txt#0_1",
      "dataset_type": "long",
      "response": "bobby fischer",
      "accuracy": 1.0,
      "g_nll": 0.19033186507294886,
      "average_nll": 0.047582966268237215,
      "perplexity": 1.0487332069925197,
      "avg_token_probability": 0.9565762993927863,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5116--124/124_2680471.txt#0_0",
      "dataset_type": "long",
      "response": "splitting into parts",
      "accuracy": 0.0,
      "g_nll": 2.953150416491553,
      "average_nll": 0.7382876041228883,
      "perplexity": 2.0923495143628417,
      "avg_token_probability": 0.657355473537242,
      "sequence_length": 4
    },
    {
      "example_id": "odql_7016--55/55_1322268.txt#0_0",
      "dataset_type": "long",
      "response": "60s",
      "accuracy": 0.0,
      "g_nll": 3.2889363020658493,
      "average_nll": 1.6444681510329247,
      "perplexity": 5.1782551248100415,
      "avg_token_probability": 0.5012607326669039,
      "sequence_length": 2
    },
    {
      "example_id": "bb_3301--62/62_294292.txt#0_1",
      "dataset_type": "long",
      "response": "thor",
      "accuracy": 1.0,
      "g_nll": 0.08239332772791386,
      "average_nll": 0.04119666386395693,
      "perplexity": 1.0420570203552364,
      "avg_token_probability": 0.9598695141550849,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_22949--160/160_1977438.txt#0_1",
      "dataset_type": "long",
      "response": "mille miglia",
      "accuracy": 1.0,
      "g_nll": 1.4792723834884711,
      "average_nll": 0.3698180958721178,
      "perplexity": 1.4474712897115132,
      "avg_token_probability": 0.8063348243526309,
      "sequence_length": 4
    },
    {
      "example_id": "qz_1958--123/123_146775.txt#0_1",
      "dataset_type": "long",
      "response": "hair",
      "accuracy": 0.0,
      "g_nll": 0.10289955884218216,
      "average_nll": 0.10289955884218216,
      "perplexity": 1.1083800765067282,
      "avg_token_probability": 0.9022175887099046,
      "sequence_length": 1
    },
    {
      "example_id": "dpql_2287--128/128_614230.txt#0_0",
      "dataset_type": "long",
      "response": "horse collar",
      "accuracy": 1.0,
      "g_nll": 0.39364447351545095,
      "average_nll": 0.19682223675772548,
      "perplexity": 1.217527589816048,
      "avg_token_probability": 0.8366486966984813,
      "sequence_length": 2
    },
    {
      "example_id": "odql_4611--0/0_1366344.txt#0_0",
      "dataset_type": "long",
      "response": "babylon",
      "accuracy": 1.0,
      "g_nll": 0.1307651326060295,
      "average_nll": 0.06538256630301476,
      "perplexity": 1.0675673615699237,
      "avg_token_probability": 0.9368410329988215,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3304--119/119_1433184.txt#0_0",
      "dataset_type": "long",
      "response": "clavicle",
      "accuracy": 0.0,
      "g_nll": 0.9752309796313057,
      "average_nll": 0.3250769932104352,
      "perplexity": 1.3841372110458667,
      "avg_token_probability": 0.7922616655511915,
      "sequence_length": 3
    },
    {
      "example_id": "odql_6502--195/195_349954.txt#0_0",
      "dataset_type": "long",
      "response": "trumpet",
      "accuracy": 1.0,
      "g_nll": 0.013146445620805025,
      "average_nll": 0.0065732228104025126,
      "perplexity": 1.0065948738525043,
      "avg_token_probability": 0.9934484145467192,
      "sequence_length": 2
    },
    {
      "example_id": "jp_2490--7/7_1413067.txt#0_0",
      "dataset_type": "long",
      "response": "charles darwin",
      "accuracy": 1.0,
      "g_nll": 0.025361448142120935,
      "average_nll": 0.006340362035530234,
      "perplexity": 1.0063605046789494,
      "avg_token_probability": 0.9937370678030235,
      "sequence_length": 4
    },
    {
      "example_id": "qw_5145--123/123_1138870.txt#0_1",
      "dataset_type": "long",
      "response": "rowing",
      "accuracy": 1.0,
      "g_nll": 0.03262907639145851,
      "average_nll": 0.03262907639145851,
      "perplexity": 1.033167242037066,
      "avg_token_probability": 0.9678975090502571,
      "sequence_length": 1
    },
    {
      "example_id": "bb_3805--121/121_2943039.txt#0_2",
      "dataset_type": "long",
      "response": "coconut shies",
      "accuracy": 0.0,
      "g_nll": 4.341107933782041,
      "average_nll": 1.0852769834455103,
      "perplexity": 2.960259648111607,
      "avg_token_probability": 0.6766506210584391,
      "sequence_length": 4
    },
    {
      "example_id": "tc_1968--49/49_57917.txt#0_2",
      "dataset_type": "long",
      "response": "georges",
      "accuracy": 0.0,
      "g_nll": 0.7754728856962174,
      "average_nll": 0.3877364428481087,
      "perplexity": 1.4736413443866971,
      "avg_token_probability": 0.7294567705217581,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1080--112/112_854470.txt#0_0",
      "dataset_type": "long",
      "response": "pepper",
      "accuracy": 0.0,
      "g_nll": 1.0526468022726476,
      "average_nll": 0.5263234011363238,
      "perplexity": 1.692697484520024,
      "avg_token_probability": 0.6733369521476128,
      "sequence_length": 2
    },
    {
      "example_id": "qw_597--13/13_1061577.txt#0_2",
      "dataset_type": "long",
      "response": "pygmalion",
      "accuracy": 1.0,
      "g_nll": 0.5353324300988334,
      "average_nll": 0.13383310752470834,
      "perplexity": 1.1432020119093762,
      "avg_token_probability": 0.8942445746293233,
      "sequence_length": 4
    },
    {
      "example_id": "qb_2850--83/83_346130.txt#0_1",
      "dataset_type": "long",
      "response": "the times",
      "accuracy": 1.0,
      "g_nll": 0.15024423133581877,
      "average_nll": 0.07512211566790938,
      "perplexity": 1.0780157854648045,
      "avg_token_probability": 0.9302212188548658,
      "sequence_length": 2
    },
    {
      "example_id": "qw_4081--32/32_1128800.txt#0_0",
      "dataset_type": "long",
      "response": "germany",
      "accuracy": 1.0,
      "g_nll": 0.02776814717799425,
      "average_nll": 0.013884073588997126,
      "perplexity": 1.0139809049576813,
      "avg_token_probability": 0.986258726944651,
      "sequence_length": 2
    },
    {
      "example_id": "bt_2066--111/111_2841537.txt#0_2",
      "dataset_type": "long",
      "response": "Hindenburg",
      "accuracy": 1.0,
      "g_nll": 2.465055017382838,
      "average_nll": 0.8216850057942793,
      "perplexity": 2.2743288679778386,
      "avg_token_probability": 0.6919998070239356,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5143--23/23_1663094.txt#0_1",
      "dataset_type": "long",
      "response": "algeria",
      "accuracy": 0.0,
      "g_nll": 0.6218638687514613,
      "average_nll": 0.2072879562504871,
      "perplexity": 1.230336803949523,
      "avg_token_probability": 0.8456056780707226,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3732--157/157_915760.txt#0_0",
      "dataset_type": "long",
      "response": "project glass",
      "accuracy": 1.0,
      "g_nll": 0.4220407330431044,
      "average_nll": 0.2110203665215522,
      "perplexity": 1.2349375061866084,
      "avg_token_probability": 0.8272103288103552,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2413--135/135_2861641.txt#0_1",
      "dataset_type": "long",
      "response": "machu picchu",
      "accuracy": 1.0,
      "g_nll": 0.11713876501016784,
      "average_nll": 0.02928469125254196,
      "perplexity": 1.0297177043732617,
      "avg_token_probability": 0.9721222485670037,
      "sequence_length": 4
    },
    {
      "example_id": "wh_2204--68/68_777602.txt#0_0",
      "dataset_type": "long",
      "response": "paddy ashdown",
      "accuracy": 1.0,
      "g_nll": 0.26350466324527133,
      "average_nll": 0.06587616581131783,
      "perplexity": 1.0680944423673846,
      "avg_token_probability": 0.9419720764830638,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_10516--89/89_1704034.txt#0_0",
      "dataset_type": "long",
      "response": "jack wilshere",
      "accuracy": 1.0,
      "g_nll": 0.14285288890789616,
      "average_nll": 0.03571322222697404,
      "perplexity": 1.0363585992598594,
      "avg_token_probability": 0.9665920693306571,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3579--194/194_311741.txt#0_1",
      "dataset_type": "long",
      "response": "swan",
      "accuracy": 1.0,
      "g_nll": 0.5305708831292577,
      "average_nll": 0.26528544156462885,
      "perplexity": 1.3038030822602824,
      "avg_token_probability": 0.7940421994981816,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_5497--118/118_700447.txt#0_0",
      "dataset_type": "long",
      "response": "je suis Charlie",
      "accuracy": 0.0,
      "g_nll": 1.910387257754337,
      "average_nll": 0.636795752584779,
      "perplexity": 1.890413810734798,
      "avg_token_probability": 0.6778624979627027,
      "sequence_length": 3
    },
    {
      "example_id": "tc_3129--45/45_95047.txt#0_1",
      "dataset_type": "long",
      "response": "uranus",
      "accuracy": 0.0,
      "g_nll": 1.4122369717806578,
      "average_nll": 0.7061184858903289,
      "perplexity": 2.026111595282644,
      "avg_token_probability": 0.6157859168411977,
      "sequence_length": 2
    },
    {
      "example_id": "qg_4076--107/107_3215880.txt#0_2",
      "dataset_type": "long",
      "response": "sesame street",
      "accuracy": 1.0,
      "g_nll": 0.04241696739336476,
      "average_nll": 0.014138989131121585,
      "perplexity": 1.014239417397792,
      "avg_token_probability": 0.9860087101195297,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8140--128/128_491508.txt#0_1",
      "dataset_type": "long",
      "response": "astronaut",
      "accuracy": 1.0,
      "g_nll": 1.3335315845906734,
      "average_nll": 0.6667657922953367,
      "perplexity": 1.947927120985404,
      "avg_token_probability": 0.6280686480963884,
      "sequence_length": 2
    },
    {
      "example_id": "odql_4941--169/169_781368.txt#0_2",
      "dataset_type": "long",
      "response": "felix",
      "accuracy": 1.0,
      "g_nll": 0.24746914207935333,
      "average_nll": 0.12373457103967667,
      "perplexity": 1.1317154410766312,
      "avg_token_probability": 0.890387156755504,
      "sequence_length": 2
    },
    {
      "example_id": "qf_1776--73/73_1938527.txt#0_2",
      "dataset_type": "long",
      "response": "anthony joshua",
      "accuracy": 1.0,
      "g_nll": 0.3045813763527576,
      "average_nll": 0.060916275270551525,
      "perplexity": 1.0628099269884403,
      "avg_token_probability": 0.9459836407410348,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_19473--126/126_273221.txt#0_0",
      "dataset_type": "long",
      "response": "uranus",
      "accuracy": 1.0,
      "g_nll": 0.03176038432866335,
      "average_nll": 0.015880192164331675,
      "perplexity": 1.0160069525199582,
      "avg_token_probability": 0.984300774687537,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8937--45/45_513006.txt#0_0",
      "dataset_type": "long",
      "response": "tangier",
      "accuracy": 1.0,
      "g_nll": 0.5092414505779743,
      "average_nll": 0.1697471501926581,
      "perplexity": 1.185005185103937,
      "avg_token_probability": 0.8631894414626474,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3949--142/142_1672963.txt#0_1",
      "dataset_type": "long",
      "response": "kipps",
      "accuracy": 1.0,
      "g_nll": 0.045068256673403084,
      "average_nll": 0.022534128336701542,
      "perplexity": 1.0227899396882454,
      "avg_token_probability": 0.9779424337461313,
      "sequence_length": 2
    },
    {
      "example_id": "qw_2956--145/145_1107670.txt#0_1",
      "dataset_type": "long",
      "response": "surrealism",
      "accuracy": 0.0,
      "g_nll": 0.928835715574678,
      "average_nll": 0.30961190519155934,
      "perplexity": 1.3628960785882998,
      "avg_token_probability": 0.7969517252122335,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2506--24/24_75775.txt#0_0",
      "dataset_type": "long",
      "response": "finland",
      "accuracy": 0.0,
      "g_nll": 0.005080313411326642,
      "average_nll": 0.002540156705663321,
      "perplexity": 1.0025433856371264,
      "avg_token_probability": 0.997466273310208,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_22883--94/94_491081.txt#0_2",
      "dataset_type": "long",
      "response": "marc",
      "accuracy": 1.0,
      "g_nll": 0.34866352193057537,
      "average_nll": 0.17433176096528769,
      "perplexity": 1.1904504453028024,
      "avg_token_probability": 0.8503741342255298,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_5156--28/28_691416.txt#0_2",
      "dataset_type": "long",
      "response": "istanbul",
      "accuracy": 1.0,
      "g_nll": 0.0031295527005568147,
      "average_nll": 0.0015647763502784073,
      "perplexity": 1.0015660012516072,
      "avg_token_probability": 0.9984375084513368,
      "sequence_length": 2
    },
    {
      "example_id": "wh_1415--64/64_759730.txt#0_0",
      "dataset_type": "long",
      "response": "the colossus of rhodes",
      "accuracy": 1.0,
      "g_nll": 1.3021772254919597,
      "average_nll": 0.1860253179274228,
      "perplexity": 1.2044527542390233,
      "avg_token_probability": 0.866713745753499,
      "sequence_length": 7
    },
    {
      "example_id": "wh_2443--99/99_783232.txt#0_2",
      "dataset_type": "long",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 0.009284534491598606,
      "average_nll": 0.009284534491598606,
      "perplexity": 1.009327769483963,
      "avg_token_probability": 0.9907584337160047,
      "sequence_length": 1
    },
    {
      "example_id": "bb_6162--31/31_932368.txt#0_1",
      "dataset_type": "long",
      "response": "thank you",
      "accuracy": 1.0,
      "g_nll": 0.45180999650619924,
      "average_nll": 0.22590499825309962,
      "perplexity": 1.2534565790569028,
      "avg_token_probability": 0.8179789407581863,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7238--11/11_75755.txt#0_0",
      "dataset_type": "long",
      "response": "moscow",
      "accuracy": 1.0,
      "g_nll": 0.005187480477616191,
      "average_nll": 0.0025937402388080955,
      "perplexity": 1.0025971068931343,
      "avg_token_probability": 0.9974112675646113,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_18399--42/42_156375.txt#0_0",
      "dataset_type": "long",
      "response": "menorca",
      "accuracy": 1.0,
      "g_nll": 0.1511625827406533,
      "average_nll": 0.07558129137032665,
      "perplexity": 1.0785108977833864,
      "avg_token_probability": 0.9298431815782031,
      "sequence_length": 2
    },
    {
      "example_id": "qw_8714--28/28_287982.txt#0_0",
      "dataset_type": "long",
      "response": "a fool",
      "accuracy": 0.0,
      "g_nll": 5.021298706531525,
      "average_nll": 2.5106493532657623,
      "perplexity": 12.312922901838135,
      "avg_token_probability": 0.3204395241861309,
      "sequence_length": 2
    },
    {
      "example_id": "qb_3427--81/81_362513.txt#0_3",
      "dataset_type": "long",
      "response": "mozart",
      "accuracy": 1.0,
      "g_nll": 2.9733750803934527,
      "average_nll": 1.4866875401967263,
      "perplexity": 4.422422134368909,
      "avg_token_probability": 0.5255381920659054,
      "sequence_length": 2
    },
    {
      "example_id": "bt_2892--150/150_2107251.txt#0_0",
      "dataset_type": "long",
      "response": "bruno mars",
      "accuracy": 1.0,
      "g_nll": 0.1000693426954058,
      "average_nll": 0.03335644756513526,
      "perplexity": 1.0339190114810772,
      "avg_token_probability": 0.9682366009769717,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1386--60/60_3214498.txt#0_0",
      "dataset_type": "long",
      "response": "eat porridge (it\u2019s a spoon)",
      "accuracy": 1.0,
      "g_nll": 0.5231882164822537,
      "average_nll": 0.05813202405358374,
      "perplexity": 1.0598549128156731,
      "avg_token_probability": 0.9470348002114597,
      "sequence_length": 9
    },
    {
      "example_id": "wh_1046--61/61_640960.txt#0_0",
      "dataset_type": "long",
      "response": "surrey",
      "accuracy": 0.0,
      "g_nll": 0.3485306883189878,
      "average_nll": 0.11617689610632927,
      "perplexity": 1.123194543302019,
      "avg_token_probability": 0.9008989425529198,
      "sequence_length": 3
    },
    {
      "example_id": "qb_9573--143/143_499089.txt#0_0",
      "dataset_type": "long",
      "response": "15",
      "accuracy": 1.0,
      "g_nll": 0.0005402297829277813,
      "average_nll": 0.0005402297829277813,
      "perplexity": 1.000540375733318,
      "avg_token_probability": 0.9994599161149075,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_901--24/24_504588.txt#0_1",
      "dataset_type": "long",
      "response": "fidelio",
      "accuracy": 0.0,
      "g_nll": 0.051483946662301605,
      "average_nll": 0.017161315554100536,
      "perplexity": 1.0173094169216619,
      "avg_token_probability": 0.9832720096500651,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9204--170/170_1040261.txt#0_2",
      "dataset_type": "long",
      "response": "snake",
      "accuracy": 1.0,
      "g_nll": 0.03632416948676109,
      "average_nll": 0.03632416948676109,
      "perplexity": 1.0369919531594483,
      "avg_token_probability": 0.9643276372138246,
      "sequence_length": 1
    },
    {
      "example_id": "tc_1250--67/67_36879.txt#0_0",
      "dataset_type": "long",
      "response": "martina hingis",
      "accuracy": 1.0,
      "g_nll": 0.5430248453437798,
      "average_nll": 0.10860496906875597,
      "perplexity": 1.1147219137184763,
      "avg_token_probability": 0.910447120465631,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_16328--35/35_1834766.txt#0_0",
      "dataset_type": "long",
      "response": "bromley",
      "accuracy": 1.0,
      "g_nll": 0.11132941767323246,
      "average_nll": 0.037109805891077485,
      "perplexity": 1.0378069719011096,
      "avg_token_probability": 0.9645594915953319,
      "sequence_length": 3
    },
    {
      "example_id": "qz_3456--13/13_2880285.txt#0_1",
      "dataset_type": "long",
      "response": "missy",
      "accuracy": 0.0,
      "g_nll": 6.455341100692749,
      "average_nll": 3.2276705503463745,
      "perplexity": 25.22083781751413,
      "avg_token_probability": 0.043278856428870305,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1194--11/11_846390.txt#0_1",
      "dataset_type": "long",
      "response": "stanley baldwin",
      "accuracy": 1.0,
      "g_nll": 1.007959373048152,
      "average_nll": 0.251989843262038,
      "perplexity": 1.286582969732389,
      "avg_token_probability": 0.8271529036375511,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_24600--154/154_2012736.txt#0_1",
      "dataset_type": "long",
      "response": "burl",
      "accuracy": 0.0,
      "g_nll": 0.826072017691331,
      "average_nll": 0.4130360088456655,
      "perplexity": 1.5113994487036246,
      "avg_token_probability": 0.7188271812416872,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2790--167/167_3215631.txt#0_0",
      "dataset_type": "long",
      "response": "gold",
      "accuracy": 1.0,
      "g_nll": 0.003761598840355873,
      "average_nll": 0.003761598840355873,
      "perplexity": 1.0037686825324916,
      "avg_token_probability": 0.9962454671100286,
      "sequence_length": 1
    },
    {
      "example_id": "qw_1239--16/16_1074565.txt#0_2",
      "dataset_type": "long",
      "response": "new york",
      "accuracy": 1.0,
      "g_nll": 0.00949261148343794,
      "average_nll": 0.00474630574171897,
      "perplexity": 1.004757587292317,
      "avg_token_probability": 0.9952747961143524,
      "sequence_length": 2
    },
    {
      "example_id": "qw_11125--81/81_1257240.txt#0_1",
      "dataset_type": "long",
      "response": "Strychnine",
      "accuracy": 1.0,
      "g_nll": 2.685359345507095,
      "average_nll": 0.6713398363767737,
      "perplexity": 1.9568574337564508,
      "avg_token_probability": 0.7668740968908723,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_11236--88/88_1721040.txt#0_0",
      "dataset_type": "long",
      "response": "sharks",
      "accuracy": 0.0,
      "g_nll": 0.5755544279236346,
      "average_nll": 0.2877772139618173,
      "perplexity": 1.3334601947148446,
      "avg_token_probability": 0.7804239245142516,
      "sequence_length": 2
    },
    {
      "example_id": "qw_12653--136/136_1283549.txt#0_1",
      "dataset_type": "long",
      "response": "endometriosis",
      "accuracy": 1.0,
      "g_nll": 0.059207590762980544,
      "average_nll": 0.014801897690745136,
      "perplexity": 1.0149119882909636,
      "avg_token_probability": 0.9856266468036297,
      "sequence_length": 4
    },
    {
      "example_id": "qb_8836--4/4_176289.txt#0_1",
      "dataset_type": "long",
      "response": "tribbiani",
      "accuracy": 1.0,
      "g_nll": 0.08843020054337103,
      "average_nll": 0.02947673351445701,
      "perplexity": 1.0299154726796722,
      "avg_token_probability": 0.9717253823759249,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_16447--2/2_201163.txt#0_1",
      "dataset_type": "long",
      "response": "the herald of free enterprise",
      "accuracy": 1.0,
      "g_nll": 0.7089766742428765,
      "average_nll": 0.1417953348485753,
      "perplexity": 1.1523407804005208,
      "avg_token_probability": 0.8876083018843026,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_14271--117/117_152714.txt#0_1",
      "dataset_type": "long",
      "response": "kia",
      "accuracy": 1.0,
      "g_nll": 0.12437782902270555,
      "average_nll": 0.06218891451135278,
      "perplexity": 1.064163361641394,
      "avg_token_probability": 0.940738252648911,
      "sequence_length": 2
    },
    {
      "example_id": "qz_2423--179/179_157731.txt#0_0",
      "dataset_type": "long",
      "response": "marillion",
      "accuracy": 1.0,
      "g_nll": 0.028086713515222073,
      "average_nll": 0.014043356757611036,
      "perplexity": 1.0141424279127396,
      "avg_token_probability": 0.9861129436539393,
      "sequence_length": 2
    },
    {
      "example_id": "qb_482--134/134_278266.txt#0_0",
      "dataset_type": "long",
      "response": "brain",
      "accuracy": 1.0,
      "g_nll": 0.025788597762584686,
      "average_nll": 0.025788597762584686,
      "perplexity": 1.0261240006330212,
      "avg_token_probability": 0.9745410880001781,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_12025--58/58_1700414.txt#0_2",
      "dataset_type": "long",
      "response": "127 hours",
      "accuracy": 1.0,
      "g_nll": 0.00084043945207668,
      "average_nll": 0.00042021972603834,
      "perplexity": 1.0004203080307161,
      "avg_token_probability": 0.9995799520188892,
      "sequence_length": 2
    },
    {
      "example_id": "qw_3080--104/104_1110125.txt#0_1",
      "dataset_type": "long",
      "response": "myanmar",
      "accuracy": 0.0,
      "g_nll": 1.1820436037378386,
      "average_nll": 0.5910218018689193,
      "perplexity": 1.8058326762063268,
      "avg_token_probability": 0.6531138098312311,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1614--48/48_867787.txt#0_1",
      "dataset_type": "long",
      "response": "norway",
      "accuracy": 1.0,
      "g_nll": 0.0076525125823536655,
      "average_nll": 0.0038262562911768327,
      "perplexity": 1.0038335857549339,
      "avg_token_probability": 0.9961882768657997,
      "sequence_length": 2
    },
    {
      "example_id": "qg_248--100/100_282206.txt#0_1",
      "dataset_type": "long",
      "response": "Rudyard Kipling",
      "accuracy": 1.0,
      "g_nll": 0.9771429253728456,
      "average_nll": 0.16285715422880762,
      "perplexity": 1.1768685669829013,
      "avg_token_probability": 0.8718773307004758,
      "sequence_length": 6
    },
    {
      "example_id": "odql_1018--140/140_2097132.txt#0_0",
      "dataset_type": "long",
      "response": "su",
      "accuracy": 0.0,
      "g_nll": 0.2786332666873932,
      "average_nll": 0.2786332666873932,
      "perplexity": 1.3213226819599597,
      "avg_token_probability": 0.756817402480875,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_21930--191/191_1362130.txt#0_0",
      "dataset_type": "long",
      "response": "HESPERIDES",
      "accuracy": 1.0,
      "g_nll": 3.824162039289149,
      "average_nll": 0.9560405098222873,
      "perplexity": 2.6013759324132177,
      "avg_token_probability": 0.6554733226287257,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2250--43/43_67037.txt#0_0",
      "dataset_type": "long",
      "response": "Tsunami",
      "accuracy": 0.0,
      "g_nll": 9.152879117598786,
      "average_nll": 3.050959705866262,
      "perplexity": 21.135618669551857,
      "avg_token_probability": 0.3445524401034987,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_11235--145/145_1446453.txt#0_0",
      "dataset_type": "long",
      "response": "atat\u00fcrk",
      "accuracy": 0.0,
      "g_nll": 0.13424382731318474,
      "average_nll": 0.06712191365659237,
      "perplexity": 1.0694258478430823,
      "avg_token_probability": 0.9351037378440499,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2300--133/133_1682374.txt#0_0",
      "dataset_type": "long",
      "response": "pascal",
      "accuracy": 1.0,
      "g_nll": 0.005782503954833373,
      "average_nll": 0.0028912519774166867,
      "perplexity": 1.0028954356774873,
      "avg_token_probability": 0.9971169326641227,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_9920--21/21_331455.txt#0_0",
      "dataset_type": "long",
      "response": "grand hotel",
      "accuracy": 1.0,
      "g_nll": 0.07565191153844353,
      "average_nll": 0.037825955769221764,
      "perplexity": 1.0385504634314822,
      "avg_token_probability": 0.9635643496439095,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2116--1/1_3114498.txt#0_0",
      "dataset_type": "long",
      "response": "sliced bread",
      "accuracy": 1.0,
      "g_nll": 0.045480342116206884,
      "average_nll": 0.015160114038735628,
      "perplexity": 1.0152756114809383,
      "avg_token_probability": 0.9851027599403928,
      "sequence_length": 3
    },
    {
      "example_id": "jp_3234--33/33_1767595.txt#0_0",
      "dataset_type": "long",
      "response": "ireland",
      "accuracy": 0.0,
      "g_nll": 0.5047354102133568,
      "average_nll": 0.2523677051066784,
      "perplexity": 1.2870692122070686,
      "avg_token_probability": 0.801832544489242,
      "sequence_length": 2
    },
    {
      "example_id": "qw_2310--122/122_2696481.txt#0_0",
      "dataset_type": "long",
      "response": "myxomatosis",
      "accuracy": 1.0,
      "g_nll": 0.11784387099532978,
      "average_nll": 0.029460967748832445,
      "perplexity": 1.0298992354017138,
      "avg_token_probability": 0.9720161208096598,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_7345--158/158_612044.txt#0_2",
      "dataset_type": "long",
      "response": "five",
      "accuracy": 1.0,
      "g_nll": 0.3471967279911041,
      "average_nll": 0.3471967279911041,
      "perplexity": 1.4150950868024361,
      "avg_token_probability": 0.7066662935418782,
      "sequence_length": 1
    },
    {
      "example_id": "odql_548--127/127_413228.txt#0_1",
      "dataset_type": "long",
      "response": "los angeles",
      "accuracy": 1.0,
      "g_nll": 0.3807160872965767,
      "average_nll": 0.12690536243219222,
      "perplexity": 1.135309569760866,
      "avg_token_probability": 0.894272493446827,
      "sequence_length": 3
    },
    {
      "example_id": "odql_206--73/73_813141.txt#0_1",
      "dataset_type": "long",
      "response": "neptune",
      "accuracy": 1.0,
      "g_nll": 0.43197135941591114,
      "average_nll": 0.21598567970795557,
      "perplexity": 1.2410846061794036,
      "avg_token_probability": 0.824482986182798,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_5388--155/155_1723803.txt#0_0",
      "dataset_type": "long",
      "response": "m69",
      "accuracy": 1.0,
      "g_nll": 0.20323477080091834,
      "average_nll": 0.10161738540045917,
      "perplexity": 1.1069598516911334,
      "avg_token_probability": 0.9079177876087379,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1833--26/26_2841025.txt#0_0",
      "dataset_type": "long",
      "response": "lancashire",
      "accuracy": 1.0,
      "g_nll": 0.013097374408971518,
      "average_nll": 0.0043657914696571725,
      "perplexity": 1.0043753354211482,
      "avg_token_probability": 0.9956499852124674,
      "sequence_length": 3
    },
    {
      "example_id": "qz_6268--46/46_250126.txt#0_1",
      "dataset_type": "long",
      "response": "manchester",
      "accuracy": 0.0,
      "g_nll": 0.01944734858625452,
      "average_nll": 0.00972367429312726,
      "perplexity": 1.009771102815863,
      "avg_token_probability": 0.9903699847801952,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5984--118/118_142159.txt#0_0",
      "dataset_type": "long",
      "response": "ravens",
      "accuracy": 1.0,
      "g_nll": 0.08702727407214184,
      "average_nll": 0.04351363703607092,
      "perplexity": 1.044474237747762,
      "avg_token_probability": 0.9583260372926132,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3072--53/53_1526395.txt#0_2",
      "dataset_type": "long",
      "response": "yves saint laurent",
      "accuracy": 1.0,
      "g_nll": 7.821411590906791,
      "average_nll": 1.5642823181813583,
      "perplexity": 4.779243728454176,
      "avg_token_probability": 0.6118144308292154,
      "sequence_length": 5
    },
    {
      "example_id": "qb_2271--145/145_2891786.txt#0_1",
      "dataset_type": "long",
      "response": "purple",
      "accuracy": 1.0,
      "g_nll": 0.005161531735211611,
      "average_nll": 0.005161531735211611,
      "perplexity": 1.0051748753881562,
      "avg_token_probability": 0.9948517660808445,
      "sequence_length": 1
    },
    {
      "example_id": "qw_3199--48/48_2960705.txt#0_2",
      "dataset_type": "long",
      "response": "atlantic ocean",
      "accuracy": 1.0,
      "g_nll": 1.6219370401777269,
      "average_nll": 0.5406456800592423,
      "perplexity": 1.7171152113782744,
      "avg_token_probability": 0.7291381838085841,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_13847--137/137_1778867.txt#0_1",
      "dataset_type": "long",
      "response": "helly hansen",
      "accuracy": 1.0,
      "g_nll": 0.8766064156643552,
      "average_nll": 0.2191516039160888,
      "perplexity": 1.2450200122901136,
      "avg_token_probability": 0.8403146287535045,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_1224--17/17_461209.txt#0_1",
      "dataset_type": "long",
      "response": "shadwell",
      "accuracy": 0.0,
      "g_nll": 2.0510317080870664,
      "average_nll": 0.6836772360290221,
      "perplexity": 1.981149508368778,
      "avg_token_probability": 0.709521621358611,
      "sequence_length": 3
    },
    {
      "example_id": "qw_12099--173/173_1273928.txt#0_0",
      "dataset_type": "long",
      "response": "new zealand",
      "accuracy": 1.0,
      "g_nll": 0.21973197988688753,
      "average_nll": 0.07324399329562918,
      "perplexity": 1.0759930399768056,
      "avg_token_probability": 0.9341628858137582,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_24481--77/77_2010082.txt#0_1",
      "dataset_type": "long",
      "response": "mary quant",
      "accuracy": 1.0,
      "g_nll": 0.01816480979323387,
      "average_nll": 0.009082404896616936,
      "perplexity": 1.0091237750880604,
      "avg_token_probability": 0.9909703312900143,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12424--70/70_2829432.txt#0_0",
      "dataset_type": "long",
      "response": "tuna",
      "accuracy": 1.0,
      "g_nll": 0.19018322229385376,
      "average_nll": 0.09509161114692688,
      "perplexity": 1.09975960074969,
      "avg_token_probability": 0.9118276113268884,
      "sequence_length": 2
    },
    {
      "example_id": "qb_404--89/89_2634070.txt#0_2",
      "dataset_type": "long",
      "response": "mallard",
      "accuracy": 1.0,
      "g_nll": 0.05681683097463974,
      "average_nll": 0.02840841548731987,
      "perplexity": 1.0288157829275641,
      "avg_token_probability": 0.972382927544722,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1626--74/74_774368.txt#0_0",
      "dataset_type": "long",
      "response": "defoe",
      "accuracy": 1.0,
      "g_nll": 0.5694711207861474,
      "average_nll": 0.2847355603930737,
      "perplexity": 1.3294104328633565,
      "avg_token_probability": 0.7829101842082434,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11066--48/48_1410700.txt#0_2",
      "dataset_type": "long",
      "response": "cello",
      "accuracy": 1.0,
      "g_nll": 0.11495223641395569,
      "average_nll": 0.057476118206977844,
      "perplexity": 1.0591599757133718,
      "avg_token_probability": 0.9457043598835613,
      "sequence_length": 2
    },
    {
      "example_id": "bb_320--142/142_2934494.txt#0_0",
      "dataset_type": "long",
      "response": "ravine",
      "accuracy": 0.0,
      "g_nll": 3.9082254162203753,
      "average_nll": 1.9541127081101877,
      "perplexity": 7.057654045484398,
      "avg_token_probability": 0.5100170805302752,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_25624--186/186_3210436.txt#0_1",
      "dataset_type": "long",
      "response": "jenni murray",
      "accuracy": 1.0,
      "g_nll": 0.173004479142719,
      "average_nll": 0.034600895828543796,
      "perplexity": 1.0352064711217248,
      "avg_token_probability": 0.9670944595345187,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_901--24/24_504588.txt#0_0",
      "dataset_type": "long",
      "response": "fidelio",
      "accuracy": 0.0,
      "g_nll": 0.2756783929048652,
      "average_nll": 0.09189279763495506,
      "perplexity": 1.0962472954786153,
      "avg_token_probability": 0.9196789606081545,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_376--165/165_2911834.txt#0_2",
      "dataset_type": "long",
      "response": "gargantua",
      "accuracy": 1.0,
      "g_nll": 0.8142128216568381,
      "average_nll": 0.20355320541420951,
      "perplexity": 1.2257503724625851,
      "avg_token_probability": 0.8581298853556443,
      "sequence_length": 4
    },
    {
      "example_id": "odql_8725--117/117_1962716.txt#0_0",
      "dataset_type": "long",
      "response": "essex eagles",
      "accuracy": 1.0,
      "g_nll": 2.0541146639345698,
      "average_nll": 0.5135286659836424,
      "perplexity": 1.671177831216823,
      "avg_token_probability": 0.7769536676478818,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_2064--32/32_2647072.txt#0_0",
      "dataset_type": "long",
      "response": "dye",
      "accuracy": 1.0,
      "g_nll": 0.29741373285651207,
      "average_nll": 0.14870686642825603,
      "perplexity": 1.1603328068520025,
      "avg_token_probability": 0.8661820872968785,
      "sequence_length": 2
    },
    {
      "example_id": "bb_6913--109/109_984790.txt#0_0",
      "dataset_type": "long",
      "response": "blackfriars",
      "accuracy": 1.0,
      "g_nll": 0.002344204734527011,
      "average_nll": 0.0005860511836317528,
      "perplexity": 1.0005862229451787,
      "avg_token_probability": 0.9994143770587274,
      "sequence_length": 4
    },
    {
      "example_id": "odql_13282--196/196_2145719.txt#0_2",
      "dataset_type": "long",
      "response": "nouakchott",
      "accuracy": 1.0,
      "g_nll": 0.03857313897970016,
      "average_nll": 0.00964328474492504,
      "perplexity": 1.009689931035845,
      "avg_token_probability": 0.9905364720291778,
      "sequence_length": 4
    },
    {
      "example_id": "qw_6502--97/97_1172999.txt#0_0",
      "dataset_type": "long",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 0.05651088431477547,
      "average_nll": 0.05651088431477547,
      "perplexity": 1.0581381318460512,
      "avg_token_probability": 0.9450561981500258,
      "sequence_length": 1
    },
    {
      "example_id": "wh_495--37/37_2926588.txt#0_1",
      "dataset_type": "long",
      "response": "tommy roe",
      "accuracy": 1.0,
      "g_nll": 0.12438487998130654,
      "average_nll": 0.031096219995326635,
      "perplexity": 1.031584758191934,
      "avg_token_probability": 0.9700350780257346,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1058--65/65_66511.txt#0_2",
      "dataset_type": "long",
      "response": "Spartacus",
      "accuracy": 1.0,
      "g_nll": 2.087720476280083,
      "average_nll": 0.6959068254266944,
      "perplexity": 2.0055269123836115,
      "avg_token_probability": 0.7062441617299403,
      "sequence_length": 3
    },
    {
      "example_id": "odql_2399--38/38_2808267.txt#0_0",
      "dataset_type": "long",
      "response": "lanzarote",
      "accuracy": 1.0,
      "g_nll": 0.020586253820511047,
      "average_nll": 0.006862084606837016,
      "perplexity": 1.0068856826558017,
      "avg_token_probability": 0.9932069054135685,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5580--182/182_233604.txt#0_1",
      "dataset_type": "long",
      "response": "Nero",
      "accuracy": 0.0,
      "g_nll": 0.4966651238501072,
      "average_nll": 0.2483325619250536,
      "perplexity": 1.281886167846897,
      "avg_token_probability": 0.8014558250609596,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12354--Lord_Snooty.txt#0_1",
      "dataset_type": "long",
      "response": "lord snooty",
      "accuracy": 1.0,
      "g_nll": 0.17157469150333782,
      "average_nll": 0.042893672875834454,
      "perplexity": 1.0438269018373343,
      "avg_token_probability": 0.9605337406070287,
      "sequence_length": 4
    },
    {
      "example_id": "qw_1849--186/186_2958618.txt#0_0",
      "dataset_type": "long",
      "response": "apogee",
      "accuracy": 0.0,
      "g_nll": 0.5986795386315862,
      "average_nll": 0.19955984621052872,
      "perplexity": 1.2208652714047885,
      "avg_token_probability": 0.8497694297532035,
      "sequence_length": 3
    },
    {
      "example_id": "tb_83--165/165_2048554.txt#0_0",
      "dataset_type": "long",
      "response": "cyclops",
      "accuracy": 1.0,
      "g_nll": 0.02982937660999596,
      "average_nll": 0.00994312553666532,
      "perplexity": 1.00999272265655,
      "avg_token_probability": 0.9901157033342217,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_13030--New_York_Stadium.txt#0_0",
      "dataset_type": "long",
      "response": "rotherham united",
      "accuracy": 1.0,
      "g_nll": 0.021235542073554825,
      "average_nll": 0.005308885518388706,
      "perplexity": 1.0053230026220854,
      "avg_token_probability": 0.9947303793811185,
      "sequence_length": 4
    },
    {
      "example_id": "qw_8760--191/191_551701.txt#0_1",
      "dataset_type": "long",
      "response": "ronald searle",
      "accuracy": 1.0,
      "g_nll": 0.7437862186689017,
      "average_nll": 0.14875724373378035,
      "perplexity": 1.1603912627647366,
      "avg_token_probability": 0.8825879242357331,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_9896--145/145_1575967.txt#0_1",
      "dataset_type": "long",
      "response": "saint cecilia",
      "accuracy": 1.0,
      "g_nll": 0.10207552427857536,
      "average_nll": 0.02041510485571507,
      "perplexity": 1.020624918465459,
      "avg_token_probability": 0.9799418807493845,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_9525--59/59_129915.txt#0_1",
      "dataset_type": "long",
      "response": "Henri Kontinen",
      "accuracy": 0.0,
      "g_nll": 1.082732478120306,
      "average_nll": 0.2706831195300765,
      "perplexity": 1.3108596187518162,
      "avg_token_probability": 0.8268296006847851,
      "sequence_length": 4
    },
    {
      "example_id": "odql_3814--123/123_2148942.txt#0_1",
      "dataset_type": "long",
      "response": "3000m",
      "accuracy": 0.0,
      "g_nll": 0.2416261946772238,
      "average_nll": 0.08054206489240794,
      "perplexity": 1.0838744387449801,
      "avg_token_probability": 0.9278511386443961,
      "sequence_length": 3
    },
    {
      "example_id": "qz_2223--157/157_12133.txt#0_0",
      "dataset_type": "long",
      "response": "matthew",
      "accuracy": 1.0,
      "g_nll": 0.03854256821796298,
      "average_nll": 0.01927128410898149,
      "perplexity": 1.0194581739096062,
      "avg_token_probability": 0.98109169798299,
      "sequence_length": 2
    },
    {
      "example_id": "qz_5877--148/148_240409.txt#0_1",
      "dataset_type": "long",
      "response": "tartar",
      "accuracy": 1.0,
      "g_nll": 0.006533371950354194,
      "average_nll": 0.0021777906501180646,
      "perplexity": 1.0021801637585743,
      "avg_token_probability": 0.9978278346881617,
      "sequence_length": 3
    },
    {
      "example_id": "qw_11886--United_Nations_peacekeeping.txt#0_0",
      "dataset_type": "long",
      "response": "blue helmets",
      "accuracy": 1.0,
      "g_nll": 1.592201292514801,
      "average_nll": 0.7961006462574005,
      "perplexity": 2.216879654803154,
      "avg_token_probability": 0.45117646531954964,
      "sequence_length": 2
    },
    {
      "example_id": "qw_3650--35/35_1121378.txt#0_1",
      "dataset_type": "long",
      "response": "risk business",
      "accuracy": 0.0,
      "g_nll": 2.437931776046753,
      "average_nll": 1.2189658880233765,
      "perplexity": 3.383686812703037,
      "avg_token_probability": 0.2960568676651234,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12631--117/117_2313442.txt#0_0",
      "dataset_type": "long",
      "response": "lower leg",
      "accuracy": 0.0,
      "g_nll": 4.519806981086731,
      "average_nll": 2.2599034905433655,
      "perplexity": 9.582164352663384,
      "avg_token_probability": 0.11020928591472649,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3656--12/12_934124.txt#0_1",
      "dataset_type": "long",
      "response": "los angeles",
      "accuracy": 1.0,
      "g_nll": 0.2864649517576936,
      "average_nll": 0.09548831725256453,
      "perplexity": 1.1001959686472227,
      "avg_token_probability": 0.9169484019939459,
      "sequence_length": 3
    },
    {
      "example_id": "qf_3202--166/166_863009.txt#0_0",
      "dataset_type": "long",
      "response": "dry ice",
      "accuracy": 1.0,
      "g_nll": 0.012165400985395536,
      "average_nll": 0.006082700492697768,
      "perplexity": 1.0061012376816698,
      "avg_token_probability": 0.9939513785894377,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_893--144/144_1486574.txt#0_1",
      "dataset_type": "long",
      "response": "tenerife south",
      "accuracy": 1.0,
      "g_nll": 0.9680855460405837,
      "average_nll": 0.3226951820135279,
      "perplexity": 1.3808443805455115,
      "avg_token_probability": 0.7845607640713324,
      "sequence_length": 3
    },
    {
      "example_id": "bb_7225--113/113_991549.txt#0_0",
      "dataset_type": "long",
      "response": "fallopian tube",
      "accuracy": 1.0,
      "g_nll": 0.11675731079799334,
      "average_nll": 0.029189327699498335,
      "perplexity": 1.029619511516425,
      "avg_token_probability": 0.9717748744858048,
      "sequence_length": 4
    },
    {
      "example_id": "qb_5053--95/95_407079.txt#0_0",
      "dataset_type": "long",
      "response": "shoji",
      "accuracy": 1.0,
      "g_nll": 0.00017045842832885683,
      "average_nll": 8.522921416442841e-05,
      "perplexity": 1.000085232846277,
      "avg_token_probability": 0.9999147762150196,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12695--170/170_2314497.txt#0_0",
      "dataset_type": "long",
      "response": "Hercule Poirot",
      "accuracy": 1.0,
      "g_nll": 0.8347406596735709,
      "average_nll": 0.13912344327892848,
      "perplexity": 1.1492659604060818,
      "avg_token_probability": 0.9044873996094286,
      "sequence_length": 6
    },
    {
      "example_id": "odql_8598--108/108_3212953.txt#0_1",
      "dataset_type": "long",
      "response": "dennis quaid",
      "accuracy": 0.0,
      "g_nll": 8.04923841356981,
      "average_nll": 2.0123096033924526,
      "perplexity": 7.480574571383914,
      "avg_token_probability": 0.4513180163471311,
      "sequence_length": 4
    },
    {
      "example_id": "qg_3271--66/66_2533261.txt#0_0",
      "dataset_type": "long",
      "response": "kentucky derby",
      "accuracy": 1.0,
      "g_nll": 0.0518551159366325,
      "average_nll": 0.012963778984158125,
      "perplexity": 1.0130481730613357,
      "avg_token_probability": 0.9872086068624009,
      "sequence_length": 4
    },
    {
      "example_id": "odql_450--14/14_979444.txt#0_1",
      "dataset_type": "long",
      "response": "the blue boy",
      "accuracy": 1.0,
      "g_nll": 0.01758642477398098,
      "average_nll": 0.005862141591326993,
      "perplexity": 1.0058793575677354,
      "avg_token_probability": 0.99418865602282,
      "sequence_length": 3
    },
    {
      "example_id": "qb_84--151/151_266387.txt#0_0",
      "dataset_type": "long",
      "response": "bees",
      "accuracy": 0.0,
      "g_nll": 0.1209955106023699,
      "average_nll": 0.06049775530118495,
      "perplexity": 1.0623652128777141,
      "avg_token_probability": 0.9429460106791958,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_4205--53/53_60605.txt#0_0",
      "dataset_type": "long",
      "response": "county cork",
      "accuracy": 0.0,
      "g_nll": 1.9133545905351639,
      "average_nll": 0.9566772952675819,
      "perplexity": 2.6030329782800696,
      "avg_token_probability": 0.5115687683732727,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_19313--55/55_1898648.txt#0_1",
      "dataset_type": "long",
      "response": "eros",
      "accuracy": 0.0,
      "g_nll": 0.2860092520713806,
      "average_nll": 0.2860092520713806,
      "perplexity": 1.3311047706716734,
      "avg_token_probability": 0.7512556652436919,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_25425--97/97_2030678.txt#0_2",
      "dataset_type": "long",
      "response": "luxembourg",
      "accuracy": 1.0,
      "g_nll": 0.025135917880106717,
      "average_nll": 0.012567958940053359,
      "perplexity": 1.01264726763723,
      "avg_token_probability": 0.9875784650616641,
      "sequence_length": 2
    },
    {
      "example_id": "qb_9568--153/153_530377.txt#0_1",
      "dataset_type": "long",
      "response": "george foreman",
      "accuracy": 1.0,
      "g_nll": 0.20987088224899253,
      "average_nll": 0.05246772056224813,
      "perplexity": 1.0538685432454455,
      "avg_token_probability": 0.950261383332534,
      "sequence_length": 4
    },
    {
      "example_id": "tb_2199--49/49_2803159.txt#0_2",
      "dataset_type": "long",
      "response": "chicago cubs",
      "accuracy": 1.0,
      "g_nll": 0.23240588419139385,
      "average_nll": 0.05810147104784846,
      "perplexity": 1.0598225315571184,
      "avg_token_probability": 0.9447594082556511,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_971--13/13_577956.txt#0_0",
      "dataset_type": "long",
      "response": "secretary",
      "accuracy": 1.0,
      "g_nll": 5.024416131898761,
      "average_nll": 2.5122080659493804,
      "perplexity": 12.33213017636004,
      "avg_token_probability": 0.4880129745921775,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_8322--184/184_1656059.txt#0_0",
      "dataset_type": "long",
      "response": "budy",
      "accuracy": 0.0,
      "g_nll": 4.882214307785034,
      "average_nll": 2.441107153892517,
      "perplexity": 11.485750198875852,
      "avg_token_probability": 0.1407410401197836,
      "sequence_length": 2
    },
    {
      "example_id": "qb_388--149/149_18617.txt#0_2",
      "dataset_type": "long",
      "response": "billie jean king",
      "accuracy": 0.0,
      "g_nll": 1.2343191327981913,
      "average_nll": 0.3085797831995478,
      "perplexity": 1.361490129253187,
      "avg_token_probability": 0.789480037158456,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1227--60/60_298938.txt#0_0",
      "dataset_type": "long",
      "response": "Colette",
      "accuracy": 1.0,
      "g_nll": 2.1297301053914452,
      "average_nll": 1.0648650526957226,
      "perplexity": 2.900447550070667,
      "avg_token_probability": 0.5594328476899142,
      "sequence_length": 2
    },
    {
      "example_id": "qw_5900--159/159_1161564.txt#0_0",
      "dataset_type": "long",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.2166008548811078,
      "average_nll": 0.1083004274405539,
      "perplexity": 1.114382486179411,
      "avg_token_probability": 0.9024103096432317,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1106--42/42_855085.txt#0_0",
      "dataset_type": "long",
      "response": "language",
      "accuracy": 1.0,
      "g_nll": 0.3155743479728699,
      "average_nll": 0.3155743479728699,
      "perplexity": 1.371046542665378,
      "avg_token_probability": 0.7293698418552251,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_5173--26/26_1584174.txt#0_2",
      "dataset_type": "long",
      "response": "iwo jima",
      "accuracy": 1.0,
      "g_nll": 0.06925424268590064,
      "average_nll": 0.01731356067147516,
      "perplexity": 1.0174643091037374,
      "avg_token_probability": 0.9832539800788849,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_2287--128/128_614230.txt#0_1",
      "dataset_type": "long",
      "response": "crabs",
      "accuracy": 0.0,
      "g_nll": 4.325255209580064,
      "average_nll": 2.162627604790032,
      "perplexity": 8.693951941643343,
      "avg_token_probability": 0.4970219689380585,
      "sequence_length": 2
    },
    {
      "example_id": "qb_1429--139/139_304888.txt#0_0",
      "dataset_type": "long",
      "response": "la boheme",
      "accuracy": 1.0,
      "g_nll": 0.23254837282001972,
      "average_nll": 0.07751612427333991,
      "perplexity": 1.0805996562023146,
      "avg_token_probability": 0.926761501926018,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_25263--68/68_2027371.txt#0_0",
      "dataset_type": "long",
      "response": "warblers",
      "accuracy": 0.0,
      "g_nll": 0.35023242235183716,
      "average_nll": 0.17511621117591858,
      "perplexity": 1.1913846607802232,
      "avg_token_probability": 0.8412306025091367,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14642--115/115_2771986.txt#0_1",
      "dataset_type": "long",
      "response": "konnie huq",
      "accuracy": 0.0,
      "g_nll": 4.149802533849652,
      "average_nll": 1.037450633462413,
      "perplexity": 2.822013489281953,
      "avg_token_probability": 0.7529945435633406,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1122--112/112_553537.txt#0_3",
      "dataset_type": "long",
      "response": "sardinia",
      "accuracy": 1.0,
      "g_nll": 0.08505263162624033,
      "average_nll": 0.028350877208746777,
      "perplexity": 1.0287565883414393,
      "avg_token_probability": 0.9726345436920192,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_2291--49/49_614335.txt#0_0",
      "dataset_type": "long",
      "response": "hydrochloric acid",
      "accuracy": 1.0,
      "g_nll": 0.11684712244709772,
      "average_nll": 0.02921178061177443,
      "perplexity": 1.0296426297325296,
      "avg_token_probability": 0.9716246012861816,
      "sequence_length": 4
    },
    {
      "example_id": "qw_15997--140/140_2983879.txt#0_1",
      "dataset_type": "long",
      "response": "eric coates",
      "accuracy": 1.0,
      "g_nll": 0.07196875031877425,
      "average_nll": 0.023989583439591417,
      "perplexity": 1.0242796483642194,
      "avg_token_probability": 0.9767696545593466,
      "sequence_length": 3
    },
    {
      "example_id": "odql_9298--52/52_868598.txt#0_0",
      "dataset_type": "long",
      "response": "skoda",
      "accuracy": 0.0,
      "g_nll": 1.875812627171399,
      "average_nll": 0.9379063135856995,
      "perplexity": 2.554627226967562,
      "avg_token_probability": 0.576525281045985,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_11961--3/3_2765982.txt#0_1",
      "dataset_type": "long",
      "response": "les invalides",
      "accuracy": 1.0,
      "g_nll": 0.0245681211667943,
      "average_nll": 0.008189373722264767,
      "perplexity": 1.008222998368837,
      "avg_token_probability": 0.9919035883917974,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_1626--73/73_1503331.txt#0_1",
      "dataset_type": "long",
      "response": "millais",
      "accuracy": 1.0,
      "g_nll": 0.976702198619023,
      "average_nll": 0.4883510993095115,
      "perplexity": 1.6296269104246943,
      "avg_token_probability": 0.68822203678204,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5527--13/13_90964.txt#0_1",
      "dataset_type": "long",
      "response": "marsupial lion",
      "accuracy": 0.0,
      "g_nll": 1.1709314342233483,
      "average_nll": 0.2927328585558371,
      "perplexity": 1.340084750434618,
      "avg_token_probability": 0.8239357002730888,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_1192--97/97_1493497.txt#0_2",
      "dataset_type": "long",
      "response": "keane",
      "accuracy": 1.0,
      "g_nll": 0.16355067639960907,
      "average_nll": 0.08177533819980454,
      "perplexity": 1.0852119767642934,
      "avg_token_probability": 0.9245360719818515,
      "sequence_length": 2
    },
    {
      "example_id": "qw_4782--187/187_1142011.txt#0_1",
      "dataset_type": "long",
      "response": "wooden clog",
      "accuracy": 1.0,
      "g_nll": 0.1102515896782279,
      "average_nll": 0.027562897419556975,
      "perplexity": 1.0279462682416893,
      "avg_token_probability": 0.9731854635251267,
      "sequence_length": 4
    },
    {
      "example_id": "bt_1538--42/42_1542499.txt#0_1",
      "dataset_type": "long",
      "response": "quincy",
      "accuracy": 1.0,
      "g_nll": 0.014443394116824493,
      "average_nll": 0.007221697058412246,
      "perplexity": 1.0072478363981923,
      "avg_token_probability": 0.9928269052615978,
      "sequence_length": 2
    },
    {
      "example_id": "bt_3044--15/15_470066.txt#0_1",
      "dataset_type": "long",
      "response": "supertramp",
      "accuracy": 1.0,
      "g_nll": 0.008949952463808586,
      "average_nll": 0.0029833174879361954,
      "perplexity": 1.002987772008201,
      "avg_token_probability": 0.9970298506185324,
      "sequence_length": 3
    },
    {
      "example_id": "qw_16785--192/192_1349169.txt#0_1",
      "dataset_type": "long",
      "response": "1880",
      "accuracy": 0.0,
      "g_nll": 0.04512588905345183,
      "average_nll": 0.022562944526725914,
      "perplexity": 1.022819413022155,
      "avg_token_probability": 0.9779363265008265,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3059--26/26_1399947.txt#0_1",
      "dataset_type": "long",
      "response": "judas",
      "accuracy": 1.0,
      "g_nll": 0.37899059040682914,
      "average_nll": 0.18949529520341457,
      "perplexity": 1.2086394375733573,
      "avg_token_probability": 0.8422731504314509,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3128--50/50_3337.txt#0_0",
      "dataset_type": "long",
      "response": "robin williams",
      "accuracy": 1.0,
      "g_nll": 0.054999166110064834,
      "average_nll": 0.013749791527516209,
      "perplexity": 1.0138447546529377,
      "avg_token_probability": 0.9865432275686974,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_455--142/142_545804.txt#0_0",
      "dataset_type": "long",
      "response": "cato",
      "accuracy": 1.0,
      "g_nll": 0.07115853577374764,
      "average_nll": 0.03557926788687382,
      "perplexity": 1.036219783825261,
      "avg_token_probability": 0.9656570104613842,
      "sequence_length": 2
    },
    {
      "example_id": "odql_7977--27/27_2226595.txt#0_0",
      "dataset_type": "long",
      "response": "gollum",
      "accuracy": 0.0,
      "g_nll": 0.0746949482081618,
      "average_nll": 0.024898316069387267,
      "perplexity": 1.025210867753363,
      "avg_token_probability": 0.9757811772952855,
      "sequence_length": 3
    },
    {
      "example_id": "bb_618--61/61_2669551.txt#0_1",
      "dataset_type": "long",
      "response": "elephants",
      "accuracy": 1.0,
      "g_nll": 0.01483945082873106,
      "average_nll": 0.00741972541436553,
      "perplexity": 1.007447319782303,
      "avg_token_probability": 0.9926083617935688,
      "sequence_length": 2
    },
    {
      "example_id": "odql_14838--54/54_107845.txt#0_2",
      "dataset_type": "long",
      "response": "friday",
      "accuracy": 0.0,
      "g_nll": 2.4464245699346066,
      "average_nll": 1.2232122849673033,
      "perplexity": 3.3980858404005123,
      "avg_token_probability": 0.5270662536774297,
      "sequence_length": 2
    },
    {
      "example_id": "qb_6250--52/52_439893.txt#0_0",
      "dataset_type": "long",
      "response": "saint moritz",
      "accuracy": 1.0,
      "g_nll": 0.818344546307344,
      "average_nll": 0.204586136576836,
      "perplexity": 1.227017142350318,
      "avg_token_probability": 0.8589977054607373,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2359--127/127_71063.txt#0_0",
      "dataset_type": "long",
      "response": "john buchan",
      "accuracy": 1.0,
      "g_nll": 0.06629313280791393,
      "average_nll": 0.02209771093597131,
      "perplexity": 1.0223436737474676,
      "avg_token_probability": 0.9783295639626203,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9139--113/113_1038621.txt#0_1",
      "dataset_type": "long",
      "response": "desmond tutu",
      "accuracy": 1.0,
      "g_nll": 0.05422332971511423,
      "average_nll": 0.013555832428778558,
      "perplexity": 1.0136481293073192,
      "avg_token_probability": 0.9867961797504874,
      "sequence_length": 4
    },
    {
      "example_id": "qw_1719--97/97_1084406.txt#0_1",
      "dataset_type": "long",
      "response": "france",
      "accuracy": 1.0,
      "g_nll": 0.027708882931619883,
      "average_nll": 0.013854441465809941,
      "perplexity": 1.013950858995761,
      "avg_token_probability": 0.9862724792310845,
      "sequence_length": 2
    },
    {
      "example_id": "qb_2899--155/155_2893117.txt#0_2",
      "dataset_type": "long",
      "response": "louis xv",
      "accuracy": 1.0,
      "g_nll": 0.1753456788137555,
      "average_nll": 0.05844855960458517,
      "perplexity": 1.0601904476760373,
      "avg_token_probability": 0.9452317758545306,
      "sequence_length": 3
    },
    {
      "example_id": "bt_2646--138/138_2413763.txt#0_2",
      "dataset_type": "long",
      "response": "ambergris",
      "accuracy": 1.0,
      "g_nll": 0.00030203518599591916,
      "average_nll": 0.00010067839533197305,
      "perplexity": 1.0001006834635717,
      "avg_token_probability": 0.9998993353185203,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_17367--193/193_1858126.txt#0_0",
      "dataset_type": "long",
      "response": "franklin d. roosevelt",
      "accuracy": 0.0,
      "g_nll": 0.7158208849650691,
      "average_nll": 0.08947761062063364,
      "perplexity": 1.0936028479492068,
      "avg_token_probability": 0.9260475589583061,
      "sequence_length": 8
    },
    {
      "example_id": "dpql_151--178/178_231745.txt#0_2",
      "dataset_type": "long",
      "response": "denali",
      "accuracy": 1.0,
      "g_nll": 0.5347117185337993,
      "average_nll": 0.26735585926689964,
      "perplexity": 1.3065052956319871,
      "avg_token_probability": 0.7929175949099563,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2345--98/98_1465999.txt#0_0",
      "dataset_type": "long",
      "response": "jaguar",
      "accuracy": 0.0,
      "g_nll": 3.6434837855922524,
      "average_nll": 1.2144945951974175,
      "perplexity": 3.368591131886748,
      "avg_token_probability": 0.6562547048857622,
      "sequence_length": 3
    },
    {
      "example_id": "odql_11051--136/136_235136.txt#0_2",
      "dataset_type": "long",
      "response": "$1000",
      "accuracy": 0.0,
      "g_nll": 1.1718253940343857,
      "average_nll": 0.39060846467812854,
      "perplexity": 1.477879757992213,
      "avg_token_probability": 0.7415871307713277,
      "sequence_length": 3
    },
    {
      "example_id": "tb_47--Hansel_and_Gretel.txt#0_0",
      "dataset_type": "long",
      "response": "the mouse",
      "accuracy": 0.0,
      "g_nll": 4.1313745975494385,
      "average_nll": 2.0656872987747192,
      "perplexity": 7.890719316091966,
      "avg_token_probability": 0.3088019830731754,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7858--119/119_484438.txt#0_2",
      "dataset_type": "long",
      "response": "yahoo",
      "accuracy": 1.0,
      "g_nll": 0.03706123307347298,
      "average_nll": 0.03706123307347298,
      "perplexity": 1.0377565639165935,
      "avg_token_probability": 0.9636171283040633,
      "sequence_length": 1
    },
    {
      "example_id": "qg_1094--4/4_3215155.txt#0_0",
      "dataset_type": "long",
      "response": "eat me",
      "accuracy": 1.0,
      "g_nll": 0.20999518944881856,
      "average_nll": 0.10499759472440928,
      "perplexity": 1.1107079387937988,
      "avg_token_probability": 0.9051588492505233,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1960--194/194_648540.txt#0_1",
      "dataset_type": "long",
      "response": "the church",
      "accuracy": 0.0,
      "g_nll": 1.1509467661380768,
      "average_nll": 0.5754733830690384,
      "perplexity": 1.777971989568324,
      "avg_token_probability": 0.6262396713616742,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11893--64/64_768.txt#0_0",
      "dataset_type": "long",
      "response": "joseph goebbels",
      "accuracy": 1.0,
      "g_nll": 0.5008227694779634,
      "average_nll": 0.08347046157966058,
      "perplexity": 1.0870531049870895,
      "avg_token_probability": 0.9318910569029267,
      "sequence_length": 6
    },
    {
      "example_id": "jp_4375--83/83_2736108.txt#0_0",
      "dataset_type": "long",
      "response": "yosemite",
      "accuracy": 1.0,
      "g_nll": 0.017230850004125386,
      "average_nll": 0.008615425002062693,
      "perplexity": 1.0086526445867752,
      "avg_token_probability": 0.9914573609005979,
      "sequence_length": 2
    },
    {
      "example_id": "jp_553--17/17_1364047.txt#0_2",
      "dataset_type": "long",
      "response": "chechnya",
      "accuracy": 1.0,
      "g_nll": 0.09433013491798192,
      "average_nll": 0.031443378305993974,
      "perplexity": 1.0319429435838186,
      "avg_token_probability": 0.9697247298922296,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_14830--197/197_1800498.txt#0_1",
      "dataset_type": "long",
      "response": "badminton world championship",
      "accuracy": 0.0,
      "g_nll": 2.30021319961088,
      "average_nll": 0.46004263992217603,
      "perplexity": 1.5841415312259959,
      "avg_token_probability": 0.7253464877068566,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_2452--77/77_2081353.txt#0_2",
      "dataset_type": "long",
      "response": "toyota",
      "accuracy": 1.0,
      "g_nll": 0.05626811054935388,
      "average_nll": 0.02813405527467694,
      "perplexity": 1.0285335555283448,
      "avg_token_probability": 0.9726421361078341,
      "sequence_length": 2
    },
    {
      "example_id": "qw_1155--186/186_1072791.txt#0_0",
      "dataset_type": "long",
      "response": "albatross",
      "accuracy": 0.0,
      "g_nll": 0.41437608003241166,
      "average_nll": 0.1381253600108039,
      "perplexity": 1.1481194695222343,
      "avg_token_probability": 0.886917078650716,
      "sequence_length": 3
    },
    {
      "example_id": "qb_250--Peter_Blake_(artist).txt#0_0",
      "dataset_type": "long",
      "response": "peter blake",
      "accuracy": 1.0,
      "g_nll": 0.037230041325074126,
      "average_nll": 0.009307510331268531,
      "perplexity": 1.0093509599033779,
      "avg_token_probability": 0.990761983647888,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_23994--31/31_726041.txt#0_0",
      "dataset_type": "long",
      "response": "richard strauss",
      "accuracy": 1.0,
      "g_nll": 0.2015501821286989,
      "average_nll": 0.05038754553217473,
      "perplexity": 1.0516785907487405,
      "avg_token_probability": 0.9542352868669345,
      "sequence_length": 4
    },
    {
      "example_id": "qw_12027--75/75_2714729.txt#0_0",
      "dataset_type": "long",
      "response": "alzheimer's disease",
      "accuracy": 0.0,
      "g_nll": 0.15696752909570932,
      "average_nll": 0.03924188227392733,
      "perplexity": 1.040022016118502,
      "avg_token_probability": 0.9620455243990138,
      "sequence_length": 4
    },
    {
      "example_id": "qb_7632--22/22_167620.txt#0_1",
      "dataset_type": "long",
      "response": "larry",
      "accuracy": 0.0,
      "g_nll": 2.6642922163009644,
      "average_nll": 1.3321461081504822,
      "perplexity": 3.7891666291598405,
      "avg_token_probability": 0.31028437690321287,
      "sequence_length": 2
    },
    {
      "example_id": "qz_2001--6/6_147605.txt#0_1",
      "dataset_type": "long",
      "response": "pablo neruda",
      "accuracy": 1.0,
      "g_nll": 0.03570166188910662,
      "average_nll": 0.008925415472276654,
      "perplexity": 1.0089653657621411,
      "avg_token_probability": 0.9911532982303223,
      "sequence_length": 4
    },
    {
      "example_id": "qw_2616--7/7_311565.txt#0_0",
      "dataset_type": "long",
      "response": "the ryder cup",
      "accuracy": 1.0,
      "g_nll": 1.0398034335521515,
      "average_nll": 0.2599508583880379,
      "perplexity": 1.2968663549966621,
      "avg_token_probability": 0.7973882862574516,
      "sequence_length": 4
    },
    {
      "example_id": "jp_2085--58/58_325202.txt#0_1",
      "dataset_type": "long",
      "response": "sweeney todd",
      "accuracy": 1.0,
      "g_nll": 0.8956892375135794,
      "average_nll": 0.2985630791711931,
      "perplexity": 1.3479205602269406,
      "avg_token_probability": 0.7931999252333987,
      "sequence_length": 3
    },
    {
      "example_id": "qb_6583--36/36_449032.txt#0_2",
      "dataset_type": "long",
      "response": "football",
      "accuracy": 1.0,
      "g_nll": 0.02240009792149067,
      "average_nll": 0.02240009792149067,
      "perplexity": 1.0226528639143222,
      "avg_token_probability": 0.9778489214535461,
      "sequence_length": 1
    },
    {
      "example_id": "bb_8929--172/172_1033394.txt#0_0",
      "dataset_type": "long",
      "response": "william bligh",
      "accuracy": 1.0,
      "g_nll": 1.0965334046431394,
      "average_nll": 0.27413335116078485,
      "perplexity": 1.315390199353729,
      "avg_token_probability": 0.8326239196662313,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1783--160/160_2646247.txt#0_0",
      "dataset_type": "long",
      "response": "smallpox",
      "accuracy": 1.0,
      "g_nll": 0.00790463836165145,
      "average_nll": 0.002634879453883817,
      "perplexity": 1.002638353799576,
      "avg_token_probability": 0.997374652416885,
      "sequence_length": 3
    },
    {
      "example_id": "qb_3092--129/129_139741.txt#0_0",
      "dataset_type": "long",
      "response": "3-4 months",
      "accuracy": 0.0,
      "g_nll": 0.7937619303265819,
      "average_nll": 0.19844048258164548,
      "perplexity": 1.219499443795698,
      "avg_token_probability": 0.837209700549475,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_20082--77/77_957747.txt#0_1",
      "dataset_type": "long",
      "response": "volkswagen",
      "accuracy": 0.0,
      "g_nll": 0.6847929888172075,
      "average_nll": 0.22826432960573584,
      "perplexity": 1.2564173898564819,
      "avg_token_probability": 0.8343061419482479,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_4588--56/56_232596.txt#0_1",
      "dataset_type": "long",
      "response": "the red-throated diver",
      "accuracy": 0.0,
      "g_nll": 4.382642569951685,
      "average_nll": 0.7304404283252808,
      "perplexity": 2.0759947332396025,
      "avg_token_probability": 0.7036811433240837,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_1303--47/47_1508907.txt#0_1",
      "dataset_type": "long",
      "response": "cranmer",
      "accuracy": 1.0,
      "g_nll": 2.331279993056569,
      "average_nll": 1.1656399965282844,
      "perplexity": 3.2079753194239844,
      "avg_token_probability": 0.5485851063121715,
      "sequence_length": 2
    },
    {
      "example_id": "odql_3698--78/78_264520.txt#0_1",
      "dataset_type": "long",
      "response": "mares tails",
      "accuracy": 0.0,
      "g_nll": 0.5141736268424211,
      "average_nll": 0.25708681342121054,
      "perplexity": 1.2931573853032907,
      "avg_token_probability": 0.7989951221031775,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_1807--198/198_1507629.txt#0_1",
      "dataset_type": "long",
      "response": "kempton park",
      "accuracy": 1.0,
      "g_nll": 0.13696382567286491,
      "average_nll": 0.045654608557621636,
      "perplexity": 1.0467128228641704,
      "avg_token_probability": 0.9560151078170954,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_20006--59/59_2784672.txt#0_0",
      "dataset_type": "long",
      "response": "colchester",
      "accuracy": 1.0,
      "g_nll": 0.0047820880954532186,
      "average_nll": 0.0023910440477266093,
      "perplexity": 1.0023939048732111,
      "avg_token_probability": 0.9976146559852366,
      "sequence_length": 2
    },
    {
      "example_id": "bb_9463--32/32_2691228.txt#0_2",
      "dataset_type": "long",
      "response": "fruits and seeds",
      "accuracy": 1.0,
      "g_nll": 0.2023959120560903,
      "average_nll": 0.05059897801402258,
      "perplexity": 1.0519009732719047,
      "avg_token_probability": 0.9516274006495001,
      "sequence_length": 4
    },
    {
      "example_id": "bb_2276--42/42_883302.txt#0_1",
      "dataset_type": "long",
      "response": "scotland",
      "accuracy": 1.0,
      "g_nll": 0.0043403072049841285,
      "average_nll": 0.0021701536024920642,
      "perplexity": 1.0021725100901597,
      "avg_token_probability": 0.99783239702697,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_25007--68/68_2021815.txt#0_1",
      "dataset_type": "long",
      "response": "faversham",
      "accuracy": 1.0,
      "g_nll": 0.44877007279910686,
      "average_nll": 0.14959002426636894,
      "perplexity": 1.1613580165095299,
      "avg_token_probability": 0.8794608294197607,
      "sequence_length": 3
    },
    {
      "example_id": "qb_574--35/35_280986.txt#0_1",
      "dataset_type": "long",
      "response": "Tokyo",
      "accuracy": 1.0,
      "g_nll": 2.197960214689374,
      "average_nll": 1.098980107344687,
      "perplexity": 3.0011036589904836,
      "avg_token_probability": 0.5421556641989804,
      "sequence_length": 2
    },
    {
      "example_id": "qw_6881--133/133_2704872.txt#0_2",
      "dataset_type": "long",
      "response": "kopassus",
      "accuracy": 0.0,
      "g_nll": 0.03159022207159978,
      "average_nll": 0.010530074023866595,
      "perplexity": 1.0105857103667932,
      "avg_token_probability": 0.9896339691873789,
      "sequence_length": 3
    },
    {
      "example_id": "bb_7700--19/19_416201.txt#0_1",
      "dataset_type": "long",
      "response": "victor hugo",
      "accuracy": 1.0,
      "g_nll": 1.7352865454740822,
      "average_nll": 0.43382163636852056,
      "perplexity": 1.54314360287166,
      "avg_token_probability": 0.7542088382635123,
      "sequence_length": 4
    },
    {
      "example_id": "qb_3569--170/170_366697.txt#0_0",
      "dataset_type": "long",
      "response": "popeye",
      "accuracy": 0.0,
      "g_nll": 0.6301871938630939,
      "average_nll": 0.21006239795436463,
      "perplexity": 1.2337550413457448,
      "avg_token_probability": 0.8245765612758795,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_8812--24/24_1020332.txt#0_0",
      "dataset_type": "long",
      "response": "capricorn",
      "accuracy": 1.0,
      "g_nll": 0.0108627630384035,
      "average_nll": 0.003620921012801167,
      "perplexity": 1.003627484466817,
      "avg_token_probability": 0.9963986672752464,
      "sequence_length": 3
    },
    {
      "example_id": "qw_597--13/13_1061577.txt#0_1",
      "dataset_type": "long",
      "response": "pygmalion",
      "accuracy": 1.0,
      "g_nll": 0.018936246608404872,
      "average_nll": 0.004734061652101218,
      "perplexity": 1.0047452850256893,
      "avg_token_probability": 0.9952883628335021,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_16522--56/56_1003309.txt#0_0",
      "dataset_type": "long",
      "response": "sir edwin landseer",
      "accuracy": 1.0,
      "g_nll": 0.21327566262331743,
      "average_nll": 0.03046795180333106,
      "perplexity": 1.030936849852427,
      "avg_token_probability": 0.9719060410188785,
      "sequence_length": 7
    },
    {
      "example_id": "jp_3307--148/148_1403703.txt#0_0",
      "dataset_type": "long",
      "response": "subgenus",
      "accuracy": 0.0,
      "g_nll": 1.7292629480361938,
      "average_nll": 0.8646314740180969,
      "perplexity": 2.374130995783892,
      "avg_token_probability": 0.47146754180612493,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_10250--131/131_472528.txt#0_0",
      "dataset_type": "long",
      "response": "rat",
      "accuracy": 0.0,
      "g_nll": 0.023947514593601227,
      "average_nll": 0.023947514593601227,
      "perplexity": 1.0242365590078075,
      "avg_token_probability": 0.9763369518548667,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_22454--52/52_1966691.txt#0_0",
      "dataset_type": "long",
      "response": "kirkintilloch",
      "accuracy": 0.0,
      "g_nll": 0.49995659746718957,
      "average_nll": 0.09999131949343791,
      "perplexity": 1.105161324673879,
      "avg_token_probability": 0.9207049756554989,
      "sequence_length": 5
    },
    {
      "example_id": "tc_69--158/158_2486.txt#0_1",
      "dataset_type": "long",
      "response": "from russsia with love",
      "accuracy": 0.0,
      "g_nll": 9.598778931889683,
      "average_nll": 1.5997964886482805,
      "perplexity": 4.952024528633909,
      "avg_token_probability": 0.5958049101636121,
      "sequence_length": 6
    },
    {
      "example_id": "qb_8861--190/190_510847.txt#0_1",
      "dataset_type": "long",
      "response": "euros",
      "accuracy": 0.0,
      "g_nll": 8.883138910867274,
      "average_nll": 4.441569455433637,
      "perplexity": 84.9080966297266,
      "avg_token_probability": 0.49368491272855575,
      "sequence_length": 2
    },
    {
      "example_id": "qf_530--164/164_2459775.txt#0_0",
      "dataset_type": "long",
      "response": "tetanus",
      "accuracy": 1.0,
      "g_nll": 0.002637142999446951,
      "average_nll": 0.0013185714997234754,
      "perplexity": 1.0013194411973343,
      "avg_token_probability": 0.9986830970044637,
      "sequence_length": 2
    },
    {
      "example_id": "wh_299--Pocketful_of_Miracles.txt#0_1",
      "dataset_type": "long",
      "response": "peter falk",
      "accuracy": 1.0,
      "g_nll": 0.10085989425715525,
      "average_nll": 0.025214973564288812,
      "perplexity": 1.0255355598638736,
      "avg_token_probability": 0.9753080495588404,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_22936--27/27_1976951.txt#0_0",
      "dataset_type": "long",
      "response": "batman",
      "accuracy": 1.0,
      "g_nll": 0.1503800294995017,
      "average_nll": 0.07519001474975084,
      "perplexity": 1.0780889842318846,
      "avg_token_probability": 0.9301877266064602,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_4975--25/25_686725.txt#0_0",
      "dataset_type": "long",
      "response": "costa brava",
      "accuracy": 1.0,
      "g_nll": 0.03597504168249088,
      "average_nll": 0.00899376042062272,
      "perplexity": 1.0090343258044554,
      "avg_token_probability": 0.9911138041825669,
      "sequence_length": 4
    },
    {
      "example_id": "odql_7656--38/38_144986.txt#0_0",
      "dataset_type": "long",
      "response": "bellona",
      "accuracy": 1.0,
      "g_nll": 0.030809191826847382,
      "average_nll": 0.015404595913423691,
      "perplexity": 1.0155238583104467,
      "avg_token_probability": 0.984830016198461,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_5542--101/101_701718.txt#0_2",
      "dataset_type": "long",
      "response": "Annie",
      "accuracy": 1.0,
      "g_nll": 0.5852893460541964,
      "average_nll": 0.2926446730270982,
      "perplexity": 1.339966579562903,
      "avg_token_probability": 0.7747138073226753,
      "sequence_length": 2
    },
    {
      "example_id": "qg_1795--111/111_375646.txt#0_1",
      "dataset_type": "long",
      "response": "benjamin franklin",
      "accuracy": 1.0,
      "g_nll": 0.12796377770382605,
      "average_nll": 0.03199094442595651,
      "perplexity": 1.0325081553087327,
      "avg_token_probability": 0.9698868048644324,
      "sequence_length": 4
    },
    {
      "example_id": "qw_13284--106/106_96340.txt#0_1",
      "dataset_type": "long",
      "response": "qu\u00e9bec",
      "accuracy": 1.0,
      "g_nll": 1.0057194232376787,
      "average_nll": 0.5028597116188394,
      "perplexity": 1.6534428860856047,
      "avg_token_probability": 0.6828873298908464,
      "sequence_length": 2
    },
    {
      "example_id": "qw_11437--137/137_654439.txt#0_0",
      "dataset_type": "long",
      "response": "groucho",
      "accuracy": 0.0,
      "g_nll": 4.461058020590144,
      "average_nll": 1.4870193401967147,
      "perplexity": 4.423889737494935,
      "avg_token_probability": 0.42112233502123814,
      "sequence_length": 3
    },
    {
      "example_id": "jp_2683--186/186_1418025.txt#0_2",
      "dataset_type": "long",
      "response": "kix",
      "accuracy": 1.0,
      "g_nll": 0.04743180043897155,
      "average_nll": 0.023715900219485775,
      "perplexity": 1.0239993585688227,
      "avg_token_probability": 0.9768374712594705,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1265--Alan_Lake.txt#0_0",
      "dataset_type": "long",
      "response": "diana dors",
      "accuracy": 1.0,
      "g_nll": 1.6153276247205213,
      "average_nll": 0.5384425415735071,
      "perplexity": 1.713336332996343,
      "avg_token_probability": 0.7136771349168253,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_18204--53/53_1875473.txt#0_1",
      "dataset_type": "long",
      "response": "nicolas poussin",
      "accuracy": 1.0,
      "g_nll": 0.10104420733750885,
      "average_nll": 0.02020884146750177,
      "perplexity": 1.0204144226212206,
      "avg_token_probability": 0.9807722703573706,
      "sequence_length": 5
    },
    {
      "example_id": "qz_3752--50/50_191237.txt#0_0",
      "dataset_type": "long",
      "response": "michael holding",
      "accuracy": 1.0,
      "g_nll": 0.031298485409934074,
      "average_nll": 0.010432828469978025,
      "perplexity": 1.0104874401778836,
      "avg_token_probability": 0.9896649606497951,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_1447--149/149_591689.txt#0_0",
      "dataset_type": "long",
      "response": "mars",
      "accuracy": 1.0,
      "g_nll": 0.6367432475090027,
      "average_nll": 0.6367432475090027,
      "perplexity": 1.8903145570201016,
      "avg_token_probability": 0.52901248434356,
      "sequence_length": 1
    },
    {
      "example_id": "qf_3026--170/170_1354094.txt#0_0",
      "dataset_type": "long",
      "response": "3",
      "accuracy": 0.0,
      "g_nll": 0.15800446271896362,
      "average_nll": 0.15800446271896362,
      "perplexity": 1.1711714213049185,
      "avg_token_probability": 0.8538459714853702,
      "sequence_length": 1
    },
    {
      "example_id": "qf_220--100/100_733704.txt#0_0",
      "dataset_type": "long",
      "response": "michael miles",
      "accuracy": 1.0,
      "g_nll": 0.09667493076995015,
      "average_nll": 0.03222497692331672,
      "perplexity": 1.0327498240489308,
      "avg_token_probability": 0.968466006144905,
      "sequence_length": 3
    },
    {
      "example_id": "qb_804--33/33_287054.txt#0_0",
      "dataset_type": "long",
      "response": "leprosy",
      "accuracy": 1.0,
      "g_nll": 0.08553054230287671,
      "average_nll": 0.02851018076762557,
      "perplexity": 1.0289204859815742,
      "avg_token_probability": 0.9726669608206578,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12312--173/173_2307375.txt#0_0",
      "dataset_type": "long",
      "response": "turkish",
      "accuracy": 0.0,
      "g_nll": 6.528140902519226,
      "average_nll": 3.264070451259613,
      "perplexity": 26.155786608729095,
      "avg_token_probability": 0.4403519137713575,
      "sequence_length": 2
    },
    {
      "example_id": "bb_7976--4/4_843121.txt#0_0",
      "dataset_type": "long",
      "response": "e",
      "accuracy": 1.0,
      "g_nll": 0.2022765725851059,
      "average_nll": 0.2022765725851059,
      "perplexity": 1.2241865377305863,
      "avg_token_probability": 0.8168689731336318,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_13707--150/150_14358.txt#0_0",
      "dataset_type": "long",
      "response": "ambulance driver",
      "accuracy": 1.0,
      "g_nll": 0.13096566777676344,
      "average_nll": 0.04365522259225448,
      "perplexity": 1.0446221306831378,
      "avg_token_probability": 0.9583951658803067,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12262--143/143_282014.txt#0_1",
      "dataset_type": "long",
      "response": "cold comfort farm",
      "accuracy": 0.0,
      "g_nll": 0.09791239132755436,
      "average_nll": 0.03263746377585145,
      "perplexity": 1.0331759076442077,
      "avg_token_probability": 0.9688926573699067,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8177--179/179_12992.txt#0_1",
      "dataset_type": "long",
      "response": "gerald ford",
      "accuracy": 0.0,
      "g_nll": 0.5504903777837171,
      "average_nll": 0.27524518889185856,
      "perplexity": 1.3168535131414523,
      "avg_token_probability": 0.788321696102793,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24363--72/72_2007380.txt#0_0",
      "dataset_type": "long",
      "response": "john constable",
      "accuracy": 1.0,
      "g_nll": 0.016180539468223287,
      "average_nll": 0.005393513156074429,
      "perplexity": 1.0054080843329898,
      "avg_token_probability": 0.99462833377202,
      "sequence_length": 3
    },
    {
      "example_id": "odql_4497--97/97_2162315.txt#0_1",
      "dataset_type": "long",
      "response": "1960",
      "accuracy": 1.0,
      "g_nll": 0.01010286109521985,
      "average_nll": 0.005051430547609925,
      "perplexity": 1.0050642105329064,
      "avg_token_probability": 0.9949637048350765,
      "sequence_length": 2
    },
    {
      "example_id": "odql_1097--173/173_3062526.txt#0_0",
      "dataset_type": "long",
      "response": "james hanratty",
      "accuracy": 0.0,
      "g_nll": 0.04479315175967713,
      "average_nll": 0.008958630351935426,
      "perplexity": 1.0089988789819104,
      "avg_token_probability": 0.9911542033413149,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_2358--33/33_1520459.txt#0_0",
      "dataset_type": "long",
      "response": "agriculture",
      "accuracy": 0.0,
      "g_nll": 0.3158290412975475,
      "average_nll": 0.15791452064877376,
      "perplexity": 1.1710660884597373,
      "avg_token_probability": 0.8644450740143961,
      "sequence_length": 2
    },
    {
      "example_id": "qb_4017--123/123_379315.txt#0_2",
      "dataset_type": "long",
      "response": "privet",
      "accuracy": 1.0,
      "g_nll": 0.001259650154679548,
      "average_nll": 0.000629825077339774,
      "perplexity": 1.0006300234588001,
      "avg_token_probability": 0.9993705494236143,
      "sequence_length": 2
    },
    {
      "example_id": "qg_3182--3/3_2567423.txt#0_0",
      "dataset_type": "long",
      "response": "talk like a pirate day",
      "accuracy": 1.0,
      "g_nll": 0.4975040771969361,
      "average_nll": 0.09950081543938723,
      "perplexity": 1.1046193714897292,
      "avg_token_probability": 0.9171690223949442,
      "sequence_length": 5
    },
    {
      "example_id": "jp_1819--183/183_300683.txt#0_0",
      "dataset_type": "long",
      "response": "gordon ramsay",
      "accuracy": 1.0,
      "g_nll": 0.11812437742082693,
      "average_nll": 0.023624875484165385,
      "perplexity": 1.0239061535402867,
      "avg_token_probability": 0.9776157522141302,
      "sequence_length": 5
    },
    {
      "example_id": "bb_3277--172/172_905779.txt#0_0",
      "dataset_type": "long",
      "response": "zygote",
      "accuracy": 1.0,
      "g_nll": 0.02598496510381665,
      "average_nll": 0.00866165503460555,
      "perplexity": 1.0086992757092297,
      "avg_token_probability": 0.9914497020843998,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23794--112/112_1995157.txt#0_0",
      "dataset_type": "long",
      "response": "speedway",
      "accuracy": 1.0,
      "g_nll": 0.023580492867040448,
      "average_nll": 0.011790246433520224,
      "perplexity": 1.011860025356245,
      "avg_token_probability": 0.988346212821393,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1511--132/132_1375093.txt#0_0",
      "dataset_type": "long",
      "response": "negro",
      "accuracy": 0.0,
      "g_nll": 0.7426340980455279,
      "average_nll": 0.37131704902276397,
      "perplexity": 1.4496426083075673,
      "avg_token_probability": 0.7339985944065135,
      "sequence_length": 2
    },
    {
      "example_id": "qz_127--19/19_99793.txt#0_2",
      "dataset_type": "long",
      "response": "white",
      "accuracy": 1.0,
      "g_nll": 0.2027454376220703,
      "average_nll": 0.2027454376220703,
      "perplexity": 1.2247606505770536,
      "avg_token_probability": 0.8164860616062769,
      "sequence_length": 1
    },
    {
      "example_id": "qb_5539--18/18_3229.txt#0_0",
      "dataset_type": "long",
      "response": "bahrain",
      "accuracy": 1.0,
      "g_nll": 0.021052164954880936,
      "average_nll": 0.010526082477440468,
      "perplexity": 1.0105816765750633,
      "avg_token_probability": 0.9895839213876334,
      "sequence_length": 2
    },
    {
      "example_id": "odql_10409--184/184_5652.txt#0",
      "dataset_type": "long",
      "response": "burkina faso",
      "accuracy": 1.0,
      "g_nll": 0.043659637733973966,
      "average_nll": 0.008731927546794794,
      "perplexity": 1.0087701620320282,
      "avg_token_probability": 0.9913671287672576,
      "sequence_length": 5
    },
    {
      "example_id": "bb_1013--62/62_852710.txt#0_0",
      "dataset_type": "long",
      "response": "Concepcion",
      "accuracy": 1.0,
      "g_nll": 0.25312397442758083,
      "average_nll": 0.08437465814252694,
      "perplexity": 1.088036459174013,
      "avg_token_probability": 0.9247979448319829,
      "sequence_length": 3
    },
    {
      "example_id": "odql_2420--173/173_2123595.txt#0_1",
      "dataset_type": "long",
      "response": "maiquetia",
      "accuracy": 0.0,
      "g_nll": 0.23697303104563616,
      "average_nll": 0.05924325776140904,
      "perplexity": 1.0610333139384613,
      "avg_token_probability": 0.9459882761883294,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_5351--130/130_201583.txt#0_0",
      "dataset_type": "long",
      "response": "four yorkshiremen",
      "accuracy": 0.0,
      "g_nll": 1.2059208778082393,
      "average_nll": 0.3014802194520598,
      "perplexity": 1.3518583743742507,
      "avg_token_probability": 0.8236253250135633,
      "sequence_length": 4
    },
    {
      "example_id": "odql_4457--196/196_990714.txt#0_1",
      "dataset_type": "long",
      "response": "alan freed",
      "accuracy": 1.0,
      "g_nll": 1.7344881892204285,
      "average_nll": 0.8672440946102142,
      "perplexity": 2.380341809028342,
      "avg_token_probability": 0.5784555258938187,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2992--93/93_3215677.txt#0_1",
      "dataset_type": "long",
      "response": "62",
      "accuracy": 0.0,
      "g_nll": 0.049459055066108704,
      "average_nll": 0.049459055066108704,
      "perplexity": 1.0507025703867834,
      "avg_token_probability": 0.951744126439018,
      "sequence_length": 1
    },
    {
      "example_id": "qg_2077--57/57_1351839.txt#0_1",
      "dataset_type": "long",
      "response": "bosnia and herzegovina",
      "accuracy": 0.0,
      "g_nll": 0.09760828674188815,
      "average_nll": 0.013944040963126878,
      "perplexity": 1.0140417125531869,
      "avg_token_probability": 0.9864885899189845,
      "sequence_length": 7
    },
    {
      "example_id": "sfq_1811--116/116_151264.txt#0_0",
      "dataset_type": "long",
      "response": "call my bluff",
      "accuracy": 1.0,
      "g_nll": 0.04135920502449153,
      "average_nll": 0.01378640167483051,
      "perplexity": 1.013881872338197,
      "avg_token_probability": 0.9864932118571835,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8247--176/176_1183968.txt#0_0",
      "dataset_type": "long",
      "response": "baseball",
      "accuracy": 1.0,
      "g_nll": 0.021624020766466856,
      "average_nll": 0.010812010383233428,
      "perplexity": 1.0108706713913516,
      "avg_token_probability": 0.9892540105038959,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_8507--177/177_73240.txt#0_0",
      "dataset_type": "long",
      "response": "tunisia",
      "accuracy": 1.0,
      "g_nll": 0.13679660018533468,
      "average_nll": 0.045598866728444896,
      "perplexity": 1.0466544788029186,
      "avg_token_probability": 0.956934680104605,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8017--6/6_2227411.txt#0_1",
      "dataset_type": "long",
      "response": "belfast",
      "accuracy": 0.0,
      "g_nll": 0.026375222951173782,
      "average_nll": 0.013187611475586891,
      "perplexity": 1.0132749515370982,
      "avg_token_probability": 0.9869847826509186,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24528--186/186_2010986.txt#0_0",
      "dataset_type": "long",
      "response": "viscount",
      "accuracy": 0.0,
      "g_nll": 0.23753513334304444,
      "average_nll": 0.11876756667152222,
      "perplexity": 1.1261081428030002,
      "avg_token_probability": 0.8942808882615259,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14796--60/60_1860775.txt#0_1",
      "dataset_type": "long",
      "response": "MASSACHUSETTS",
      "accuracy": 1.0,
      "g_nll": 5.372648427008244,
      "average_nll": 0.895441404501374,
      "perplexity": 2.448416292948161,
      "avg_token_probability": 0.825827812860625,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_7117--62/62_1628292.txt#0_0",
      "dataset_type": "long",
      "response": "greyfriars bobby",
      "accuracy": 1.0,
      "g_nll": 0.06484191867636468,
      "average_nll": 0.010806986446060781,
      "perplexity": 1.0108655928533659,
      "avg_token_probability": 0.9893623495703476,
      "sequence_length": 6
    },
    {
      "example_id": "odql_8059--174/174_3212822.txt#0_1",
      "dataset_type": "long",
      "response": "tenerife",
      "accuracy": 0.0,
      "g_nll": 2.6545547148562036,
      "average_nll": 1.3272773574281018,
      "perplexity": 3.7707629591921172,
      "avg_token_probability": 0.5350998634928109,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_4709--117/117_679477.txt#0_0",
      "dataset_type": "long",
      "response": "gloucestershire",
      "accuracy": 0.0,
      "g_nll": 1.1072340976677992,
      "average_nll": 0.2768085244169498,
      "perplexity": 1.318913807065613,
      "avg_token_probability": 0.8272295589853758,
      "sequence_length": 4
    },
    {
      "example_id": "qw_11850--49/49_1269426.txt#0_0",
      "dataset_type": "long",
      "response": "venice",
      "accuracy": 1.0,
      "g_nll": 0.014929304819361278,
      "average_nll": 0.007464652409680639,
      "perplexity": 1.0074925823800678,
      "avg_token_probability": 0.9925907570471839,
      "sequence_length": 2
    },
    {
      "example_id": "qg_1429--123/123_375481.txt#0_2",
      "dataset_type": "long",
      "response": "sherlock holmes",
      "accuracy": 1.0,
      "g_nll": 0.004505467283706821,
      "average_nll": 0.0011263668209267053,
      "perplexity": 1.001127001410272,
      "avg_token_probability": 0.9988751915060484,
      "sequence_length": 4
    },
    {
      "example_id": "qw_10270--138/138_49533.txt#0_0",
      "dataset_type": "long",
      "response": "1982",
      "accuracy": 1.0,
      "g_nll": 0.008298132073832676,
      "average_nll": 0.004149066036916338,
      "perplexity": 1.0041576853279521,
      "avg_token_probability": 0.9958664226710712,
      "sequence_length": 2
    },
    {
      "example_id": "tc_2833--59/59_86475.txt#0_0",
      "dataset_type": "long",
      "response": "cellulose",
      "accuracy": 1.0,
      "g_nll": 0.04236819493235089,
      "average_nll": 0.014122731644116962,
      "perplexity": 1.014222928547678,
      "avg_token_probability": 0.9861054975723261,
      "sequence_length": 3
    },
    {
      "example_id": "qb_9685--66/66_533419.txt#0_1",
      "dataset_type": "long",
      "response": "failure",
      "accuracy": 1.0,
      "g_nll": 0.030749116092920303,
      "average_nll": 0.030749116092920303,
      "perplexity": 1.0312267532327564,
      "avg_token_probability": 0.9697188294088913,
      "sequence_length": 1
    },
    {
      "example_id": "odql_6714--124/124_3212527.txt#0_0",
      "dataset_type": "long",
      "response": "oasis",
      "accuracy": 1.0,
      "g_nll": 0.1169991996139288,
      "average_nll": 0.0584995998069644,
      "perplexity": 1.0602445613920228,
      "avg_token_probability": 0.9439945950991973,
      "sequence_length": 2
    },
    {
      "example_id": "qw_15940--120/120_2983777.txt#0_0",
      "dataset_type": "long",
      "response": "primal scream",
      "accuracy": 1.0,
      "g_nll": 0.01835016098812048,
      "average_nll": 0.006116720329373493,
      "perplexity": 1.0061354656636667,
      "avg_token_probability": 0.9939202414944922,
      "sequence_length": 3
    },
    {
      "example_id": "bb_5924--97/97_919135.txt#0_2",
      "dataset_type": "long",
      "response": "squamata",
      "accuracy": 0.0,
      "g_nll": 2.5695890868082643,
      "average_nll": 0.8565296956027547,
      "perplexity": 2.3549740200997538,
      "avg_token_probability": 0.6892325330836054,
      "sequence_length": 3
    },
    {
      "example_id": "qb_2710--0/0_341544.txt#0_2",
      "dataset_type": "long",
      "response": "mercury",
      "accuracy": 1.0,
      "g_nll": 0.010793904773890972,
      "average_nll": 0.005396952386945486,
      "perplexity": 1.0054115421694576,
      "avg_token_probability": 0.9946186692756035,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_5912--30/30_711491.txt#0_0",
      "dataset_type": "long",
      "response": "rugby",
      "accuracy": 1.0,
      "g_nll": 0.4250963139347732,
      "average_nll": 0.2125481569673866,
      "perplexity": 1.2368256739047916,
      "avg_token_probability": 0.8255717552900553,
      "sequence_length": 2
    },
    {
      "example_id": "jp_2368--22/22_1179904.txt#0_1",
      "dataset_type": "long",
      "response": "the rio grande",
      "accuracy": 1.0,
      "g_nll": 2.652244931552559,
      "average_nll": 0.8840816438508531,
      "perplexity": 2.4207602503035237,
      "avg_token_probability": 0.6884498903207307,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_2455--24/24_81962.txt#0_0",
      "dataset_type": "long",
      "response": "mata hari",
      "accuracy": 1.0,
      "g_nll": 0.028997160028666258,
      "average_nll": 0.00966572000955542,
      "perplexity": 1.0097125839507535,
      "avg_token_probability": 0.9903860979251965,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1013--32/32_749045.txt#0_1",
      "dataset_type": "long",
      "response": "leeds",
      "accuracy": 1.0,
      "g_nll": 0.03465967599368014,
      "average_nll": 0.01732983799684007,
      "perplexity": 1.017480870836134,
      "avg_token_probability": 0.9829666578793741,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_12851--98/98_131425.txt#0_0",
      "dataset_type": "long",
      "response": "33",
      "accuracy": 1.0,
      "g_nll": 0.000291662581730634,
      "average_nll": 0.000291662581730634,
      "perplexity": 1.000291705119397,
      "avg_token_probability": 0.9997083799476653,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_12855--189/189_1756387.txt#0_1",
      "dataset_type": "long",
      "response": "the humber",
      "accuracy": 1.0,
      "g_nll": 0.539311885832376,
      "average_nll": 0.179770628610792,
      "perplexity": 1.1969427872031808,
      "avg_token_probability": 0.8514697994772421,
      "sequence_length": 3
    },
    {
      "example_id": "jp_3645--146/146_1169963.txt#0_0",
      "dataset_type": "long",
      "response": "taiwan",
      "accuracy": 1.0,
      "g_nll": 0.05421061689776252,
      "average_nll": 0.018070205632587506,
      "perplexity": 1.0182344596748636,
      "avg_token_probability": 0.9823013770970568,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12368--150/150_2308380.txt#0_0",
      "dataset_type": "long",
      "response": "bristol",
      "accuracy": 1.0,
      "g_nll": 1.3645123452879488,
      "average_nll": 0.6822561726439744,
      "perplexity": 1.978336168782511,
      "avg_token_probability": 0.6255338709737015,
      "sequence_length": 2
    },
    {
      "example_id": "qb_6543--195/195_826037.txt#0_0",
      "dataset_type": "long",
      "response": "john f. kennedy international airport",
      "accuracy": 0.0,
      "g_nll": 0.46549034862755434,
      "average_nll": 0.06649862123250776,
      "perplexity": 1.068759490503315,
      "avg_token_probability": 0.9379283011323981,
      "sequence_length": 7
    },
    {
      "example_id": "odql_6620--156/156_349458.txt#0_0",
      "dataset_type": "long",
      "response": "sisyphus",
      "accuracy": 1.0,
      "g_nll": 0.010732095327739444,
      "average_nll": 0.002683023831934861,
      "perplexity": 1.0026866263615466,
      "avg_token_probability": 0.9973312287481741,
      "sequence_length": 4
    },
    {
      "example_id": "jp_3697--38/38_1394540.txt#0_1",
      "dataset_type": "long",
      "response": "albania",
      "accuracy": 0.0,
      "g_nll": 0.004970261292321254,
      "average_nll": 0.0016567537641070846,
      "perplexity": 1.0016581269388574,
      "avg_token_probability": 0.998347302851692,
      "sequence_length": 3
    },
    {
      "example_id": "wh_585--16/16_792479.txt#0_1",
      "dataset_type": "long",
      "response": "lowestoft",
      "accuracy": 1.0,
      "g_nll": 0.887434780559488,
      "average_nll": 0.443717390279744,
      "perplexity": 1.558489978962139,
      "avg_token_probability": 0.7058527038877392,
      "sequence_length": 2
    },
    {
      "example_id": "bt_2178--180/180_535996.txt#0_2",
      "dataset_type": "long",
      "response": "bedser",
      "accuracy": 1.0,
      "g_nll": 0.022734610240149777,
      "average_nll": 0.011367305120074889,
      "perplexity": 1.0114321584357748,
      "avg_token_probability": 0.9887602300573239,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_16988--36/36_2777799.txt#0_0",
      "dataset_type": "long",
      "response": "ts eliot",
      "accuracy": 1.0,
      "g_nll": 2.4490184974856675,
      "average_nll": 0.8163394991618892,
      "perplexity": 2.2622038639466475,
      "avg_token_probability": 0.6945289811716702,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3892--168/168_918990.txt#0_0",
      "dataset_type": "long",
      "response": "bees",
      "accuracy": 1.0,
      "g_nll": 2.182213048450649,
      "average_nll": 1.0911065242253244,
      "perplexity": 2.9775670003933317,
      "avg_token_probability": 0.5540314989080011,
      "sequence_length": 2
    },
    {
      "example_id": "odql_2538--189/189_425837.txt#0_1",
      "dataset_type": "long",
      "response": "volleyball",
      "accuracy": 1.0,
      "g_nll": 0.008225827710703015,
      "average_nll": 0.004112913855351508,
      "perplexity": 1.0041213834931917,
      "avg_token_probability": 0.9958961627956251,
      "sequence_length": 2
    },
    {
      "example_id": "qf_1723--128/128_76305.txt#0_1",
      "dataset_type": "long",
      "response": "cardinals",
      "accuracy": 1.0,
      "g_nll": 0.2553200721636131,
      "average_nll": 0.12766003608180654,
      "perplexity": 1.136166681356408,
      "avg_token_probability": 0.887333743864922,
      "sequence_length": 2
    },
    {
      "example_id": "qz_6774--13/13_1663667.txt#0_0",
      "dataset_type": "long",
      "response": "lithium",
      "accuracy": 1.0,
      "g_nll": 0.02219094429165125,
      "average_nll": 0.011095472145825624,
      "perplexity": 1.011157255189472,
      "avg_token_probability": 0.9889769506427935,
      "sequence_length": 2
    }
  ]
}