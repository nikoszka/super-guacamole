{
  "short": [
    {
      "example_id": "dpql_3081--77/77_636065.txt#0_2",
      "dataset_type": "short",
      "response": "donald trump",
      "accuracy": 1.0,
      "g_nll": 0.057121010911941994,
      "average_nll": 0.014280252727985498,
      "perplexity": 1.0143827026262298,
      "avg_token_probability": 0.9858818806192788,
      "sequence_length": 4
    },
    {
      "example_id": "qw_11987--33/33_444596.txt#0_0",
      "dataset_type": "short",
      "response": "puerto rico",
      "accuracy": 1.0,
      "g_nll": 0.06445912318304181,
      "average_nll": 0.02148637439434727,
      "perplexity": 1.0217188687041439,
      "avg_token_probability": 0.9788552533191669,
      "sequence_length": 3
    },
    {
      "example_id": "odql_13428--179/179_2328241.txt#0_2",
      "dataset_type": "short",
      "response": "romania",
      "accuracy": 1.0,
      "g_nll": 0.15764052025224373,
      "average_nll": 0.052546840084081246,
      "perplexity": 1.0539519281193042,
      "avg_token_probability": 0.9505203804116428,
      "sequence_length": 3
    },
    {
      "example_id": "wh_252--162/162_2659147.txt#0_1",
      "dataset_type": "short",
      "response": "the battle of the Somme",
      "accuracy": 1.0,
      "g_nll": 1.353673068541866,
      "average_nll": 0.1933818669345523,
      "perplexity": 1.2133460417974227,
      "avg_token_probability": 0.8736054266780942,
      "sequence_length": 7
    },
    {
      "example_id": "qz_3971--21/21_196365.txt#0_0",
      "dataset_type": "short",
      "response": "michelle ryan",
      "accuracy": 1.0,
      "g_nll": 0.07209660383340832,
      "average_nll": 0.014419320766681665,
      "perplexity": 1.014523780648666,
      "avg_token_probability": 0.9857758664398023,
      "sequence_length": 5
    },
    {
      "example_id": "qg_762--191/191_2525665.txt#0_2",
      "dataset_type": "short",
      "response": "adam's apple",
      "accuracy": 0.0,
      "g_nll": 0.15283071692101657,
      "average_nll": 0.038207679230254143,
      "perplexity": 1.0389469781835168,
      "avg_token_probability": 0.9637927815686255,
      "sequence_length": 4
    },
    {
      "example_id": "qb_2537--177/177_336227.txt#0_1",
      "dataset_type": "short",
      "response": "transfusion",
      "accuracy": 0.0,
      "g_nll": 0.10579279571538791,
      "average_nll": 0.035264265238462635,
      "perplexity": 1.0358934232539407,
      "avg_token_probability": 0.9661222866712958,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2182--78/78_65006.txt#0_0",
      "dataset_type": "short",
      "response": "mae west",
      "accuracy": 1.0,
      "g_nll": 0.8856621903721589,
      "average_nll": 0.22141554759303972,
      "perplexity": 1.2478418605220782,
      "avg_token_probability": 0.8488398091556615,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_14528--95/95_493438.txt#0_0",
      "dataset_type": "short",
      "response": "ship",
      "accuracy": 0.0,
      "g_nll": 6.523057699203491,
      "average_nll": 3.2615288496017456,
      "perplexity": 26.08939342637291,
      "avg_token_probability": 0.052939116343812036,
      "sequence_length": 2
    },
    {
      "example_id": "bb_8426--98/98_1020444.txt#0_1",
      "dataset_type": "short",
      "response": "goat",
      "accuracy": 1.0,
      "g_nll": 0.12199973873794079,
      "average_nll": 0.060999869368970394,
      "perplexity": 1.0628987753396248,
      "avg_token_probability": 0.9418376357092528,
      "sequence_length": 2
    },
    {
      "example_id": "odql_9757--12/12_139633.txt#0_2",
      "dataset_type": "short",
      "response": "goliath",
      "accuracy": 1.0,
      "g_nll": 0.5983963064095406,
      "average_nll": 0.14959907660238514,
      "perplexity": 1.1613685295601142,
      "avg_token_probability": 0.8867576353445548,
      "sequence_length": 4
    },
    {
      "example_id": "bb_4945--Germany.txt#0_0",
      "dataset_type": "short",
      "response": "bundesliga",
      "accuracy": 0.0,
      "g_nll": 0.9228661056840792,
      "average_nll": 0.2307165264210198,
      "perplexity": 1.2595021532562554,
      "avg_token_probability": 0.8356296306345392,
      "sequence_length": 4
    },
    {
      "example_id": "qz_5999--134/134_385487.txt#0_0",
      "dataset_type": "short",
      "response": "stan getz",
      "accuracy": 0.0,
      "g_nll": 1.8094930425273787,
      "average_nll": 0.4523732606318447,
      "perplexity": 1.5720386192086024,
      "avg_token_probability": 0.7246285096623725,
      "sequence_length": 4
    },
    {
      "example_id": "qf_944--160/160_2466400.txt#0_1",
      "dataset_type": "short",
      "response": "basketball",
      "accuracy": 1.0,
      "g_nll": 0.018305400852113962,
      "average_nll": 0.009152700426056981,
      "perplexity": 1.0091947144714324,
      "avg_token_probability": 0.9908967998965164,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_127--7/7_554431.txt#0_1",
      "dataset_type": "short",
      "response": "melrose",
      "accuracy": 1.0,
      "g_nll": 0.9313248650723835,
      "average_nll": 0.31044162169079453,
      "perplexity": 1.3640273652101342,
      "avg_token_probability": 0.7924401504827133,
      "sequence_length": 3
    },
    {
      "example_id": "odql_5884--11/11_405432.txt#0_0",
      "dataset_type": "short",
      "response": "indian ocean",
      "accuracy": 0.0,
      "g_nll": 0.543491305783391,
      "average_nll": 0.18116376859446368,
      "perplexity": 1.1986114581344127,
      "avg_token_probability": 0.8578212355018042,
      "sequence_length": 3
    },
    {
      "example_id": "qz_1794--3/3_142632.txt#0_1",
      "dataset_type": "short",
      "response": "sunday post",
      "accuracy": 1.0,
      "g_nll": 0.15978885802906007,
      "average_nll": 0.05326295267635336,
      "perplexity": 1.0547069466735615,
      "avg_token_probability": 0.949933523470211,
      "sequence_length": 3
    },
    {
      "example_id": "qb_6369--37/37_442927.txt#0_0",
      "dataset_type": "short",
      "response": "bosworth field",
      "accuracy": 0.0,
      "g_nll": 1.32372201997714,
      "average_nll": 0.330930504994285,
      "perplexity": 1.3922630336186324,
      "avg_token_probability": 0.795496998220844,
      "sequence_length": 4
    },
    {
      "example_id": "odql_6714--124/124_3212527.txt#0_0",
      "dataset_type": "short",
      "response": "oasis",
      "accuracy": 1.0,
      "g_nll": 0.10553970001637936,
      "average_nll": 0.05276985000818968,
      "perplexity": 1.0541869960690766,
      "avg_token_probability": 0.9489582536237673,
      "sequence_length": 2
    },
    {
      "example_id": "bb_4857--20/20_940277.txt#0_1",
      "dataset_type": "short",
      "response": "barbator",
      "accuracy": 0.0,
      "g_nll": 1.03488090634346,
      "average_nll": 0.258720226585865,
      "perplexity": 1.2952713716369526,
      "avg_token_probability": 0.787990458863542,
      "sequence_length": 4
    },
    {
      "example_id": "odql_11176--12/12_2284937.txt#0_0",
      "dataset_type": "short",
      "response": "bcc",
      "accuracy": 1.0,
      "g_nll": 0.10260336226201616,
      "average_nll": 0.03420112075400539,
      "perplexity": 1.0347927040899532,
      "avg_token_probability": 0.9671886227258729,
      "sequence_length": 3
    },
    {
      "example_id": "bt_3472--182/182_791380.txt#0_1",
      "dataset_type": "short",
      "response": "ice",
      "accuracy": 0.0,
      "g_nll": 3.6206631660461426,
      "average_nll": 1.8103315830230713,
      "perplexity": 6.11247388881324,
      "avg_token_probability": 0.2077310248154602,
      "sequence_length": 2
    },
    {
      "example_id": "qw_15697--74/74_1333011.txt#0_0",
      "dataset_type": "short",
      "response": "kis",
      "accuracy": 0.0,
      "g_nll": 3.4983403086662292,
      "average_nll": 1.1661134362220764,
      "perplexity": 3.209494461859694,
      "avg_token_probability": 0.3423689003615225,
      "sequence_length": 3
    },
    {
      "example_id": "tb_1169--191/191_2063376.txt#0_2",
      "dataset_type": "short",
      "response": "luxor",
      "accuracy": 1.0,
      "g_nll": 0.10957634076089562,
      "average_nll": 0.03652544692029854,
      "perplexity": 1.0372006972454115,
      "avg_token_probability": 0.9650139906710519,
      "sequence_length": 3
    },
    {
      "example_id": "qw_11463--117/117_93387.txt#0_0",
      "dataset_type": "short",
      "response": "moon",
      "accuracy": 1.0,
      "g_nll": 0.06948451325297356,
      "average_nll": 0.03474225662648678,
      "perplexity": 1.0353528190782066,
      "avg_token_probability": 0.9660247577929859,
      "sequence_length": 2
    },
    {
      "example_id": "wh_2955--50/50_794801.txt#0_0",
      "dataset_type": "short",
      "response": "italy",
      "accuracy": 0.0,
      "g_nll": 0.5999116076836799,
      "average_nll": 0.19997053589455996,
      "perplexity": 1.2213667711506837,
      "avg_token_probability": 0.8439626941001741,
      "sequence_length": 3
    },
    {
      "example_id": "qg_4089--181/181_2265370.txt#0_2",
      "dataset_type": "short",
      "response": "washington",
      "accuracy": 1.0,
      "g_nll": 0.04464653134346008,
      "average_nll": 0.02232326567173004,
      "perplexity": 1.0225742942124456,
      "avg_token_probability": 0.977983632797653,
      "sequence_length": 2
    },
    {
      "example_id": "bb_5497--152/152_643813.txt#0_2",
      "dataset_type": "short",
      "response": "portugal",
      "accuracy": 1.0,
      "g_nll": 0.023740120651382313,
      "average_nll": 0.00791337355046077,
      "perplexity": 1.0079447670457833,
      "avg_token_probability": 0.9921633176521866,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_17967--123/123_103337.txt#0_0",
      "dataset_type": "short",
      "response": "charles atlas",
      "accuracy": 0.0,
      "g_nll": 0.013550713470976916,
      "average_nll": 0.003387678367744229,
      "perplexity": 1.0033934230353028,
      "avg_token_probability": 0.9966212393253678,
      "sequence_length": 4
    },
    {
      "example_id": "bb_3805--121/121_2943039.txt#0_2",
      "dataset_type": "short",
      "response": "coconut shy",
      "accuracy": 0.0,
      "g_nll": 0.045321233570575714,
      "average_nll": 0.022660616785287857,
      "perplexity": 1.022919318983267,
      "avg_token_probability": 0.9775945843007191,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_6274--62/62_943590.txt#0_0",
      "dataset_type": "short",
      "response": "giuliano bugiardini",
      "accuracy": 0.0,
      "g_nll": 4.371278167189303,
      "average_nll": 0.6244683095984718,
      "perplexity": 1.867252893013766,
      "avg_token_probability": 0.806084193074196,
      "sequence_length": 7
    },
    {
      "example_id": "sfq_7181--189/189_1916422.txt#0_1",
      "dataset_type": "short",
      "response": "sternum",
      "accuracy": 0.0,
      "g_nll": 0.01482884312281385,
      "average_nll": 0.004942947707604617,
      "perplexity": 1.0049551842268087,
      "avg_token_probability": 0.9950794548888847,
      "sequence_length": 3
    },
    {
      "example_id": "qg_488--37/37_82446.txt#0_0",
      "dataset_type": "short",
      "response": "galileo galilei",
      "accuracy": 1.0,
      "g_nll": 0.16278978229820495,
      "average_nll": 0.02713163038303416,
      "perplexity": 1.0275030444816484,
      "avg_token_probability": 0.97357796472182,
      "sequence_length": 6
    },
    {
      "example_id": "qf_1931--68/68_2482724.txt#0_0",
      "dataset_type": "short",
      "response": "midnight cowboy",
      "accuracy": 1.0,
      "g_nll": 0.029700138606131077,
      "average_nll": 0.009900046202043692,
      "perplexity": 1.0099492137792587,
      "avg_token_probability": 0.9901801930705759,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1696--66/66_26750.txt#0_0",
      "dataset_type": "short",
      "response": "tanzania",
      "accuracy": 1.0,
      "g_nll": 0.14801737474374477,
      "average_nll": 0.03700434368593619,
      "perplexity": 1.0376975282605272,
      "avg_token_probability": 0.9650354671120837,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1494--24/24_306802.txt#0_1",
      "dataset_type": "short",
      "response": "sheffield",
      "accuracy": 1.0,
      "g_nll": 0.04139953802223317,
      "average_nll": 0.013799846007411057,
      "perplexity": 1.0138955033949162,
      "avg_token_probability": 0.9863851998505556,
      "sequence_length": 3
    },
    {
      "example_id": "qb_5390--56/56_2628509.txt#0_1",
      "dataset_type": "short",
      "response": "malvinas",
      "accuracy": 0.0,
      "g_nll": 0.13358226487707725,
      "average_nll": 0.03339556621926931,
      "perplexity": 1.0339594577923874,
      "avg_token_probability": 0.9682180516654418,
      "sequence_length": 4
    },
    {
      "example_id": "qw_661--149/149_1063026.txt#0_1",
      "dataset_type": "short",
      "response": "france",
      "accuracy": 1.0,
      "g_nll": 0.026257462333887815,
      "average_nll": 0.013128731166943908,
      "perplexity": 1.013215291351634,
      "avg_token_probability": 0.9869739820826131,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8182--60/60_558286.txt#0_0",
      "dataset_type": "short",
      "response": "smell",
      "accuracy": 1.0,
      "g_nll": 0.06590495817363262,
      "average_nll": 0.03295247908681631,
      "perplexity": 1.0335014251428418,
      "avg_token_probability": 0.9678140762629224,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14324--47/47_1545305.txt#0_1",
      "dataset_type": "short",
      "response": "audi",
      "accuracy": 1.0,
      "g_nll": 0.021022357046604156,
      "average_nll": 0.021022357046604156,
      "perplexity": 1.0212448834017547,
      "avg_token_probability": 0.9791970723701564,
      "sequence_length": 1
    },
    {
      "example_id": "bt_4353--105/105_2445487.txt#0_0",
      "dataset_type": "short",
      "response": "sheryl crow",
      "accuracy": 1.0,
      "g_nll": 0.052072396014409605,
      "average_nll": 0.013018099003602401,
      "perplexity": 1.0131032033524041,
      "avg_token_probability": 0.9872255787013112,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2257--150/150_49392.txt#0_0",
      "dataset_type": "short",
      "response": "december",
      "accuracy": 1.0,
      "g_nll": 0.3500560633838177,
      "average_nll": 0.17502803169190884,
      "perplexity": 1.1912796097273204,
      "avg_token_probability": 0.8494563135498462,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_20420--125/125_1922602.txt#0_1",
      "dataset_type": "short",
      "response": "wisconsin",
      "accuracy": 1.0,
      "g_nll": 0.02709927680916735,
      "average_nll": 0.00903309226972245,
      "perplexity": 1.0090740137707899,
      "avg_token_probability": 0.9910436731978169,
      "sequence_length": 3
    },
    {
      "example_id": "bt_4479--28/28_2447641.txt#0",
      "dataset_type": "short",
      "response": "the press gang",
      "accuracy": 0.0,
      "g_nll": 0.6945956689596642,
      "average_nll": 0.17364891723991605,
      "perplexity": 1.1896378311616704,
      "avg_token_probability": 0.8516302992607641,
      "sequence_length": 4
    },
    {
      "example_id": "qb_3147--114/114_354667.txt#0_2",
      "dataset_type": "short",
      "response": "the gondoliers",
      "accuracy": 1.0,
      "g_nll": 0.5514095281978371,
      "average_nll": 0.09190158803297284,
      "perplexity": 1.0962569319710227,
      "avg_token_probability": 0.9199934648997735,
      "sequence_length": 6
    },
    {
      "example_id": "qb_388--149/149_18617.txt#0_2",
      "dataset_type": "short",
      "response": "billie jean",
      "accuracy": 0.0,
      "g_nll": 3.173601734517433,
      "average_nll": 0.7934004336293583,
      "perplexity": 2.2109016828942134,
      "avg_token_probability": 0.6298335188829844,
      "sequence_length": 4
    },
    {
      "example_id": "qz_3777--128/128_191983.txt#0_2",
      "dataset_type": "short",
      "response": "Vienna",
      "accuracy": 1.0,
      "g_nll": 1.857611607760191,
      "average_nll": 0.9288058038800955,
      "perplexity": 2.5314842831102076,
      "avg_token_probability": 0.5641525936177205,
      "sequence_length": 2
    },
    {
      "example_id": "qw_1816--138/138_1086182.txt#0_2",
      "dataset_type": "short",
      "response": "paul gauguin",
      "accuracy": 1.0,
      "g_nll": 0.07881787000206941,
      "average_nll": 0.015763574000413883,
      "perplexity": 1.0158884745631027,
      "avg_token_probability": 0.9845608358017165,
      "sequence_length": 5
    },
    {
      "example_id": "qb_6828--25/25_2632281.txt#0_0",
      "dataset_type": "short",
      "response": "young",
      "accuracy": 0.0,
      "g_nll": 0.7348809391260147,
      "average_nll": 0.36744046956300735,
      "perplexity": 1.4440338320057116,
      "avg_token_probability": 0.7115694980930207,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24386--33/33_2007786.txt#0_2",
      "dataset_type": "short",
      "response": "squeeze",
      "accuracy": 1.0,
      "g_nll": 0.00011657988943625242,
      "average_nll": 0.00011657988943625242,
      "perplexity": 1.0001165866851356,
      "avg_token_probability": 0.999883426905735,
      "sequence_length": 1
    },
    {
      "example_id": "qb_3702--159/159_370045.txt#0_0",
      "dataset_type": "short",
      "response": "freedom",
      "accuracy": 0.0,
      "g_nll": 9.059049606323242,
      "average_nll": 4.529524803161621,
      "perplexity": 92.71449297849396,
      "avg_token_probability": 0.010821311202848165,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_13345--191/191_1767828.txt#0_0",
      "dataset_type": "short",
      "response": "mir",
      "accuracy": 1.0,
      "g_nll": 0.04371986165642738,
      "average_nll": 0.02185993082821369,
      "perplexity": 1.0221006096576066,
      "avg_token_probability": 0.9784030713141305,
      "sequence_length": 2
    },
    {
      "example_id": "qw_12886--79/79_2977981.txt#0_1",
      "dataset_type": "short",
      "response": "lisieux",
      "accuracy": 1.0,
      "g_nll": 0.014629985201963791,
      "average_nll": 0.0073149926009818955,
      "perplexity": 1.0073418125152995,
      "avg_token_probability": 0.9927382366149433,
      "sequence_length": 2
    },
    {
      "example_id": "qf_2195--13/13_325145.txt#0_0",
      "dataset_type": "short",
      "response": "andre agassi",
      "accuracy": 1.0,
      "g_nll": 0.042818056705755225,
      "average_nll": 0.010704514176438806,
      "perplexity": 1.0107620124689327,
      "avg_token_probability": 0.9894273834216738,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5330--140/140_950316.txt#0_1",
      "dataset_type": "short",
      "response": "red sea",
      "accuracy": 1.0,
      "g_nll": 0.6312423478811979,
      "average_nll": 0.31562117394059896,
      "perplexity": 1.3711107447496906,
      "avg_token_probability": 0.7645835438714685,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_3467--85/85_645957.txt#0_1",
      "dataset_type": "short",
      "response": "willie nelson",
      "accuracy": 1.0,
      "g_nll": 0.22511930945029235,
      "average_nll": 0.04502386189005847,
      "perplexity": 1.0460528204083093,
      "avg_token_probability": 0.9574074713540197,
      "sequence_length": 5
    },
    {
      "example_id": "odql_3325--52/52_2810331.txt#0_2",
      "dataset_type": "short",
      "response": "cumbria",
      "accuracy": 0.0,
      "g_nll": 1.313666242229374,
      "average_nll": 0.3284165605573435,
      "perplexity": 1.3887673575192807,
      "avg_token_probability": 0.7925602508661034,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1076--114/114_1377217.txt#0_1",
      "dataset_type": "short",
      "response": "Maine",
      "accuracy": 1.0,
      "g_nll": 4.562994170933962,
      "average_nll": 2.281497085466981,
      "perplexity": 9.79132789722006,
      "avg_token_probability": 0.4922710118875425,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1706--93/93_2394404.txt#0_0",
      "dataset_type": "short",
      "response": "heavy duty",
      "accuracy": 0.0,
      "g_nll": 0.9472898654639721,
      "average_nll": 0.3157632884879907,
      "perplexity": 1.3713056133791102,
      "avg_token_probability": 0.7586893796767252,
      "sequence_length": 3
    },
    {
      "example_id": "tc_635--8/8_16499.txt#0_0",
      "dataset_type": "short",
      "response": "peru",
      "accuracy": 1.0,
      "g_nll": 0.14172695018351078,
      "average_nll": 0.07086347509175539,
      "perplexity": 1.0734346652948146,
      "avg_token_probability": 0.9328988164560892,
      "sequence_length": 2
    },
    {
      "example_id": "qg_1526--104/104_341553.txt#0_2",
      "dataset_type": "short",
      "response": "mercury",
      "accuracy": 1.0,
      "g_nll": 0.02591078431578353,
      "average_nll": 0.012955392157891765,
      "perplexity": 1.013039676837937,
      "avg_token_probability": 0.9872077677356141,
      "sequence_length": 2
    },
    {
      "example_id": "bb_6324--63/63_972094.txt#0_0",
      "dataset_type": "short",
      "response": "zambia",
      "accuracy": 1.0,
      "g_nll": 0.06948417211242486,
      "average_nll": 0.02316139070414162,
      "perplexity": 1.0234316985817675,
      "avg_token_probability": 0.9772386457570782,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22542--154/154_1968503.txt#0_0",
      "dataset_type": "short",
      "response": "daewoo",
      "accuracy": 1.0,
      "g_nll": 0.4917471291237234,
      "average_nll": 0.12293678228093086,
      "perplexity": 1.130812931273597,
      "avg_token_probability": 0.9005899215349948,
      "sequence_length": 4
    },
    {
      "example_id": "odql_3105--120/120_2809792.txt#0_0",
      "dataset_type": "short",
      "response": "enid blyton",
      "accuracy": 0.0,
      "g_nll": 0.10312307439707524,
      "average_nll": 0.020624614879415047,
      "perplexity": 1.0208387720177645,
      "avg_token_probability": 0.9800753475385374,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_6987--15/15_2753573.txt#0_0",
      "dataset_type": "short",
      "response": "photography",
      "accuracy": 0.0,
      "g_nll": 0.016413562814705074,
      "average_nll": 0.008206781407352537,
      "perplexity": 1.0082405493500515,
      "avg_token_probability": 0.9918513419104937,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8861--190/190_510847.txt#0_0",
      "dataset_type": "short",
      "response": "lita",
      "accuracy": 0.0,
      "g_nll": 2.650904081761837,
      "average_nll": 0.8836346939206123,
      "perplexity": 2.419678533433168,
      "avg_token_probability": 0.49339989199167533,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1783--27/27_359862.txt#0_1",
      "dataset_type": "short",
      "response": "heather stanning and helen glover",
      "accuracy": 1.0,
      "g_nll": 2.2023200771540132,
      "average_nll": 0.22023200771540133,
      "perplexity": 1.2463658635421004,
      "avg_token_probability": 0.874608037653054,
      "sequence_length": 10
    },
    {
      "example_id": "sfq_8966--137/137_1670959.txt#0_2",
      "dataset_type": "short",
      "response": "mussolini",
      "accuracy": 1.0,
      "g_nll": 0.23578712856397033,
      "average_nll": 0.07859570952132344,
      "perplexity": 1.0817668855985776,
      "avg_token_probability": 0.9273957979900205,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22266--109/109_693915.txt#0_1",
      "dataset_type": "short",
      "response": "carry on cleo",
      "accuracy": 1.0,
      "g_nll": 0.18674963249941356,
      "average_nll": 0.03734992649988271,
      "perplexity": 1.0380562006643084,
      "avg_token_probability": 0.9647279928506183,
      "sequence_length": 5
    },
    {
      "example_id": "qb_1502--134/134_2618044.txt#0_0",
      "dataset_type": "short",
      "response": "bats",
      "accuracy": 1.0,
      "g_nll": 0.03439327236264944,
      "average_nll": 0.01719663618132472,
      "perplexity": 1.0173453495629265,
      "avg_token_probability": 0.9829657771353983,
      "sequence_length": 2
    },
    {
      "example_id": "qw_4625--182/182_2700800.txt#0_0",
      "dataset_type": "short",
      "response": "aqueous humor",
      "accuracy": 1.0,
      "g_nll": 0.9049828490217351,
      "average_nll": 0.22624571225543377,
      "perplexity": 1.2538837220273689,
      "avg_token_probability": 0.8491584440335032,
      "sequence_length": 4
    },
    {
      "example_id": "bb_476--74/74_94569.txt#0_0",
      "dataset_type": "short",
      "response": "1969",
      "accuracy": 0.0,
      "g_nll": 0.9249838120194909,
      "average_nll": 0.23124595300487272,
      "perplexity": 1.2601691437242721,
      "avg_token_probability": 0.8125667700564421,
      "sequence_length": 4
    },
    {
      "example_id": "qz_2001--6/6_147605.txt#0_1",
      "dataset_type": "short",
      "response": "pablo neruda",
      "accuracy": 1.0,
      "g_nll": 0.039941374023328535,
      "average_nll": 0.007988274804665707,
      "perplexity": 1.0080202662004596,
      "avg_token_probability": 0.9920666637904809,
      "sequence_length": 5
    },
    {
      "example_id": "qb_5694--135/135_424629.txt#0_0",
      "dataset_type": "short",
      "response": "queen",
      "accuracy": 1.0,
      "g_nll": 0.2985554854385555,
      "average_nll": 0.14927774271927774,
      "perplexity": 1.1609954024531937,
      "avg_token_probability": 0.8700241400882189,
      "sequence_length": 2
    },
    {
      "example_id": "odql_206--73/73_813141.txt#0_1",
      "dataset_type": "short",
      "response": "uranus",
      "accuracy": 0.0,
      "g_nll": 1.4464832060039043,
      "average_nll": 0.7232416030019522,
      "perplexity": 2.0611036731613868,
      "avg_token_probability": 0.6049611027678043,
      "sequence_length": 2
    },
    {
      "example_id": "bt_3256--60/60_2424928.txt#0_1",
      "dataset_type": "short",
      "response": "patent flour",
      "accuracy": 1.0,
      "g_nll": 0.41173962340690196,
      "average_nll": 0.13724654113563398,
      "perplexity": 1.147110923690935,
      "avg_token_probability": 0.884346456238355,
      "sequence_length": 3
    },
    {
      "example_id": "qw_10365--192/192_1229944.txt#0_0",
      "dataset_type": "short",
      "response": "old trafford",
      "accuracy": 1.0,
      "g_nll": 0.9112657726946054,
      "average_nll": 0.22781644317365135,
      "perplexity": 1.2558547835557996,
      "avg_token_probability": 0.8430560283213179,
      "sequence_length": 4
    },
    {
      "example_id": "bb_455--108/108_838985.txt#0_2",
      "dataset_type": "short",
      "response": "amazon",
      "accuracy": 1.0,
      "g_nll": 0.3672560453414917,
      "average_nll": 0.18362802267074585,
      "perplexity": 1.2015687836101365,
      "avg_token_probability": 0.8330826040241847,
      "sequence_length": 2
    },
    {
      "example_id": "odql_14628--194/194_851470.txt#0_0",
      "dataset_type": "short",
      "response": "amelia",
      "accuracy": 0.0,
      "g_nll": 0.48540421582583804,
      "average_nll": 0.16180140527527934,
      "perplexity": 1.175626744866414,
      "avg_token_probability": 0.8710683298746954,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23316--131/131_1618418.txt#0_0",
      "dataset_type": "short",
      "response": "vienna",
      "accuracy": 1.0,
      "g_nll": 0.034478958463296294,
      "average_nll": 0.011492986154432097,
      "perplexity": 1.0115592842641146,
      "avg_token_probability": 0.9886252827970661,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5580--182/182_233604.txt#0_1",
      "dataset_type": "short",
      "response": "nero",
      "accuracy": 0.0,
      "g_nll": 2.060219958422522,
      "average_nll": 0.6867399861408406,
      "perplexity": 1.9872265757660268,
      "avg_token_probability": 0.6995801620913484,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_24100--95/95_438302.txt#0_0",
      "dataset_type": "short",
      "response": "friends",
      "accuracy": 0.0,
      "g_nll": 0.09818368963897228,
      "average_nll": 0.04909184481948614,
      "perplexity": 1.0503168124682498,
      "avg_token_probability": 0.9524071087399797,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7625--62/62_2634404.txt#0_1",
      "dataset_type": "short",
      "response": "vietnam",
      "accuracy": 1.0,
      "g_nll": 0.070294412507792,
      "average_nll": 0.023431470835930668,
      "perplexity": 1.0237081444793894,
      "avg_token_probability": 0.9773197728909714,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12623--90/90_717718.txt#0_2",
      "dataset_type": "short",
      "response": "utah",
      "accuracy": 1.0,
      "g_nll": 0.024833287294313777,
      "average_nll": 0.008277762431437926,
      "perplexity": 1.008312117836741,
      "avg_token_probability": 0.9917881301207528,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_12132--72/72_1740634.txt#0_1",
      "dataset_type": "short",
      "response": "st helens",
      "accuracy": 1.0,
      "g_nll": 1.0077722701239509,
      "average_nll": 0.2519430675309877,
      "perplexity": 1.2865227902809027,
      "avg_token_probability": 0.8288371583287422,
      "sequence_length": 4
    },
    {
      "example_id": "qb_8699--41/41_310048.txt#0_1",
      "dataset_type": "short",
      "response": "red",
      "accuracy": 1.0,
      "g_nll": 0.0024293928872793913,
      "average_nll": 0.0024293928872793913,
      "perplexity": 1.0024323462533242,
      "avg_token_probability": 0.9975735556993792,
      "sequence_length": 1
    },
    {
      "example_id": "odql_2538--189/189_425837.txt#0_1",
      "dataset_type": "short",
      "response": "volleyball",
      "accuracy": 1.0,
      "g_nll": 0.14079066924750805,
      "average_nll": 0.07039533462375402,
      "perplexity": 1.0729322646944361,
      "avg_token_probability": 0.9339191605622574,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3059--26/26_1399947.txt#0_1",
      "dataset_type": "short",
      "response": "judas",
      "accuracy": 1.0,
      "g_nll": 0.6935829817521153,
      "average_nll": 0.2311943272507051,
      "perplexity": 1.2601040882211327,
      "avg_token_probability": 0.8046539808190644,
      "sequence_length": 3
    },
    {
      "example_id": "jp_4006--145/145_1449904.txt#0_2",
      "dataset_type": "short",
      "response": "orson welles",
      "accuracy": 1.0,
      "g_nll": 0.025812527525886253,
      "average_nll": 0.00516250550517725,
      "perplexity": 1.0051758541977367,
      "avg_token_probability": 0.9948597249064625,
      "sequence_length": 5
    },
    {
      "example_id": "odql_2197--13/13_665800.txt#0_2",
      "dataset_type": "short",
      "response": "chile",
      "accuracy": 1.0,
      "g_nll": 0.028069747611539242,
      "average_nll": 0.009356582537179747,
      "perplexity": 1.0094004921968385,
      "avg_token_probability": 0.9907137452793431,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3075--57/57_635879.txt#0_2",
      "dataset_type": "short",
      "response": "michaela tabb",
      "accuracy": 1.0,
      "g_nll": 0.1837590697318774,
      "average_nll": 0.03675181394637548,
      "perplexity": 1.037435511858832,
      "avg_token_probability": 0.9647688088655781,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_5662--54/54_415167.txt#0_2",
      "dataset_type": "short",
      "response": "puck",
      "accuracy": 1.0,
      "g_nll": 0.21814687922596931,
      "average_nll": 0.10907343961298466,
      "perplexity": 1.115244250440191,
      "avg_token_probability": 0.8997742741855779,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_1971--27/27_3001523.txt#0_2",
      "dataset_type": "short",
      "response": "sir cloudesley shovell",
      "accuracy": 1.0,
      "g_nll": 0.8159395196385049,
      "average_nll": 0.10199243995481311,
      "perplexity": 1.1073750998904937,
      "avg_token_probability": 0.9269086472685742,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_3203--Hell's_Kitchen_(UK_TV_series).txt#0_0",
      "dataset_type": "short",
      "response": "linda evans",
      "accuracy": 1.0,
      "g_nll": 0.7132664403034141,
      "average_nll": 0.14265328806068284,
      "perplexity": 1.1533298591054635,
      "avg_token_probability": 0.8959400521124493,
      "sequence_length": 5
    },
    {
      "example_id": "bt_264--110/110_1711.txt#0_0",
      "dataset_type": "short",
      "response": "amelia earhart",
      "accuracy": 1.0,
      "g_nll": 0.08939559628925053,
      "average_nll": 0.017879119257850108,
      "perplexity": 1.0180399075320554,
      "avg_token_probability": 0.9825046527737523,
      "sequence_length": 5
    },
    {
      "example_id": "jp_4375--83/83_2736108.txt#0_0",
      "dataset_type": "short",
      "response": "yosemite",
      "accuracy": 1.0,
      "g_nll": 0.20894938390119933,
      "average_nll": 0.06964979463373311,
      "perplexity": 1.072132648894338,
      "avg_token_probability": 0.9360625521681275,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1864--103/103_769441.txt#0_0",
      "dataset_type": "short",
      "response": "ringway",
      "accuracy": 1.0,
      "g_nll": 0.11256208069971763,
      "average_nll": 0.03752069356657254,
      "perplexity": 1.0382334816131982,
      "avg_token_probability": 0.9642021558153604,
      "sequence_length": 3
    },
    {
      "example_id": "qw_3063--131/131_82670.txt#0_0",
      "dataset_type": "short",
      "response": "billy the kid",
      "accuracy": 0.0,
      "g_nll": 2.063687462359667,
      "average_nll": 0.41273749247193336,
      "perplexity": 1.5109483385561695,
      "avg_token_probability": 0.782824482356349,
      "sequence_length": 5
    },
    {
      "example_id": "odql_2542--11/11_2808626.txt#0_1",
      "dataset_type": "short",
      "response": "thailand",
      "accuracy": 1.0,
      "g_nll": 0.05463735345983878,
      "average_nll": 0.01821245115327959,
      "perplexity": 1.0183793092676254,
      "avg_token_probability": 0.982040778180158,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_4014--107/107_660870.txt#0_2",
      "dataset_type": "short",
      "response": "portia de rossi",
      "accuracy": 0.0,
      "g_nll": 0.7822445053602678,
      "average_nll": 0.1303740842267113,
      "perplexity": 1.1392544807529492,
      "avg_token_probability": 0.9020929766989721,
      "sequence_length": 6
    },
    {
      "example_id": "qz_3311--68/68_97141.txt#0_1",
      "dataset_type": "short",
      "response": "one",
      "accuracy": 0.0,
      "g_nll": 0.7281207013875246,
      "average_nll": 0.3640603506937623,
      "perplexity": 1.4391610659060015,
      "avg_token_probability": 0.7338751252454961,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_21423--144/144_2788272.txt#0_0",
      "dataset_type": "short",
      "response": "byward tower",
      "accuracy": 0.0,
      "g_nll": 0.16276849526911974,
      "average_nll": 0.040692123817279935,
      "perplexity": 1.0415313934686037,
      "avg_token_probability": 0.9612735111333287,
      "sequence_length": 4
    },
    {
      "example_id": "wh_1190--127/127_411355.txt#0_0",
      "dataset_type": "short",
      "response": "peter stuyvesant",
      "accuracy": 1.0,
      "g_nll": 0.10111990191762033,
      "average_nll": 0.016853316986270055,
      "perplexity": 1.0169961353258277,
      "avg_token_probability": 0.9835172513058802,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_15186--22/22_2773455.txt#0_0",
      "dataset_type": "short",
      "response": "the spider",
      "accuracy": 0.0,
      "g_nll": 1.0932412091642618,
      "average_nll": 0.3644137363880873,
      "perplexity": 1.439669734711368,
      "avg_token_probability": 0.718415199114888,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8229--63/63_493737.txt#0_1",
      "dataset_type": "short",
      "response": "the rescuers",
      "accuracy": 1.0,
      "g_nll": 0.3716209791600704,
      "average_nll": 0.0929052447900176,
      "perplexity": 1.0973577499775609,
      "avg_token_probability": 0.9152127654272684,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_8614--100/100_1662867.txt#0_5",
      "dataset_type": "short",
      "response": "moctezuma",
      "accuracy": 0.0,
      "g_nll": 1.3462389631386031,
      "average_nll": 0.26924779262772064,
      "perplexity": 1.3089794563224544,
      "avg_token_probability": 0.8452077591595897,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_22477--53/53_318005.txt#0_2",
      "dataset_type": "short",
      "response": "27",
      "accuracy": 1.0,
      "g_nll": 0.553118773503229,
      "average_nll": 0.18437292450107634,
      "perplexity": 1.2024641678415897,
      "avg_token_probability": 0.857278161438547,
      "sequence_length": 3
    },
    {
      "example_id": "bb_2276--42/42_883302.txt#0_1",
      "dataset_type": "short",
      "response": "scotland",
      "accuracy": 1.0,
      "g_nll": 0.027494598529301584,
      "average_nll": 0.009164866176433861,
      "perplexity": 1.0092069921570939,
      "avg_token_probability": 0.9909302132952336,
      "sequence_length": 3
    },
    {
      "example_id": "tc_1938--155/155_57137.txt#0_1",
      "dataset_type": "short",
      "response": "south africa",
      "accuracy": 1.0,
      "g_nll": 0.05896502663381398,
      "average_nll": 0.01965500887793799,
      "perplexity": 1.0198494403263574,
      "avg_token_probability": 0.9806238554046219,
      "sequence_length": 3
    },
    {
      "example_id": "odql_1853--0/0_2112627.txt#0_0",
      "dataset_type": "short",
      "response": "hoagy",
      "accuracy": 0.0,
      "g_nll": 0.3528448101133108,
      "average_nll": 0.11761493670443694,
      "perplexity": 1.124810904572877,
      "avg_token_probability": 0.894741656098229,
      "sequence_length": 3
    },
    {
      "example_id": "qg_3271--66/66_2533261.txt#0_2",
      "dataset_type": "short",
      "response": "horse race",
      "accuracy": 0.0,
      "g_nll": 7.728056749328971,
      "average_nll": 2.5760189164429903,
      "perplexity": 13.14470369218242,
      "avg_token_probability": 0.5024709540852161,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5877--148/148_240409.txt#0_0",
      "dataset_type": "short",
      "response": "plaque",
      "accuracy": 0.0,
      "g_nll": 0.7101676184684038,
      "average_nll": 0.3550838092342019,
      "perplexity": 1.4263001863988267,
      "avg_token_probability": 0.7429657393843594,
      "sequence_length": 2
    },
    {
      "example_id": "odql_14853--114/114_1247260.txt#0_1",
      "dataset_type": "short",
      "response": "1982",
      "accuracy": 1.0,
      "g_nll": 2.567542631732067,
      "average_nll": 0.6418856579330168,
      "perplexity": 1.9000603673041645,
      "avg_token_probability": 0.7038327612536396,
      "sequence_length": 4
    },
    {
      "example_id": "wh_156--I'm_a_Celebrity...Get_Me_Out_of_Here!_(UK_series_5).txt#0_0",
      "dataset_type": "short",
      "response": "carol thatcher",
      "accuracy": 1.0,
      "g_nll": 6.949240490561351,
      "average_nll": 1.3898480981122703,
      "perplexity": 4.014240236009549,
      "avg_token_probability": 0.6092317044424275,
      "sequence_length": 5
    },
    {
      "example_id": "odql_4802--148/148_1652317.txt#0_1",
      "dataset_type": "short",
      "response": "acne",
      "accuracy": 0.0,
      "g_nll": 0.2817363440990448,
      "average_nll": 0.2817363440990448,
      "perplexity": 1.325429216680236,
      "avg_token_probability": 0.7544725794597097,
      "sequence_length": 1
    },
    {
      "example_id": "qf_364--185/185_2457360.txt#0_0",
      "dataset_type": "short",
      "response": "lager",
      "accuracy": 0.0,
      "g_nll": 0.1318615429336205,
      "average_nll": 0.04395384764454017,
      "perplexity": 1.0449341276042616,
      "avg_token_probability": 0.9576766576805156,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8247--176/176_1183968.txt#0_0",
      "dataset_type": "short",
      "response": "baseball",
      "accuracy": 1.0,
      "g_nll": 0.03947581723332405,
      "average_nll": 0.019737908616662025,
      "perplexity": 1.0199339890829846,
      "avg_token_probability": 0.9804910278708832,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_25528--123/123_2033010.txt#0_0",
      "dataset_type": "short",
      "response": "susie dent",
      "accuracy": 1.0,
      "g_nll": 0.01875445768200734,
      "average_nll": 0.00937722884100367,
      "perplexity": 1.0094213328012203,
      "avg_token_probability": 0.9907099032090221,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2553--36/36_81264.txt#0_1",
      "dataset_type": "short",
      "response": "e pluribus unum",
      "accuracy": 1.0,
      "g_nll": 0.019522778004102292,
      "average_nll": 0.0032537963340170486,
      "perplexity": 1.003259095675409,
      "avg_token_probability": 0.9967632008761994,
      "sequence_length": 6
    },
    {
      "example_id": "bt_2852--75/75_2417406.txt#0_1",
      "dataset_type": "short",
      "response": "Claire Goose",
      "accuracy": 1.0,
      "g_nll": 1.4968036813661456,
      "average_nll": 0.49893456045538187,
      "perplexity": 1.646965593310412,
      "avg_token_probability": 0.7322228657158489,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_13456--144/144_1578174.txt#0_1",
      "dataset_type": "short",
      "response": "doncaster rovers",
      "accuracy": 1.0,
      "g_nll": 0.47622453941585263,
      "average_nll": 0.09524490788317053,
      "perplexity": 1.0999282032299122,
      "avg_token_probability": 0.9205920418905873,
      "sequence_length": 5
    },
    {
      "example_id": "qw_14570--139/139_1315393.txt#0_0",
      "dataset_type": "short",
      "response": "tarantula",
      "accuracy": 1.0,
      "g_nll": 0.03136681855221468,
      "average_nll": 0.00784170463805367,
      "perplexity": 1.0078725313291212,
      "avg_token_probability": 0.9922208787465056,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_10437--135/135_1466322.txt#0_0",
      "dataset_type": "short",
      "response": "Adam Smith",
      "accuracy": 1.0,
      "g_nll": 5.791467780247331,
      "average_nll": 1.9304892600824435,
      "perplexity": 6.892881828654312,
      "avg_token_probability": 0.46337730060236376,
      "sequence_length": 3
    },
    {
      "example_id": "qz_2980--37/37_55261.txt#0_2",
      "dataset_type": "short",
      "response": "persistence of memory",
      "accuracy": 1.0,
      "g_nll": 0.04673296331020538,
      "average_nll": 0.011683240827551344,
      "perplexity": 1.0117517564538692,
      "avg_token_probability": 0.9884790431911242,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1688--101/101_312407.txt#0_0",
      "dataset_type": "short",
      "response": "valentino",
      "accuracy": 1.0,
      "g_nll": 2.0245072393799433,
      "average_nll": 0.5061268098449858,
      "perplexity": 1.6588536803911387,
      "avg_token_probability": 0.7746132736289685,
      "sequence_length": 4
    },
    {
      "example_id": "qw_9748--135/135_39202.txt#0_2",
      "dataset_type": "short",
      "response": "taekwondo",
      "accuracy": 1.0,
      "g_nll": 0.02047145852472454,
      "average_nll": 0.004094291704944908,
      "perplexity": 1.0041026847678676,
      "avg_token_probability": 0.9959267321509389,
      "sequence_length": 5
    },
    {
      "example_id": "jp_2683--186/186_1418025.txt#0_2",
      "dataset_type": "short",
      "response": "kix",
      "accuracy": 1.0,
      "g_nll": 0.061845028722927964,
      "average_nll": 0.020615009574309322,
      "perplexity": 1.0208289665969876,
      "avg_token_probability": 0.9797311199694886,
      "sequence_length": 3
    },
    {
      "example_id": "odql_4564--45/45_1774711.txt#0_0",
      "dataset_type": "short",
      "response": "dogs",
      "accuracy": 1.0,
      "g_nll": 0.02909189648926258,
      "average_nll": 0.02909189648926258,
      "perplexity": 1.029519199328184,
      "avg_token_probability": 0.9713271988055717,
      "sequence_length": 1
    },
    {
      "example_id": "odql_8365--188/188_262755.txt#0_1",
      "dataset_type": "short",
      "response": "harvard",
      "accuracy": 1.0,
      "g_nll": 0.02032984868856147,
      "average_nll": 0.0067766162295204895,
      "perplexity": 1.0067996294478283,
      "avg_token_probability": 0.9932597398852104,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1878--25/25_1397691.txt#0_0",
      "dataset_type": "short",
      "response": "shaft",
      "accuracy": 1.0,
      "g_nll": 1.0192147828638554,
      "average_nll": 0.5096073914319277,
      "perplexity": 1.6646375156828515,
      "avg_token_probability": 0.6796975116157428,
      "sequence_length": 2
    },
    {
      "example_id": "qz_3023--106/106_173524.txt#0_0",
      "dataset_type": "short",
      "response": "prince edward and princess wales",
      "accuracy": 0.0,
      "g_nll": 2.6257587992804474,
      "average_nll": 0.3282198499100559,
      "perplexity": 1.3884941990609205,
      "avg_token_probability": 0.7963271582720085,
      "sequence_length": 8
    },
    {
      "example_id": "tc_783--72/72_2593823.txt#0_1",
      "dataset_type": "short",
      "response": "cartoons",
      "accuracy": 1.0,
      "g_nll": 0.11457449942827225,
      "average_nll": 0.11457449942827225,
      "perplexity": 1.1213961813264486,
      "avg_token_probability": 0.8917455014133768,
      "sequence_length": 1
    },
    {
      "example_id": "tc_2359--127/127_71063.txt#0_0",
      "dataset_type": "short",
      "response": "john buchan",
      "accuracy": 1.0,
      "g_nll": 0.06576823026989587,
      "average_nll": 0.016442057567473967,
      "perplexity": 1.0165779720789763,
      "avg_token_probability": 0.9837498937282418,
      "sequence_length": 4
    },
    {
      "example_id": "qb_574--35/35_280986.txt#0_1",
      "dataset_type": "short",
      "response": "tokyo",
      "accuracy": 1.0,
      "g_nll": 0.9453229621331047,
      "average_nll": 0.23633074053327618,
      "perplexity": 1.2665931545940416,
      "avg_token_probability": 0.8416135088020271,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1631--122/122_596455.txt#0_0",
      "dataset_type": "short",
      "response": "lost in translation",
      "accuracy": 1.0,
      "g_nll": 0.054760593819082715,
      "average_nll": 0.013690148454770679,
      "perplexity": 1.0137842876397203,
      "avg_token_probability": 0.9866141253755164,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_634--98/98_901854.txt#0_0",
      "dataset_type": "short",
      "response": "bitter almond",
      "accuracy": 1.0,
      "g_nll": 0.08123009651899338,
      "average_nll": 0.02707669883966446,
      "perplexity": 1.0274466037038017,
      "avg_token_probability": 0.9733768612612949,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_25402--57/57_2030191.txt#0_2",
      "dataset_type": "short",
      "response": "patrick armstrong",
      "accuracy": 0.0,
      "g_nll": 0.12833149370271713,
      "average_nll": 0.025666298740543426,
      "perplexity": 1.0259985143448325,
      "avg_token_probability": 0.9755207629082981,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_668--20/20_1363097.txt#0_1",
      "dataset_type": "short",
      "response": "demi moore",
      "accuracy": 1.0,
      "g_nll": 1.5924649711231496,
      "average_nll": 0.5308216570410499,
      "perplexity": 1.7003288219611368,
      "avg_token_probability": 0.7335550577529549,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1572--159/159_685600.txt#0_0",
      "dataset_type": "short",
      "response": "cheers",
      "accuracy": 1.0,
      "g_nll": 0.026793234050273895,
      "average_nll": 0.026793234050273895,
      "perplexity": 1.0271554000436147,
      "avg_token_probability": 0.9735625202939481,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_25914--50_(number).txt#0_0",
      "dataset_type": "short",
      "response": "neon",
      "accuracy": 0.0,
      "g_nll": 2.7517814449965954,
      "average_nll": 1.3758907224982977,
      "perplexity": 3.958601168158489,
      "avg_token_probability": 0.5236700420475098,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_9379--37/37_1679055.txt#0_1",
      "dataset_type": "short",
      "response": "honda",
      "accuracy": 1.0,
      "g_nll": 0.049562147818505764,
      "average_nll": 0.024781073909252882,
      "perplexity": 1.0250906768624917,
      "avg_token_probability": 0.9756067418323695,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7372--177/177_98429.txt#0_2",
      "dataset_type": "short",
      "response": "3",
      "accuracy": 0.0,
      "g_nll": 0.21477329172194004,
      "average_nll": 0.07159109724064668,
      "perplexity": 1.0742160043580027,
      "avg_token_probability": 0.9335856513520001,
      "sequence_length": 3
    },
    {
      "example_id": "bt_824--124/124_995752.txt#0_1",
      "dataset_type": "short",
      "response": "the 2I's",
      "accuracy": 0.0,
      "g_nll": 1.8142131355125457,
      "average_nll": 0.3023688559187576,
      "perplexity": 1.3530602189458985,
      "avg_token_probability": 0.8005166723362483,
      "sequence_length": 6
    },
    {
      "example_id": "wh_495--37/37_2926588.txt#0_1",
      "dataset_type": "short",
      "response": "tommy roe",
      "accuracy": 1.0,
      "g_nll": 0.16687569104209388,
      "average_nll": 0.033375138208418774,
      "perplexity": 1.0339383362731007,
      "avg_token_probability": 0.9680643780802111,
      "sequence_length": 5
    },
    {
      "example_id": "tb_1170--172/172_2063410.txt#0_0",
      "dataset_type": "short",
      "response": "crete",
      "accuracy": 1.0,
      "g_nll": 0.05030600231111748,
      "average_nll": 0.01676866743703916,
      "perplexity": 1.0169100507049662,
      "avg_token_probability": 0.9834686716504762,
      "sequence_length": 3
    },
    {
      "example_id": "qw_1393--125/125_2694259.txt#0_0",
      "dataset_type": "short",
      "response": "eddie fisher",
      "accuracy": 0.0,
      "g_nll": 1.4602607613851362,
      "average_nll": 0.29205215227702724,
      "perplexity": 1.3391728567319556,
      "avg_token_probability": 0.8330808574810673,
      "sequence_length": 5
    },
    {
      "example_id": "qb_8996--151/151_2637831.txt#0_0",
      "dataset_type": "short",
      "response": "chris moyles",
      "accuracy": 0.0,
      "g_nll": 2.2641420541049158,
      "average_nll": 0.45282841082098313,
      "perplexity": 1.5727542957412677,
      "avg_token_probability": 0.8156246044806654,
      "sequence_length": 5
    },
    {
      "example_id": "qw_2309--75/75_263961.txt#0_1",
      "dataset_type": "short",
      "response": "unesco",
      "accuracy": 1.0,
      "g_nll": 0.9073698974680156,
      "average_nll": 0.3024566324893385,
      "perplexity": 1.3531789911443388,
      "avg_token_probability": 0.7984596136088548,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_6084--154/154_2657783.txt#0_0",
      "dataset_type": "short",
      "response": "stanislas wawrinka",
      "accuracy": 1.0,
      "g_nll": 0.03135182707103468,
      "average_nll": 0.003918978383879335,
      "perplexity": 1.003926667621036,
      "avg_token_probability": 0.9961021384047628,
      "sequence_length": 8
    },
    {
      "example_id": "qg_3077--116/116_1355464.txt#0_0",
      "dataset_type": "short",
      "response": "chile",
      "accuracy": 1.0,
      "g_nll": 0.041881998999087955,
      "average_nll": 0.013960666333029318,
      "perplexity": 1.0140585715118975,
      "avg_token_probability": 0.986186089744761,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23639--86/86_1607077.txt#0_0",
      "dataset_type": "short",
      "response": "bear grylls",
      "accuracy": 1.0,
      "g_nll": 0.3296742883940169,
      "average_nll": 0.05494571473233615,
      "perplexity": 1.0564832616421553,
      "avg_token_probability": 0.9510943777140818,
      "sequence_length": 6
    },
    {
      "example_id": "jp_1655--92/92_1391976.txt#0_0",
      "dataset_type": "short",
      "response": "space jam",
      "accuracy": 1.0,
      "g_nll": 0.07145856178249232,
      "average_nll": 0.023819520594164107,
      "perplexity": 1.0241054712636517,
      "avg_token_probability": 0.97694231094419,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1024--151/151_108800.txt#0_0",
      "dataset_type": "short",
      "response": "swindon",
      "accuracy": 1.0,
      "g_nll": 0.8919322043466309,
      "average_nll": 0.22298305108665772,
      "perplexity": 1.2497993908149005,
      "avg_token_probability": 0.8333550923675149,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2250--43/43_67037.txt#0_0",
      "dataset_type": "short",
      "response": "earthquake",
      "accuracy": 1.0,
      "g_nll": 0.12936263624578714,
      "average_nll": 0.06468131812289357,
      "perplexity": 1.066818994326717,
      "avg_token_probability": 0.9387064024462001,
      "sequence_length": 2
    },
    {
      "example_id": "qw_3191--24/24_1085500.txt#0_2",
      "dataset_type": "short",
      "response": "jack brabham",
      "accuracy": 1.0,
      "g_nll": 0.09087792047631638,
      "average_nll": 0.018175584095263275,
      "perplexity": 1.01834176531062,
      "avg_token_probability": 0.9823356167536254,
      "sequence_length": 5
    },
    {
      "example_id": "jp_2475--173/173_133780.txt#0_2",
      "dataset_type": "short",
      "response": "california",
      "accuracy": 0.0,
      "g_nll": 2.758145470172167,
      "average_nll": 1.3790727350860834,
      "perplexity": 3.971217549001636,
      "avg_token_probability": 0.5154015618484691,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12438--52/52_1779413.txt#0_0",
      "dataset_type": "short",
      "response": "Puckett and The Union Gap",
      "accuracy": 0.0,
      "g_nll": 3.9285727399783354,
      "average_nll": 0.5612246771397622,
      "perplexity": 1.752817822281685,
      "avg_token_probability": 0.675683144732998,
      "sequence_length": 7
    },
    {
      "example_id": "qf_530--164/164_2459775.txt#0_0",
      "dataset_type": "short",
      "response": "tetanus",
      "accuracy": 1.0,
      "g_nll": 0.026209468345768983,
      "average_nll": 0.00873648944858966,
      "perplexity": 1.0087747639529376,
      "avg_token_probability": 0.9913537212583012,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1386--197/197_2415925.txt#0_2",
      "dataset_type": "short",
      "response": "jupiter",
      "accuracy": 1.0,
      "g_nll": 0.061273343626453425,
      "average_nll": 0.020424447875484475,
      "perplexity": 1.0206344542287957,
      "avg_token_probability": 0.9799824690540747,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_16856--9/9_1846523.txt#0_0",
      "dataset_type": "short",
      "response": "bangladesh",
      "accuracy": 1.0,
      "g_nll": 0.025686464876343962,
      "average_nll": 0.00856215495878132,
      "perplexity": 1.0085989150478423,
      "avg_token_probability": 0.9914935078636228,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_10025--172/172_3020164.txt#0_1",
      "dataset_type": "short",
      "response": "nutbush",
      "accuracy": 1.0,
      "g_nll": 0.032925659092114756,
      "average_nll": 0.008231414773028689,
      "perplexity": 1.008265386014097,
      "avg_token_probability": 0.9918904844709414,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_21320--53/53_2087897.txt#0_0",
      "dataset_type": "short",
      "response": "gorky",
      "accuracy": 1.0,
      "g_nll": 0.4543873406344119,
      "average_nll": 0.11359683515860297,
      "perplexity": 1.1203003681038233,
      "avg_token_probability": 0.9068494109149474,
      "sequence_length": 4
    },
    {
      "example_id": "odql_7980--187/187_83028.txt#0_0",
      "dataset_type": "short",
      "response": "magma",
      "accuracy": 1.0,
      "g_nll": 0.02193855494260788,
      "average_nll": 0.01096927747130394,
      "perplexity": 1.0110296605798128,
      "avg_token_probability": 0.9891042518634711,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_16964--98/98_1848963.txt#0_0",
      "dataset_type": "short",
      "response": "helen gurley brown",
      "accuracy": 1.0,
      "g_nll": 0.2435711387368542,
      "average_nll": 0.03479587696240775,
      "perplexity": 1.0354083365325812,
      "avg_token_probability": 0.9678005224037624,
      "sequence_length": 7
    },
    {
      "example_id": "odql_6538--139/139_826850.txt#0_0",
      "dataset_type": "short",
      "response": "william iv",
      "accuracy": 1.0,
      "g_nll": 1.0680289165666181,
      "average_nll": 0.26700722914165453,
      "perplexity": 1.3060498879161746,
      "avg_token_probability": 0.8288004477525546,
      "sequence_length": 4
    },
    {
      "example_id": "odql_5186--146/146_2175002.txt#0_1",
      "dataset_type": "short",
      "response": "m11",
      "accuracy": 1.0,
      "g_nll": 0.5386154421285028,
      "average_nll": 0.17953848070950093,
      "perplexity": 1.1966649516978785,
      "avg_token_probability": 0.8560328664800848,
      "sequence_length": 3
    },
    {
      "example_id": "bb_2691--72/72_892989.txt#0_2",
      "dataset_type": "short",
      "response": "obi",
      "accuracy": 1.0,
      "g_nll": 0.013053371568275907,
      "average_nll": 0.004351123856091969,
      "perplexity": 1.0043606037398936,
      "avg_token_probability": 0.9956719189575022,
      "sequence_length": 3
    },
    {
      "example_id": "qg_1199--189/189_3112721.txt#0_0",
      "dataset_type": "short",
      "response": "arizona diamondbacks",
      "accuracy": 1.0,
      "g_nll": 0.26256972995906835,
      "average_nll": 0.05251394599181367,
      "perplexity": 1.0539172598975282,
      "avg_token_probability": 0.9521568607297788,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_24050--117/117_2000989.txt#0_0",
      "dataset_type": "short",
      "response": "leonard nimoy",
      "accuracy": 0.0,
      "g_nll": 0.2670746117910312,
      "average_nll": 0.0445124352985052,
      "perplexity": 1.0455179779577666,
      "avg_token_probability": 0.9600408978893994,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_20082--77/77_957747.txt#0_1",
      "dataset_type": "short",
      "response": "vw",
      "accuracy": 1.0,
      "g_nll": 1.081997325643897,
      "average_nll": 0.5409986628219485,
      "perplexity": 1.717721430435616,
      "avg_token_probability": 0.6601574685058853,
      "sequence_length": 2
    },
    {
      "example_id": "odql_6144--10/10_2193236.txt#0_1",
      "dataset_type": "short",
      "response": "richard seddon",
      "accuracy": 1.0,
      "g_nll": 0.07483420165272037,
      "average_nll": 0.014966840330544073,
      "perplexity": 1.0150794043601206,
      "avg_token_probability": 0.985412352373011,
      "sequence_length": 5
    },
    {
      "example_id": "qz_2845--76/76_169229.txt#0_0",
      "dataset_type": "short",
      "response": "brazil",
      "accuracy": 0.0,
      "g_nll": 0.971667468547821,
      "average_nll": 0.971667468547821,
      "perplexity": 2.642346818148392,
      "avg_token_probability": 0.3784514557785203,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_18636--83/83_1734.txt#0_0",
      "dataset_type": "short",
      "response": "1919",
      "accuracy": 1.0,
      "g_nll": 0.02291136488202028,
      "average_nll": 0.00572784122050507,
      "perplexity": 1.0057442766679225,
      "avg_token_probability": 0.9943299313145312,
      "sequence_length": 4
    },
    {
      "example_id": "qw_10365--192/192_1229944.txt#0_2",
      "dataset_type": "short",
      "response": "old trafford",
      "accuracy": 1.0,
      "g_nll": 0.41521088770241477,
      "average_nll": 0.10380272192560369,
      "perplexity": 1.1093815766651374,
      "avg_token_probability": 0.911428568925111,
      "sequence_length": 4
    },
    {
      "example_id": "qb_2774--81/81_303935.txt#0_0",
      "dataset_type": "short",
      "response": "funny folks",
      "accuracy": 0.0,
      "g_nll": 1.205711789545603,
      "average_nll": 0.40190392984853435,
      "perplexity": 1.4946677328228402,
      "avg_token_probability": 0.7033577053661816,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_2761--66/66_24883.txt#0_1",
      "dataset_type": "short",
      "response": "cabaret",
      "accuracy": 1.0,
      "g_nll": 0.0014663482143078,
      "average_nll": 0.0007331741071539,
      "perplexity": 1.0007334429449872,
      "avg_token_probability": 0.999267265382077,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_9852--85/85_243465.txt#0_0",
      "dataset_type": "short",
      "response": "leicestershire",
      "accuracy": 1.0,
      "g_nll": 0.3085459137487305,
      "average_nll": 0.06170918274974611,
      "perplexity": 1.0636529711123388,
      "avg_token_probability": 0.9434337295535687,
      "sequence_length": 5
    },
    {
      "example_id": "qb_3427--81/81_362513.txt#0_3",
      "dataset_type": "short",
      "response": "wolfgang amadeus mozart",
      "accuracy": 1.0,
      "g_nll": 1.1223173098678672,
      "average_nll": 0.1402896637334834,
      "perplexity": 1.1506070397218988,
      "avg_token_probability": 0.914106825200111,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_1674--197/197_3000842.txt#0_2",
      "dataset_type": "short",
      "response": "fiji",
      "accuracy": 1.0,
      "g_nll": 0.8736512376926839,
      "average_nll": 0.2912170792308946,
      "perplexity": 1.3380550163793208,
      "avg_token_probability": 0.8012817777475298,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_4253--27/27_289038.txt#0_1",
      "dataset_type": "short",
      "response": "the Calcutta cup",
      "accuracy": 1.0,
      "g_nll": 9.824000565329754,
      "average_nll": 1.6373334275549591,
      "perplexity": 5.141441191179887,
      "avg_token_probability": 0.6088686412239676,
      "sequence_length": 6
    },
    {
      "example_id": "odql_6028--County_Durham.txt#0_0",
      "dataset_type": "short",
      "response": "county durham",
      "accuracy": 0.0,
      "g_nll": 1.538628428024822,
      "average_nll": 0.3846571070062055,
      "perplexity": 1.4691104873722647,
      "avg_token_probability": 0.7791710088535119,
      "sequence_length": 4
    },
    {
      "example_id": "qw_3390--174/174_1038938.txt#0_0",
      "dataset_type": "short",
      "response": "winnipeg",
      "accuracy": 1.0,
      "g_nll": 0.24517675876268186,
      "average_nll": 0.061294189690670464,
      "perplexity": 1.063211654090146,
      "avg_token_probability": 0.9448131432311417,
      "sequence_length": 4
    },
    {
      "example_id": "wh_4331--119/119_827530.txt#0_1",
      "dataset_type": "short",
      "response": "gironde",
      "accuracy": 1.0,
      "g_nll": 0.11358503847407064,
      "average_nll": 0.02839625961851766,
      "perplexity": 1.0288032768538966,
      "avg_token_probability": 0.9725249464452167,
      "sequence_length": 4
    },
    {
      "example_id": "qb_6543--195/195_826037.txt#0_0",
      "dataset_type": "short",
      "response": "john f. kennedy international airport",
      "accuracy": 0.0,
      "g_nll": 0.891779865596618,
      "average_nll": 0.11147248319957725,
      "perplexity": 1.1179229819187142,
      "avg_token_probability": 0.9088169259511473,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_24154--30/30_69897.txt#0_0",
      "dataset_type": "short",
      "response": "learned hand",
      "accuracy": 0.0,
      "g_nll": 2.3460940780933015,
      "average_nll": 0.7820313593644338,
      "perplexity": 2.185908123498466,
      "avg_token_probability": 0.6694264736607534,
      "sequence_length": 3
    },
    {
      "example_id": "qb_3569--170/170_366697.txt#0_0",
      "dataset_type": "short",
      "response": "popeye",
      "accuracy": 0.0,
      "g_nll": 0.9775244737975299,
      "average_nll": 0.32584149126584333,
      "perplexity": 1.3851957858397574,
      "avg_token_probability": 0.7631405142700509,
      "sequence_length": 3
    },
    {
      "example_id": "odql_9564--83/83_2256714.txt#0_2",
      "dataset_type": "short",
      "response": "red",
      "accuracy": 1.0,
      "g_nll": 0.08237545471638441,
      "average_nll": 0.041187727358192205,
      "perplexity": 1.0420477080482766,
      "avg_token_probability": 0.9600292776923365,
      "sequence_length": 2
    },
    {
      "example_id": "jp_2812--168/168_2992342.txt#0_0",
      "dataset_type": "short",
      "response": "a dragon",
      "accuracy": 1.0,
      "g_nll": 0.21868827722937567,
      "average_nll": 0.07289609240979189,
      "perplexity": 1.0756187661539152,
      "avg_token_probability": 0.9327513725932567,
      "sequence_length": 3
    },
    {
      "example_id": "bt_462--66/66_2369917.txt#0_0",
      "dataset_type": "short",
      "response": "bogeyman",
      "accuracy": 1.0,
      "g_nll": 0.9678326159191784,
      "average_nll": 0.2419581539797946,
      "perplexity": 1.273740890713913,
      "avg_token_probability": 0.8433270128271431,
      "sequence_length": 4
    },
    {
      "example_id": "qz_847--105/105_2601971.txt#0_0",
      "dataset_type": "short",
      "response": "happy birthday to you",
      "accuracy": 1.0,
      "g_nll": 0.06130157987354323,
      "average_nll": 0.015325394968385808,
      "perplexity": 1.0154434310461598,
      "avg_token_probability": 0.9851223364447899,
      "sequence_length": 4
    },
    {
      "example_id": "qz_2430--114/114_2605502.txt#0_1",
      "dataset_type": "short",
      "response": "castor",
      "accuracy": 1.0,
      "g_nll": 0.4251502752269971,
      "average_nll": 0.21257513761349855,
      "perplexity": 1.2368590447107823,
      "avg_token_probability": 0.8268353194164463,
      "sequence_length": 2
    },
    {
      "example_id": "tb_470--Bill_Gold.txt#0_0",
      "dataset_type": "short",
      "response": "poster design",
      "accuracy": 1.0,
      "g_nll": 1.4143965868279338,
      "average_nll": 0.4714655289426446,
      "perplexity": 1.6023407500690847,
      "avg_token_probability": 0.6824354941065923,
      "sequence_length": 3
    },
    {
      "example_id": "qf_2679--160/160_194946.txt#0_0",
      "dataset_type": "short",
      "response": "jon pertwee",
      "accuracy": 1.0,
      "g_nll": 0.533783605282224,
      "average_nll": 0.1067567210564448,
      "perplexity": 1.112663533941288,
      "avg_token_probability": 0.9063956154987002,
      "sequence_length": 5
    },
    {
      "example_id": "qf_2195--13/13_325145.txt#0_2",
      "dataset_type": "short",
      "response": "andre agassi",
      "accuracy": 1.0,
      "g_nll": 0.1647663355106488,
      "average_nll": 0.0823831677553244,
      "perplexity": 1.0858718011878532,
      "avg_token_probability": 0.9238997278015315,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7372--177/177_98429.txt#0_1",
      "dataset_type": "short",
      "response": "3",
      "accuracy": 0.0,
      "g_nll": 0.8826909027993679,
      "average_nll": 0.29423030093312263,
      "perplexity": 1.3420929533378827,
      "avg_token_probability": 0.7910978473020199,
      "sequence_length": 3
    },
    {
      "example_id": "qz_6793--102/102_2886022.txt#0_1",
      "dataset_type": "short",
      "response": "verruckt",
      "accuracy": 0.0,
      "g_nll": 1.0139894645199092,
      "average_nll": 0.2534973661299773,
      "perplexity": 1.2885239856761228,
      "avg_token_probability": 0.8404477786538425,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_17762--30/30_127115.txt#0_1",
      "dataset_type": "short",
      "response": "england",
      "accuracy": 0.0,
      "g_nll": 3.6975535131350625,
      "average_nll": 1.8487767565675313,
      "perplexity": 6.352044671376447,
      "avg_token_probability": 0.5123589709613717,
      "sequence_length": 2
    },
    {
      "example_id": "bb_9184--196/196_1039737.txt#0_0",
      "dataset_type": "short",
      "response": "scales",
      "accuracy": 0.0,
      "g_nll": 0.017770633101463318,
      "average_nll": 0.008885316550731659,
      "perplexity": 1.0089249081502565,
      "avg_token_probability": 0.9911541519987672,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1265--Alan_Lake.txt#0_0",
      "dataset_type": "short",
      "response": "diana dors",
      "accuracy": 1.0,
      "g_nll": 1.7087892830604687,
      "average_nll": 0.4271973207651172,
      "perplexity": 1.5329551157591386,
      "avg_token_probability": 0.777776614721025,
      "sequence_length": 4
    },
    {
      "example_id": "bb_1777--A_Gay_Girl_In_Damascus.txt#0_1",
      "dataset_type": "short",
      "response": "syria",
      "accuracy": 1.0,
      "g_nll": 0.024705051255295984,
      "average_nll": 0.008235017085098661,
      "perplexity": 1.008269018107209,
      "avg_token_probability": 0.9918217823729866,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_2901--81/81_239948.txt#0_4",
      "dataset_type": "short",
      "response": "the holy grail",
      "accuracy": 1.0,
      "g_nll": 0.2689082096325137,
      "average_nll": 0.05378164192650274,
      "perplexity": 1.0552541537317999,
      "avg_token_probability": 0.9505910131983896,
      "sequence_length": 5
    },
    {
      "example_id": "odql_11679--16/16_33164.txt#0_0",
      "dataset_type": "short",
      "response": "tasmania",
      "accuracy": 1.0,
      "g_nll": 0.2850946881226264,
      "average_nll": 0.09503156270754214,
      "perplexity": 1.0996935638846916,
      "avg_token_probability": 0.9157831900221218,
      "sequence_length": 3
    },
    {
      "example_id": "bb_5083--184/184_944999.txt#0_1",
      "dataset_type": "short",
      "response": "guitar",
      "accuracy": 1.0,
      "g_nll": 0.24984669499099255,
      "average_nll": 0.12492334749549627,
      "perplexity": 1.1330615977288063,
      "avg_token_probability": 0.8877965265798115,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3061--16/16_2854536.txt#0_2",
      "dataset_type": "short",
      "response": "ragdoll",
      "accuracy": 0.0,
      "g_nll": 2.700728668947704,
      "average_nll": 0.9002428896492347,
      "perplexity": 2.4602005958523616,
      "avg_token_probability": 0.6779107754910165,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9983--123/123_77798.txt#0_0",
      "dataset_type": "short",
      "response": "florida",
      "accuracy": 1.0,
      "g_nll": 0.1555478349328041,
      "average_nll": 0.07777391746640205,
      "perplexity": 1.0808782633480787,
      "avg_token_probability": 0.925440390249039,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_19297--8/8_1898303.txt#0_0",
      "dataset_type": "short",
      "response": "spa-francorchamps",
      "accuracy": 0.0,
      "g_nll": 3.17146364088876,
      "average_nll": 0.52857727348146,
      "perplexity": 1.6965169112022804,
      "avg_token_probability": 0.7528015471315895,
      "sequence_length": 6
    },
    {
      "example_id": "qg_762--191/191_2525665.txt#0_1",
      "dataset_type": "short",
      "response": "adam's apple",
      "accuracy": 0.0,
      "g_nll": 0.25354523747228086,
      "average_nll": 0.06338630936807021,
      "perplexity": 1.0654383485563828,
      "avg_token_probability": 0.9422449297488165,
      "sequence_length": 4
    },
    {
      "example_id": "jp_2596--47/47_1415882.txt#0_0",
      "dataset_type": "short",
      "response": "cleopatra",
      "accuracy": 1.0,
      "g_nll": 0.1148222349252137,
      "average_nll": 0.028705558731303427,
      "perplexity": 1.0291215340104494,
      "avg_token_probability": 0.9721136932387302,
      "sequence_length": 4
    },
    {
      "example_id": "qg_2692--122/122_2425619.txt#0_0",
      "dataset_type": "short",
      "response": "jon stewart",
      "accuracy": 1.0,
      "g_nll": 0.08306472108228036,
      "average_nll": 0.02076618027057009,
      "perplexity": 1.0209832976875066,
      "avg_token_probability": 0.9797386707717609,
      "sequence_length": 4
    },
    {
      "example_id": "tb_1432--124/124_757762.txt#0_0",
      "dataset_type": "short",
      "response": "toledo",
      "accuracy": 1.0,
      "g_nll": 0.03255222720326856,
      "average_nll": 0.01085074240108952,
      "perplexity": 1.0109098252104944,
      "avg_token_probability": 0.9892465903213788,
      "sequence_length": 3
    },
    {
      "example_id": "qw_14885--124/124_1320147.txt#0_1",
      "dataset_type": "short",
      "response": "ko-ko",
      "accuracy": 0.0,
      "g_nll": 1.4860040330677293,
      "average_nll": 0.37150100826693233,
      "perplexity": 1.4499093079962904,
      "avg_token_probability": 0.7337809876521373,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_8765--71/71_3209534.txt#0_0",
      "dataset_type": "short",
      "response": "lord belborough",
      "accuracy": 1.0,
      "g_nll": 0.16531926492461935,
      "average_nll": 0.04132981623115484,
      "perplexity": 1.0421957819522645,
      "avg_token_probability": 0.9610864103770631,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_15153--128/128_1671883.txt#0_0",
      "dataset_type": "short",
      "response": "isaac",
      "accuracy": 0.0,
      "g_nll": 0.4588985764021345,
      "average_nll": 0.15296619213404483,
      "perplexity": 1.1652855824580148,
      "avg_token_probability": 0.8759353998574252,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9024--160/160_3017466.txt#0_2",
      "dataset_type": "short",
      "response": "doubting castle",
      "accuracy": 1.0,
      "g_nll": 0.06058097048503441,
      "average_nll": 0.015145242621258603,
      "perplexity": 1.0152605130057337,
      "avg_token_probability": 0.9850690341435736,
      "sequence_length": 4
    },
    {
      "example_id": "qb_9081--150/150_353549.txt#0_2",
      "dataset_type": "short",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.046792393550276756,
      "average_nll": 0.023396196775138378,
      "perplexity": 1.023672034772939,
      "avg_token_probability": 0.9768797048837367,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24683--102/102_3210389.txt#0_0",
      "dataset_type": "short",
      "response": "paul bayes",
      "accuracy": 1.0,
      "g_nll": 0.06410532433073968,
      "average_nll": 0.01602633108268492,
      "perplexity": 1.016155441526786,
      "avg_token_probability": 0.9841556528515407,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_5747--32/32_1597239.txt#0_1",
      "dataset_type": "short",
      "response": "the three graces",
      "accuracy": 1.0,
      "g_nll": 1.7707253545067942,
      "average_nll": 0.3541450709013588,
      "perplexity": 1.4249618919921216,
      "avg_token_probability": 0.7560060502109031,
      "sequence_length": 5
    },
    {
      "example_id": "qb_10061--5/5_543694.txt#0_2",
      "dataset_type": "short",
      "response": "pisces",
      "accuracy": 1.0,
      "g_nll": 0.01927575569243345,
      "average_nll": 0.006425251897477817,
      "perplexity": 1.0064459381094257,
      "avg_token_probability": 0.9936267578464161,
      "sequence_length": 3
    },
    {
      "example_id": "qb_9573--143/143_499089.txt#0_0",
      "dataset_type": "short",
      "response": "15",
      "accuracy": 1.0,
      "g_nll": 0.9450662562157959,
      "average_nll": 0.3150220854052653,
      "perplexity": 1.3702895740233825,
      "avg_token_probability": 0.7624849674757864,
      "sequence_length": 3
    },
    {
      "example_id": "qw_3199--48/48_2960705.txt#0_2",
      "dataset_type": "short",
      "response": "atlantic",
      "accuracy": 1.0,
      "g_nll": 0.38782658610944054,
      "average_nll": 0.12927552870314685,
      "perplexity": 1.1380036336386719,
      "avg_token_probability": 0.8910379429850082,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8510--52/52_28284.txt#0_0",
      "dataset_type": "short",
      "response": "alaska",
      "accuracy": 0.0,
      "g_nll": 2.60574016161263,
      "average_nll": 0.8685800538708767,
      "perplexity": 2.3835239738596528,
      "avg_token_probability": 0.5167831316207906,
      "sequence_length": 3
    },
    {
      "example_id": "qb_603--163/163_281670.txt#0_1",
      "dataset_type": "short",
      "response": "belgium",
      "accuracy": 1.0,
      "g_nll": 0.22958157767334342,
      "average_nll": 0.07652719255778113,
      "perplexity": 1.0795315451619656,
      "avg_token_probability": 0.930597904052772,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3955--64/64_337467.txt#0_1",
      "dataset_type": "short",
      "response": "baseball",
      "accuracy": 0.0,
      "g_nll": 1.8880884130485356,
      "average_nll": 0.9440442065242678,
      "perplexity": 2.5703554751579425,
      "avg_token_probability": 0.5727426520354749,
      "sequence_length": 2
    },
    {
      "example_id": "qb_406--51/51_276103.txt#0_0",
      "dataset_type": "short",
      "response": "madison square garden",
      "accuracy": 1.0,
      "g_nll": 0.3027757588934037,
      "average_nll": 0.06055515177868074,
      "perplexity": 1.0624261906486852,
      "avg_token_probability": 0.9466174875259996,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_23994--31/31_726041.txt#0_0",
      "dataset_type": "short",
      "response": "richard strauss",
      "accuracy": 1.0,
      "g_nll": 0.1313445115513332,
      "average_nll": 0.02626890231026664,
      "perplexity": 1.0266169710354969,
      "avg_token_probability": 0.975074794533491,
      "sequence_length": 5
    },
    {
      "example_id": "wh_2249--178/178_778787.txt#0_0",
      "dataset_type": "short",
      "response": "baby prams",
      "accuracy": 0.0,
      "g_nll": 0.7445545606315136,
      "average_nll": 0.1861386401578784,
      "perplexity": 1.204589253245652,
      "avg_token_probability": 0.8627739039955921,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5653--76/76_551825.txt#0_0",
      "dataset_type": "short",
      "response": "trombone",
      "accuracy": 1.0,
      "g_nll": 1.0515412122476846,
      "average_nll": 0.35051373741589487,
      "perplexity": 1.4197967639852291,
      "avg_token_probability": 0.7792755409392873,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_178--183/183_95802.txt#0_0",
      "dataset_type": "short",
      "response": "pulsar",
      "accuracy": 1.0,
      "g_nll": 0.0226579605077859,
      "average_nll": 0.01132898025389295,
      "perplexity": 1.0113933961764345,
      "avg_token_probability": 0.9887947080416941,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2312--185/185_26932.txt#0_0",
      "dataset_type": "short",
      "response": "apollo 11",
      "accuracy": 1.0,
      "g_nll": 0.42688079959043534,
      "average_nll": 0.08537615991808707,
      "perplexity": 1.0891266754553963,
      "avg_token_probability": 0.928926691337519,
      "sequence_length": 5
    },
    {
      "example_id": "jp_3598--137/137_1440776.txt#0_0",
      "dataset_type": "short",
      "response": "lake nicaragua",
      "accuracy": 0.0,
      "g_nll": 0.04943257845388871,
      "average_nll": 0.009886515690777743,
      "perplexity": 1.0099355487424913,
      "avg_token_probability": 0.9902354677718549,
      "sequence_length": 5
    },
    {
      "example_id": "qb_804--33/33_287054.txt#0_0",
      "dataset_type": "short",
      "response": "leprosy",
      "accuracy": 1.0,
      "g_nll": 0.10692397563980194,
      "average_nll": 0.026730993909950485,
      "perplexity": 1.0270914717368562,
      "avg_token_probability": 0.9742422676559674,
      "sequence_length": 4
    },
    {
      "example_id": "qb_9589--67/67_530673.txt#0_1",
      "dataset_type": "short",
      "response": "world war 1",
      "accuracy": 1.0,
      "g_nll": 1.4355430851137498,
      "average_nll": 0.28710861702275,
      "perplexity": 1.3325689452868106,
      "avg_token_probability": 0.8347553391439002,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_25127--Norway.txt#0_0",
      "dataset_type": "short",
      "response": "norwege",
      "accuracy": 0.0,
      "g_nll": 6.2894256338477135,
      "average_nll": 1.5723564084619284,
      "perplexity": 4.817987975584245,
      "avg_token_probability": 0.4826278577015849,
      "sequence_length": 4
    },
    {
      "example_id": "odql_9408--60/60_2232484.txt#0_0",
      "dataset_type": "short",
      "response": "steve coogan",
      "accuracy": 1.0,
      "g_nll": 0.08062919414442149,
      "average_nll": 0.016125838828884298,
      "perplexity": 1.0162565618956072,
      "avg_token_probability": 0.9841722429080828,
      "sequence_length": 5
    },
    {
      "example_id": "jp_65--42/42_1350780.txt#0_0",
      "dataset_type": "short",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.02551840990463461,
      "average_nll": 0.008506136634878203,
      "perplexity": 1.0085424166096195,
      "avg_token_probability": 0.9915724801226132,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2814--35/35_85764.txt#0_0",
      "dataset_type": "short",
      "response": "ornithology",
      "accuracy": 1.0,
      "g_nll": 0.029793532317853533,
      "average_nll": 0.007448383079463383,
      "perplexity": 1.00747619128389,
      "avg_token_probability": 0.9926406620667523,
      "sequence_length": 4
    },
    {
      "example_id": "bb_3629--115/115_2676705.txt#0_0",
      "dataset_type": "short",
      "response": "mozilla",
      "accuracy": 1.0,
      "g_nll": 0.08494254807010293,
      "average_nll": 0.042471274035051465,
      "perplexity": 1.0433860836711155,
      "avg_token_probability": 0.9590532329012078,
      "sequence_length": 2
    },
    {
      "example_id": "odql_2332--58/58_2808154.txt#0_0",
      "dataset_type": "short",
      "response": "david paradine",
      "accuracy": 1.0,
      "g_nll": 0.7394109070273771,
      "average_nll": 0.24647030234245904,
      "perplexity": 1.2795011844653692,
      "avg_token_probability": 0.8163996512496411,
      "sequence_length": 3
    },
    {
      "example_id": "qw_7795--46/46_1197146.txt#0_1",
      "dataset_type": "short",
      "response": "book",
      "accuracy": 0.0,
      "g_nll": 0.4264300325885415,
      "average_nll": 0.21321501629427075,
      "perplexity": 1.2376507377113366,
      "avg_token_probability": 0.8247022320739532,
      "sequence_length": 2
    },
    {
      "example_id": "qb_723--49/49_284785.txt#0_0",
      "dataset_type": "short",
      "response": "bangladesh",
      "accuracy": 1.0,
      "g_nll": 0.041809572372585535,
      "average_nll": 0.013936524124195179,
      "perplexity": 1.0140340901936118,
      "avg_token_probability": 0.9863059159036202,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_1470--Sting_(musician).txt#0_0",
      "dataset_type": "short",
      "response": "lute",
      "accuracy": 1.0,
      "g_nll": 0.09998255874961615,
      "average_nll": 0.03332751958320538,
      "perplexity": 1.0338891027231982,
      "avg_token_probability": 0.9675209273036169,
      "sequence_length": 3
    },
    {
      "example_id": "jp_4058--151/151_131448.txt#0_0",
      "dataset_type": "short",
      "response": "gremlins",
      "accuracy": 1.0,
      "g_nll": 0.06354945641941612,
      "average_nll": 0.01588736410485403,
      "perplexity": 1.0160142392875222,
      "avg_token_probability": 0.9843616608438773,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1767--17/17_778036.txt#0_2",
      "dataset_type": "short",
      "response": "bolivia",
      "accuracy": 1.0,
      "g_nll": 0.07582567478675628,
      "average_nll": 0.02527522492891876,
      "perplexity": 1.0255973516423327,
      "avg_token_probability": 0.9752000775726416,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3732--157/157_915760.txt#0_0",
      "dataset_type": "short",
      "response": "project glass",
      "accuracy": 1.0,
      "g_nll": 0.3735671790782362,
      "average_nll": 0.12452239302607875,
      "perplexity": 1.132607382682933,
      "avg_token_probability": 0.8948111130559812,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_893--144/144_1486574.txt#0_1",
      "dataset_type": "short",
      "response": "tenerife",
      "accuracy": 1.0,
      "g_nll": 0.719905570139872,
      "average_nll": 0.23996852337995733,
      "perplexity": 1.2712091363246765,
      "avg_token_probability": 0.8216716777420877,
      "sequence_length": 3
    },
    {
      "example_id": "qz_3456--13/13_2880285.txt#0_0",
      "dataset_type": "short",
      "response": "pippin fort",
      "accuracy": 0.0,
      "g_nll": 1.998350896697957,
      "average_nll": 0.3996701793395914,
      "perplexity": 1.49133274416695,
      "avg_token_probability": 0.777753559427237,
      "sequence_length": 5
    },
    {
      "example_id": "qw_1421--131/131_1224551.txt#0_0",
      "dataset_type": "short",
      "response": "new zealand",
      "accuracy": 1.0,
      "g_nll": 0.2631643630402323,
      "average_nll": 0.06579109076005807,
      "perplexity": 1.0680035780431483,
      "avg_token_probability": 0.9408331305810838,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5852--179/179_961871.txt#0_1",
      "dataset_type": "short",
      "response": "gold",
      "accuracy": 1.0,
      "g_nll": 0.022513864561915398,
      "average_nll": 0.011256932280957699,
      "perplexity": 1.011320529957363,
      "avg_token_probability": 0.9888106205865832,
      "sequence_length": 2
    },
    {
      "example_id": "qb_2689--32/32_340888.txt#0_0",
      "dataset_type": "short",
      "response": "a kid for two farthings",
      "accuracy": 0.0,
      "g_nll": 0.3572815579827875,
      "average_nll": 0.05104022256896964,
      "perplexity": 1.0523652212646217,
      "avg_token_probability": 0.9563927060787851,
      "sequence_length": 7
    },
    {
      "example_id": "odql_14833--5/5_2353740.txt#0_2",
      "dataset_type": "short",
      "response": "atomic kitten",
      "accuracy": 1.0,
      "g_nll": 0.08142058132216334,
      "average_nll": 0.02714019377405445,
      "perplexity": 1.0275118434296673,
      "avg_token_probability": 0.9733597996333506,
      "sequence_length": 3
    },
    {
      "example_id": "qw_11763--166/166_2714280.txt#0_0",
      "dataset_type": "short",
      "response": "a psychological horror film",
      "accuracy": 0.0,
      "g_nll": 1.9929187428206205,
      "average_nll": 0.3985837485641241,
      "perplexity": 1.48971339419233,
      "avg_token_probability": 0.7460560861404224,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_11775--22/22_3209747.txt#0_0",
      "dataset_type": "short",
      "response": "wheel arrangement",
      "accuracy": 1.0,
      "g_nll": 0.260064035654068,
      "average_nll": 0.08668801188468933,
      "perplexity": 1.090556386005731,
      "avg_token_probability": 0.9190134570869898,
      "sequence_length": 3
    },
    {
      "example_id": "wh_4376--136/136_828693.txt#0_0",
      "dataset_type": "short",
      "response": "volleyball",
      "accuracy": 1.0,
      "g_nll": 0.22061308287084103,
      "average_nll": 0.11030654143542051,
      "perplexity": 1.1166203083932922,
      "avg_token_probability": 0.8995623771534238,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_99--56/56_553713.txt#0_1",
      "dataset_type": "short",
      "response": "liver",
      "accuracy": 1.0,
      "g_nll": 0.051005857065320015,
      "average_nll": 0.025502928532660007,
      "perplexity": 1.0258309104453704,
      "avg_token_probability": 0.9748266748758074,
      "sequence_length": 2
    },
    {
      "example_id": "qf_1776--73/73_1938527.txt#0_0",
      "dataset_type": "short",
      "response": "anthony joshua",
      "accuracy": 1.0,
      "g_nll": 0.04282285640862682,
      "average_nll": 0.007137142734771136,
      "perplexity": 1.007162672839171,
      "avg_token_probability": 0.9929308144978818,
      "sequence_length": 6
    },
    {
      "example_id": "odql_3170--63/63_218030.txt#0_1",
      "dataset_type": "short",
      "response": "jessica",
      "accuracy": 1.0,
      "g_nll": 0.38401373208034784,
      "average_nll": 0.09600343302008696,
      "perplexity": 1.100762842928443,
      "avg_token_probability": 0.9164920305762652,
      "sequence_length": 4
    },
    {
      "example_id": "qw_11746--112/112_168063.txt#0_1",
      "dataset_type": "short",
      "response": "winter",
      "accuracy": 1.0,
      "g_nll": 0.9497006507590413,
      "average_nll": 0.47485032537952065,
      "perplexity": 1.6077735365819128,
      "avg_token_probability": 0.6907221222973786,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_1253--77/77_2739999.txt#0_0",
      "dataset_type": "short",
      "response": "nettle",
      "accuracy": 1.0,
      "g_nll": 1.1181710362432113,
      "average_nll": 0.3727236787477371,
      "perplexity": 1.4516831535003087,
      "avg_token_probability": 0.7685460939276085,
      "sequence_length": 3
    },
    {
      "example_id": "tc_1945--192/192_114247.txt#0_2",
      "dataset_type": "short",
      "response": "schenck",
      "accuracy": 0.0,
      "g_nll": 0.03711518523641644,
      "average_nll": 0.00927879630910411,
      "perplexity": 1.009321977793642,
      "avg_token_probability": 0.990806659470025,
      "sequence_length": 4
    },
    {
      "example_id": "bb_9117--33/33_1038201.txt#0_0",
      "dataset_type": "short",
      "response": "arthur",
      "accuracy": 1.0,
      "g_nll": 0.08832711260765791,
      "average_nll": 0.029442370869219303,
      "perplexity": 1.0298800826677104,
      "avg_token_probability": 0.9712922053188718,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9352--51/51_1224596.txt#0_0",
      "dataset_type": "short",
      "response": "jarrah",
      "accuracy": 1.0,
      "g_nll": 0.3435225039575016,
      "average_nll": 0.1145075013191672,
      "perplexity": 1.1213210524195176,
      "avg_token_probability": 0.9017085203233304,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_6097--26/26_171255.txt#0_0",
      "dataset_type": "short",
      "response": "russia",
      "accuracy": 0.0,
      "g_nll": 0.024428161792457104,
      "average_nll": 0.0081427205974857,
      "perplexity": 1.0081759627126776,
      "avg_token_probability": 0.9919128998728045,
      "sequence_length": 3
    },
    {
      "example_id": "bb_2192--123/123_881325.txt#0_1",
      "dataset_type": "short",
      "response": "a tale of two cities",
      "accuracy": 0.0,
      "g_nll": 0.2572479840782762,
      "average_nll": 0.042874664013046036,
      "perplexity": 1.0438070600635676,
      "avg_token_probability": 0.9593490214206178,
      "sequence_length": 6
    },
    {
      "example_id": "bb_1292--123/123_187127.txt#0_1",
      "dataset_type": "short",
      "response": "september",
      "accuracy": 1.0,
      "g_nll": 0.1390046812593937,
      "average_nll": 0.06950234062969685,
      "perplexity": 1.071974570297348,
      "avg_token_probability": 0.933228664785915,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_15107--7/7_246280.txt#0_0",
      "dataset_type": "short",
      "response": "chimaera",
      "accuracy": 1.0,
      "g_nll": 0.21902637928587865,
      "average_nll": 0.05475659482146966,
      "perplexity": 1.0562834785139608,
      "avg_token_probability": 0.9492928875120223,
      "sequence_length": 4
    },
    {
      "example_id": "bb_2094--73/73_318480.txt#0_1",
      "dataset_type": "short",
      "response": "10",
      "accuracy": 0.0,
      "g_nll": 2.0441928901709616,
      "average_nll": 0.6813976300569872,
      "perplexity": 1.97663841183284,
      "avg_token_probability": 0.6993326177150019,
      "sequence_length": 3
    },
    {
      "example_id": "bb_6229--199/199_904245.txt#0_2",
      "dataset_type": "short",
      "response": "hydrogen cyanide",
      "accuracy": 1.0,
      "g_nll": 0.48422596010732377,
      "average_nll": 0.12105649002683094,
      "perplexity": 1.12868867021975,
      "avg_token_probability": 0.901811610646918,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3356--154/154_643057.txt#0_1",
      "dataset_type": "short",
      "response": "noah",
      "accuracy": 1.0,
      "g_nll": 0.34107453499382245,
      "average_nll": 0.11369151166460749,
      "perplexity": 1.1204064392494943,
      "avg_token_probability": 0.9021467928209578,
      "sequence_length": 3
    },
    {
      "example_id": "qw_1930--34/34_112989.txt#0_2",
      "dataset_type": "short",
      "response": "nose",
      "accuracy": 1.0,
      "g_nll": 0.3254245202988386,
      "average_nll": 0.1627122601494193,
      "perplexity": 1.1766980580484931,
      "avg_token_probability": 0.8588863540981524,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_7913--89/89_1646797.txt#0_0",
      "dataset_type": "short",
      "response": "upright vacuum cleaner",
      "accuracy": 1.0,
      "g_nll": 0.2982119903899729,
      "average_nll": 0.07455299759749323,
      "perplexity": 1.0774024417501533,
      "avg_token_probability": 0.9343657557378079,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_223--47/47_103402.txt#0_0",
      "dataset_type": "short",
      "response": "surrealist",
      "accuracy": 1.0,
      "g_nll": 0.9151629704865627,
      "average_nll": 0.3050543234955209,
      "perplexity": 1.3566987016202634,
      "avg_token_probability": 0.788161757555557,
      "sequence_length": 3
    },
    {
      "example_id": "odql_6316--193/193_2196789.txt#0_1",
      "dataset_type": "short",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.00015639170487702359,
      "average_nll": 7.819585243851179e-05,
      "perplexity": 1.000078198909814,
      "avg_token_probability": 0.9999218095862491,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7632--22/22_167620.txt#0_1",
      "dataset_type": "short",
      "response": "leroy",
      "accuracy": 1.0,
      "g_nll": 0.6268641948699383,
      "average_nll": 0.31343209743496914,
      "perplexity": 1.3681125612555542,
      "avg_token_probability": 0.7671321799243069,
      "sequence_length": 2
    },
    {
      "example_id": "bb_7149--145/145_206396.txt#0_1",
      "dataset_type": "short",
      "response": "kazakhstan",
      "accuracy": 1.0,
      "g_nll": 3.5686278090579435,
      "average_nll": 1.1895426030193146,
      "perplexity": 3.2855780501595877,
      "avg_token_probability": 0.6505792422989349,
      "sequence_length": 3
    },
    {
      "example_id": "qb_10019--56/56_542648.txt#0_0",
      "dataset_type": "short",
      "response": "ely",
      "accuracy": 1.0,
      "g_nll": 0.16706286351654853,
      "average_nll": 0.055687621172182844,
      "perplexity": 1.0572673642069632,
      "avg_token_probability": 0.9473627500439354,
      "sequence_length": 3
    },
    {
      "example_id": "jp_4108--93/93_1452941.txt#0_1",
      "dataset_type": "short",
      "response": "moonwalk",
      "accuracy": 1.0,
      "g_nll": 0.541419381275773,
      "average_nll": 0.18047312709192434,
      "perplexity": 1.197783933110462,
      "avg_token_probability": 0.8574863033827101,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_14649--125/125_1796628.txt#0_1",
      "dataset_type": "short",
      "response": "dutch",
      "accuracy": 1.0,
      "g_nll": 0.9804908260699676,
      "average_nll": 0.3268302753566559,
      "perplexity": 1.3865661227674855,
      "avg_token_probability": 0.7861554347939843,
      "sequence_length": 3
    },
    {
      "example_id": "qb_6660--191/191_451330.txt#0_2",
      "dataset_type": "short",
      "response": "Grail",
      "accuracy": 1.0,
      "g_nll": 1.682305267879201,
      "average_nll": 0.5607684226264004,
      "perplexity": 1.7520182736518735,
      "avg_token_probability": 0.7252739966046625,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_19040--87/87_1892922.txt#0_2",
      "dataset_type": "short",
      "response": "red",
      "accuracy": 1.0,
      "g_nll": 0.02958521991968155,
      "average_nll": 0.014792609959840775,
      "perplexity": 1.0149025621052987,
      "avg_token_probability": 0.9853600969690295,
      "sequence_length": 2
    },
    {
      "example_id": "tb_1113--48/48_567125.txt#0_1",
      "dataset_type": "short",
      "response": "man",
      "accuracy": 1.0,
      "g_nll": 0.2889489019289613,
      "average_nll": 0.14447445096448064,
      "perplexity": 1.1554321744089793,
      "avg_token_probability": 0.8736594119817138,
      "sequence_length": 2
    },
    {
      "example_id": "qb_9264--54/54_521908.txt#0_0",
      "dataset_type": "short",
      "response": "gulf of bahrein",
      "accuracy": 0.0,
      "g_nll": 2.9403716683154926,
      "average_nll": 0.4200530954736418,
      "perplexity": 1.5220423670336352,
      "avg_token_probability": 0.7539351201686895,
      "sequence_length": 7
    },
    {
      "example_id": "bb_5625--174/174_2946795.txt#0_1",
      "dataset_type": "short",
      "response": "directors",
      "accuracy": 1.0,
      "g_nll": 1.7638242617249489,
      "average_nll": 0.8819121308624744,
      "perplexity": 2.415514072382029,
      "avg_token_probability": 0.54836289335854,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11051--136/136_235136.txt#0_2",
      "dataset_type": "short",
      "response": "$100",
      "accuracy": 0.0,
      "g_nll": 0.6293749138712883,
      "average_nll": 0.2097916379570961,
      "perplexity": 1.2334210350539316,
      "avg_token_probability": 0.8281086984068803,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9028--150/150_617354.txt#0_0",
      "dataset_type": "short",
      "response": "beyonce",
      "accuracy": 0.0,
      "g_nll": 0.0854998412396526,
      "average_nll": 0.028499947079884198,
      "perplexity": 1.0289099563844883,
      "avg_token_probability": 0.9721188120225653,
      "sequence_length": 3
    },
    {
      "example_id": "bb_7263--97/97_992367.txt#0_0",
      "dataset_type": "short",
      "response": "turkey",
      "accuracy": 1.0,
      "g_nll": 0.11076187342405319,
      "average_nll": 0.055380936712026596,
      "perplexity": 1.0569431664518785,
      "avg_token_probability": 0.9468760248692016,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7717--108/108_2904125.txt#0_0",
      "dataset_type": "short",
      "response": "ireland",
      "accuracy": 1.0,
      "g_nll": 0.1177601087729272,
      "average_nll": 0.03925336959097573,
      "perplexity": 1.0400339632497586,
      "avg_token_probability": 0.9624853953996834,
      "sequence_length": 3
    },
    {
      "example_id": "qg_158--17/17_2239200.txt#0_0",
      "dataset_type": "short",
      "response": "carmen",
      "accuracy": 1.0,
      "g_nll": 0.43956194826751016,
      "average_nll": 0.1465206494225034,
      "perplexity": 1.1577988384490052,
      "avg_token_probability": 0.876842723969106,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_11783--191/191_1732943.txt#0_1",
      "dataset_type": "short",
      "response": "chess",
      "accuracy": 1.0,
      "g_nll": 0.20584776299074292,
      "average_nll": 0.10292388149537146,
      "perplexity": 1.1084070355787878,
      "avg_token_probability": 0.9063677822370129,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1920--147/147_1398729.txt#0_2",
      "dataset_type": "short",
      "response": "most interesting man in the world",
      "accuracy": 0.0,
      "g_nll": 7.472146025796974,
      "average_nll": 1.0674494322567105,
      "perplexity": 2.9079531018566263,
      "avg_token_probability": 0.7928319318690694,
      "sequence_length": 7
    },
    {
      "example_id": "sfq_893--144/144_1486574.txt#0_0",
      "dataset_type": "short",
      "response": "tenerife",
      "accuracy": 1.0,
      "g_nll": 0.048273688031258644,
      "average_nll": 0.016091229343752882,
      "perplexity": 1.0162213903878758,
      "avg_token_probability": 0.984114977008018,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_5791--38/38_519647.txt#0_2",
      "dataset_type": "short",
      "response": "Gladstone",
      "accuracy": 1.0,
      "g_nll": 4.314237448306812,
      "average_nll": 1.4380791494356042,
      "perplexity": 4.212596272551976,
      "avg_token_probability": 0.6639840105431882,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_1635--165/165_596631.txt#0_2",
      "dataset_type": "short",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 0.025319733656942844,
      "average_nll": 0.012659866828471422,
      "perplexity": 1.0127403421863879,
      "avg_token_probability": 0.9874281567774165,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3490--39/39_3004847.txt#0_0",
      "dataset_type": "short",
      "response": "leeds",
      "accuracy": 1.0,
      "g_nll": 0.9940066213457612,
      "average_nll": 0.3313355404485871,
      "perplexity": 1.3928270637273468,
      "avg_token_probability": 0.7461461977870805,
      "sequence_length": 3
    },
    {
      "example_id": "qz_2567--129/129_10435.txt#0_0",
      "dataset_type": "short",
      "response": "ironside",
      "accuracy": 1.0,
      "g_nll": 0.28275122260674834,
      "average_nll": 0.09425040753558278,
      "perplexity": 1.0988348680007232,
      "avg_token_probability": 0.9161372135571795,
      "sequence_length": 3
    },
    {
      "example_id": "odql_9169--18/18_3213102.txt#0_1",
      "dataset_type": "short",
      "response": "lungs",
      "accuracy": 1.0,
      "g_nll": 0.050273902248591185,
      "average_nll": 0.025136951124295592,
      "perplexity": 1.0254555481985863,
      "avg_token_probability": 0.9753646929886945,
      "sequence_length": 2
    },
    {
      "example_id": "odql_9169--18/18_3213102.txt#0_0",
      "dataset_type": "short",
      "response": "lungs",
      "accuracy": 1.0,
      "g_nll": 0.06309748068451881,
      "average_nll": 0.03154874034225941,
      "perplexity": 1.0320516769217467,
      "avg_token_probability": 0.9690483781113417,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_4619--47/47_1571719.txt#0_0",
      "dataset_type": "short",
      "response": "eye",
      "accuracy": 1.0,
      "g_nll": 0.4379776818677783,
      "average_nll": 0.21898884093388915,
      "perplexity": 1.2448173856105236,
      "avg_token_probability": 0.8207859893602601,
      "sequence_length": 2
    },
    {
      "example_id": "qb_1032--189/189_293644.txt#0_0",
      "dataset_type": "short",
      "response": "squash",
      "accuracy": 1.0,
      "g_nll": 0.01733155222609639,
      "average_nll": 0.008665776113048196,
      "perplexity": 1.0087034326466355,
      "avg_token_probability": 0.9914042712291719,
      "sequence_length": 2
    },
    {
      "example_id": "qb_5053--95/95_407079.txt#0_0",
      "dataset_type": "short",
      "response": "shoji",
      "accuracy": 1.0,
      "g_nll": 0.014640657387644751,
      "average_nll": 0.004880219129214917,
      "perplexity": 1.0048921467939018,
      "avg_token_probability": 0.9951544633010112,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1572--159/159_685600.txt#0_2",
      "dataset_type": "short",
      "response": "slainte",
      "accuracy": 0.0,
      "g_nll": 3.5498720144441904,
      "average_nll": 1.1832906714813969,
      "perplexity": 3.2651009186381796,
      "avg_token_probability": 0.6743648331254589,
      "sequence_length": 3
    },
    {
      "example_id": "qb_7640--89/89_2634466.txt#0_1",
      "dataset_type": "short",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.019981103869213257,
      "average_nll": 0.006660367956404419,
      "perplexity": 1.006682597532042,
      "avg_token_probability": 0.9933891614999021,
      "sequence_length": 3
    },
    {
      "example_id": "bt_4328--47/47_176665.txt#0_0",
      "dataset_type": "short",
      "response": "arthur wellesley",
      "accuracy": 1.0,
      "g_nll": 0.9352775785869198,
      "average_nll": 0.15587959643115332,
      "perplexity": 1.1686854807155636,
      "avg_token_probability": 0.8736198705111432,
      "sequence_length": 6
    },
    {
      "example_id": "qw_597--13/13_1061577.txt#0_2",
      "dataset_type": "short",
      "response": "pygmalion",
      "accuracy": 1.0,
      "g_nll": 0.6921574714936014,
      "average_nll": 0.1384314942987203,
      "perplexity": 1.1484710020638162,
      "avg_token_probability": 0.8948172052998908,
      "sequence_length": 5
    },
    {
      "example_id": "qw_2152--104/104_1092719.txt#0_1",
      "dataset_type": "short",
      "response": "Euripides",
      "accuracy": 1.0,
      "g_nll": 0.885455846786499,
      "average_nll": 0.885455846786499,
      "perplexity": 2.4240891529159896,
      "avg_token_probability": 0.41252608172314054,
      "sequence_length": 1
    },
    {
      "example_id": "bb_7071--140/140_988015.txt#0_0",
      "dataset_type": "short",
      "response": "indigo",
      "accuracy": 0.0,
      "g_nll": 0.12245386838276318,
      "average_nll": 0.04081795612758773,
      "perplexity": 1.0416624600161322,
      "avg_token_probability": 0.9606898322421856,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1626--6/6_1391170.txt#0_1",
      "dataset_type": "short",
      "response": "playboy",
      "accuracy": 1.0,
      "g_nll": 0.053800567155121826,
      "average_nll": 0.017933522385040607,
      "perplexity": 1.0180952935932037,
      "avg_token_probability": 0.9823148457659568,
      "sequence_length": 3
    },
    {
      "example_id": "odql_4032--121/121_112363.txt#0_0",
      "dataset_type": "short",
      "response": "butter",
      "accuracy": 1.0,
      "g_nll": 0.41634383890777826,
      "average_nll": 0.20817191945388913,
      "perplexity": 1.2314248572406925,
      "avg_token_probability": 0.8278894244953765,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3803--102/102_234526.txt#0_0",
      "dataset_type": "short",
      "response": "charles de gaule",
      "accuracy": 1.0,
      "g_nll": 5.208317587766942,
      "average_nll": 0.8680529312944903,
      "perplexity": 2.38226789564435,
      "avg_token_probability": 0.6632476876906678,
      "sequence_length": 6
    },
    {
      "example_id": "wh_2348--155/155_1799804.txt#0_0",
      "dataset_type": "short",
      "response": "gower",
      "accuracy": 1.0,
      "g_nll": 0.40710818764910073,
      "average_nll": 0.20355409382455036,
      "perplexity": 1.225751461432375,
      "avg_token_probability": 0.8327848739443078,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_20420--125/125_1922602.txt#0_0",
      "dataset_type": "short",
      "response": "wisconsin",
      "accuracy": 1.0,
      "g_nll": 0.04010570417449344,
      "average_nll": 0.01336856805816448,
      "perplexity": 1.013458326900529,
      "avg_token_probability": 0.9867643052037028,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9826--160/160_662376.txt#0_0",
      "dataset_type": "short",
      "response": "the lady of the lake",
      "accuracy": 0.0,
      "g_nll": 0.17403953291250218,
      "average_nll": 0.029006588818750362,
      "perplexity": 1.029431377189546,
      "avg_token_probability": 0.9727534222748281,
      "sequence_length": 6
    },
    {
      "example_id": "qb_4219--118/118_12770.txt#0_0",
      "dataset_type": "short",
      "response": "Neville Chamberlain",
      "accuracy": 0.0,
      "g_nll": 0.9615893028571918,
      "average_nll": 0.24039732571429795,
      "perplexity": 1.2717543506560025,
      "avg_token_probability": 0.8361062618413675,
      "sequence_length": 4
    },
    {
      "example_id": "qg_952--87/87_3215096.txt#0_0",
      "dataset_type": "short",
      "response": "grumpy",
      "accuracy": 0.0,
      "g_nll": 0.6036791801452637,
      "average_nll": 0.6036791801452637,
      "perplexity": 1.8288350512755365,
      "avg_token_probability": 0.5467961691255543,
      "sequence_length": 1
    },
    {
      "example_id": "qb_2899--155/155_2893117.txt#0_2",
      "dataset_type": "short",
      "response": "louis xv",
      "accuracy": 1.0,
      "g_nll": 0.17061495780944824,
      "average_nll": 0.08530747890472412,
      "perplexity": 1.0890518757003365,
      "avg_token_probability": 0.9207595314679273,
      "sequence_length": 2
    },
    {
      "example_id": "qw_9374--152/152_1224920.txt#0_1",
      "dataset_type": "short",
      "response": "coffee shop",
      "accuracy": 1.0,
      "g_nll": 1.8370522912591696,
      "average_nll": 0.6123507637530565,
      "perplexity": 1.8447629073858998,
      "avg_token_probability": 0.7070492520541726,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22602--23/23_1970054.txt#0_0",
      "dataset_type": "short",
      "response": "bubba",
      "accuracy": 1.0,
      "g_nll": 0.14811844378675687,
      "average_nll": 0.04937281459558562,
      "perplexity": 1.0506119612098788,
      "avg_token_probability": 0.9528870388453962,
      "sequence_length": 3
    },
    {
      "example_id": "qg_207--Stockholm_syndrome.txt#0_2",
      "dataset_type": "short",
      "response": "stockholm syndrome",
      "accuracy": 1.0,
      "g_nll": 0.3204047679901123,
      "average_nll": 0.3204047679901123,
      "perplexity": 1.3776852944008051,
      "avg_token_probability": 0.7258551746644931,
      "sequence_length": 1
    },
    {
      "example_id": "qb_2850--83/83_346130.txt#0_1",
      "dataset_type": "short",
      "response": "the times",
      "accuracy": 1.0,
      "g_nll": 0.1921183897939045,
      "average_nll": 0.06403946326463483,
      "perplexity": 1.0661344710781129,
      "avg_token_probability": 0.9403014326443581,
      "sequence_length": 3
    },
    {
      "example_id": "tc_1323--96/96_39278.txt#0_4",
      "dataset_type": "short",
      "response": "vienna",
      "accuracy": 1.0,
      "g_nll": 0.03520547616426484,
      "average_nll": 0.011735158721421612,
      "perplexity": 1.0118042858377796,
      "avg_token_probability": 0.9883716408640156,
      "sequence_length": 3
    },
    {
      "example_id": "wh_2204--68/68_777602.txt#0_0",
      "dataset_type": "short",
      "response": "paddy ashdown",
      "accuracy": 1.0,
      "g_nll": 0.403885429833565,
      "average_nll": 0.080777085966713,
      "perplexity": 1.084129202016182,
      "avg_token_probability": 0.9318665692706487,
      "sequence_length": 5
    },
    {
      "example_id": "odql_2749--117/117_545479.txt#0_1",
      "dataset_type": "short",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.023274686185686733,
      "average_nll": 0.007758228728562244,
      "perplexity": 1.007788401764362,
      "avg_token_probability": 0.9922907110383633,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_17026--190/190_2777940.txt#0_0",
      "dataset_type": "short",
      "response": "virtual",
      "accuracy": 1.0,
      "g_nll": 0.024244952714070678,
      "average_nll": 0.012122476357035339,
      "perplexity": 1.0121962513841583,
      "avg_token_probability": 0.988004917474635,
      "sequence_length": 2
    },
    {
      "example_id": "qb_1032--189/189_293644.txt#0_1",
      "dataset_type": "short",
      "response": "squash",
      "accuracy": 1.0,
      "g_nll": 0.01952971631544642,
      "average_nll": 0.00976485815772321,
      "perplexity": 1.0098126899485877,
      "avg_token_probability": 0.9903262903557536,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_19691--43/43_2784025.txt#0_1",
      "dataset_type": "short",
      "response": "vodka",
      "accuracy": 0.0,
      "g_nll": 1.458447303622961,
      "average_nll": 0.7292236518114805,
      "perplexity": 2.073470247784197,
      "avg_token_probability": 0.5979629035170977,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5063--41/41_1050220.txt#0_1",
      "dataset_type": "short",
      "response": "phaethon",
      "accuracy": 0.0,
      "g_nll": 0.16852428623928972,
      "average_nll": 0.04213107155982243,
      "perplexity": 1.04303118151555,
      "avg_token_probability": 0.9600078869708957,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1022--144/144_7564.txt#0_1",
      "dataset_type": "short",
      "response": "adolf hitler",
      "accuracy": 1.0,
      "g_nll": 0.31973055966682296,
      "average_nll": 0.0639461119333646,
      "perplexity": 1.066034950651179,
      "avg_token_probability": 0.9433004468246964,
      "sequence_length": 5
    },
    {
      "example_id": "qz_6774--13/13_1663667.txt#0_0",
      "dataset_type": "short",
      "response": "lithium",
      "accuracy": 1.0,
      "g_nll": 0.05276104807853699,
      "average_nll": 0.026380524039268494,
      "perplexity": 1.0267315701925857,
      "avg_token_probability": 0.9741253578265854,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3084--56/56_1290885.txt#0_1",
      "dataset_type": "short",
      "response": "tony montana",
      "accuracy": 1.0,
      "g_nll": 0.1869140005146619,
      "average_nll": 0.03738280010293238,
      "perplexity": 1.0380903258726986,
      "avg_token_probability": 0.9654336080254524,
      "sequence_length": 5
    },
    {
      "example_id": "tc_1702--106/106_2596452.txt#0_1",
      "dataset_type": "short",
      "response": "le",
      "accuracy": 0.0,
      "g_nll": 0.29664020240306854,
      "average_nll": 0.14832010120153427,
      "perplexity": 1.1598841172452496,
      "avg_token_probability": 0.8630619963591368,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3674--119/119_1039262.txt#0_0",
      "dataset_type": "short",
      "response": "vienna",
      "accuracy": 0.0,
      "g_nll": 0.030428112469962798,
      "average_nll": 0.010142704156654267,
      "perplexity": 1.010194315726501,
      "avg_token_probability": 0.9899392979603195,
      "sequence_length": 3
    },
    {
      "example_id": "qb_7766--9/9_891153.txt#0_2",
      "dataset_type": "short",
      "response": "troilus and cressida",
      "accuracy": 1.0,
      "g_nll": 0.4057677822047481,
      "average_nll": 0.05072097277559351,
      "perplexity": 1.0520293075082285,
      "avg_token_probability": 0.9524294864524286,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_2345--98/98_1465999.txt#0_0",
      "dataset_type": "short",
      "response": "jaguar",
      "accuracy": 0.0,
      "g_nll": 0.14902503768826136,
      "average_nll": 0.049675012562753786,
      "perplexity": 1.0509295019865108,
      "avg_token_probability": 0.9522117371057539,
      "sequence_length": 3
    },
    {
      "example_id": "odql_13192--6/6_701893.txt#0_1",
      "dataset_type": "short",
      "response": "gettysburg",
      "accuracy": 1.0,
      "g_nll": 0.06575942689772774,
      "average_nll": 0.013151885379545546,
      "perplexity": 1.0132387518255044,
      "avg_token_probability": 0.9870903547016157,
      "sequence_length": 5
    },
    {
      "example_id": "tc_2499--34/34_75500.txt#0_1",
      "dataset_type": "short",
      "response": "russian",
      "accuracy": 0.0,
      "g_nll": 3.1722638681530952,
      "average_nll": 1.5861319340765476,
      "perplexity": 4.884817541729659,
      "avg_token_probability": 0.5046515776802752,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12482--151/151_684649.txt#0_2",
      "dataset_type": "short",
      "response": "philippines",
      "accuracy": 1.0,
      "g_nll": 0.3553388901054859,
      "average_nll": 0.17766944505274296,
      "perplexity": 1.1944304310838008,
      "avg_token_probability": 0.8490030104446986,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3074--10/10_725914.txt#0_0",
      "dataset_type": "short",
      "response": "spain",
      "accuracy": 1.0,
      "g_nll": 0.10052303595693957,
      "average_nll": 0.03350767865231319,
      "perplexity": 1.0340753840011325,
      "avg_token_probability": 0.9675137100094346,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_261--149/149_644071.txt#0_0",
      "dataset_type": "short",
      "response": "carpathia",
      "accuracy": 1.0,
      "g_nll": 0.04211054174538731,
      "average_nll": 0.010527635436346827,
      "perplexity": 1.010583245968097,
      "avg_token_probability": 0.9895853074157066,
      "sequence_length": 4
    },
    {
      "example_id": "bb_1013--62/62_852710.txt#0_2",
      "dataset_type": "short",
      "response": "santiago",
      "accuracy": 0.0,
      "g_nll": 0.3207506545659271,
      "average_nll": 0.10691688485530904,
      "perplexity": 1.1128417566317723,
      "avg_token_probability": 0.9049521593956826,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22341--192/192_1964271.txt#0_0",
      "dataset_type": "short",
      "response": "griffin",
      "accuracy": 1.0,
      "g_nll": 0.3834917799104005,
      "average_nll": 0.12783059330346683,
      "perplexity": 1.136360479315274,
      "avg_token_probability": 0.8925296111106635,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1590--168/168_763549.txt#0",
      "dataset_type": "short",
      "response": "knickerbocker glory\nAnswer the following question with a brief, concise answer. Use only a few words or a short phrase, not a full sentence.",
      "accuracy": 0.0,
      "g_nll": 7.622662046409133,
      "average_nll": 0.22419594254144506,
      "perplexity": 1.2513161814814908,
      "avg_token_probability": 0.9445843244580078,
      "sequence_length": 34
    },
    {
      "example_id": "sfq_19632--8/8_1905614.txt#0_1",
      "dataset_type": "short",
      "response": "jim grant",
      "accuracy": 0.0,
      "g_nll": 1.208060760051012,
      "average_nll": 0.402686920017004,
      "perplexity": 1.4958385012530406,
      "avg_token_probability": 0.6963975415209146,
      "sequence_length": 3
    },
    {
      "example_id": "qb_4246--21/21_114528.txt#0_0",
      "dataset_type": "short",
      "response": "the river thames",
      "accuracy": 1.0,
      "g_nll": 0.7348805842919433,
      "average_nll": 0.14697611685838866,
      "perplexity": 1.1583262982284828,
      "avg_token_probability": 0.8869191422079957,
      "sequence_length": 5
    },
    {
      "example_id": "wh_3949--72/72_779517.txt#0_0",
      "dataset_type": "short",
      "response": "argentina",
      "accuracy": 1.0,
      "g_nll": 0.03823272752924822,
      "average_nll": 0.012744242509749407,
      "perplexity": 1.0128257964477976,
      "avg_token_probability": 0.9873926574681707,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_10974--173/173_3209699.txt#0_0",
      "dataset_type": "short",
      "response": "herbert kitchener",
      "accuracy": 1.0,
      "g_nll": 0.860834625384939,
      "average_nll": 0.1434724375641565,
      "perplexity": 1.1542749957381435,
      "avg_token_probability": 0.9019992672680391,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_10250--131/131_472528.txt#0_2",
      "dataset_type": "short",
      "response": "dragon",
      "accuracy": 1.0,
      "g_nll": 0.17484581470489502,
      "average_nll": 0.08742290735244751,
      "perplexity": 1.0913581255124232,
      "avg_token_probability": 0.9171692197099843,
      "sequence_length": 2
    },
    {
      "example_id": "qf_1735--151/151_264049.txt#0_1",
      "dataset_type": "short",
      "response": "1948",
      "accuracy": 1.0,
      "g_nll": 0.5076798083100584,
      "average_nll": 0.1269199520775146,
      "perplexity": 1.1353261336456502,
      "avg_token_probability": 0.8891939414460087,
      "sequence_length": 4
    },
    {
      "example_id": "bt_2905--92/92_2418244.txt#0_0",
      "dataset_type": "short",
      "response": "sweet william",
      "accuracy": 1.0,
      "g_nll": 8.220677000237629,
      "average_nll": 2.055169250059407,
      "perplexity": 7.808159292231643,
      "avg_token_probability": 0.7280802476696332,
      "sequence_length": 4
    },
    {
      "example_id": "jp_2608--196/196_1416223.txt#0_2",
      "dataset_type": "short",
      "response": "puffy",
      "accuracy": 0.0,
      "g_nll": 2.356391564011574,
      "average_nll": 0.7854638546705246,
      "perplexity": 2.193424134829243,
      "avg_token_probability": 0.5495458107804636,
      "sequence_length": 3
    },
    {
      "example_id": "odql_11066--48/48_1410700.txt#0_2",
      "dataset_type": "short",
      "response": "cello",
      "accuracy": 1.0,
      "g_nll": 0.08418853208422661,
      "average_nll": 0.028062844028075535,
      "perplexity": 1.0284603149794906,
      "avg_token_probability": 0.9728575772812541,
      "sequence_length": 3
    },
    {
      "example_id": "odql_14378--126/126_2345574.txt#0_0",
      "dataset_type": "short",
      "response": "tennessee",
      "accuracy": 1.0,
      "g_nll": 0.24380043991550338,
      "average_nll": 0.08126681330516779,
      "perplexity": 1.0846602597509583,
      "avg_token_probability": 0.9268692270413613,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_2677--78/78_1954583.txt#0_2",
      "dataset_type": "short",
      "response": "keswick",
      "accuracy": 1.0,
      "g_nll": 0.03479589754488188,
      "average_nll": 0.011598632514960627,
      "perplexity": 1.011666157466239,
      "avg_token_probability": 0.9885538716373569,
      "sequence_length": 3
    },
    {
      "example_id": "wh_51--126/126_725818.txt#0_1",
      "dataset_type": "short",
      "response": "avro",
      "accuracy": 1.0,
      "g_nll": 0.06700480542758669,
      "average_nll": 0.022334935142528895,
      "perplexity": 1.0225862271829371,
      "avg_token_probability": 0.9780479699537376,
      "sequence_length": 3
    },
    {
      "example_id": "tc_453--150/150_2867003.txt#0_0",
      "dataset_type": "short",
      "response": "joshua",
      "accuracy": 1.0,
      "g_nll": 0.32573210254668084,
      "average_nll": 0.08143302563667021,
      "perplexity": 1.0848405586451542,
      "avg_token_probability": 0.9292871870328169,
      "sequence_length": 4
    },
    {
      "example_id": "jp_3234--33/33_1767595.txt#0_0",
      "dataset_type": "short",
      "response": "canada",
      "accuracy": 0.0,
      "g_nll": 2.4332200530916452,
      "average_nll": 1.2166100265458226,
      "perplexity": 3.375724697791316,
      "avg_token_probability": 0.5342735333024506,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_23017--20/20_341916.txt#0_0",
      "dataset_type": "short",
      "response": "will carling",
      "accuracy": 1.0,
      "g_nll": 0.0399350624065562,
      "average_nll": 0.00998376560163905,
      "perplexity": 1.0100337696604926,
      "avg_token_probability": 0.9901033234796764,
      "sequence_length": 4
    },
    {
      "example_id": "odql_11582--149/149_524763.txt#0_0",
      "dataset_type": "short",
      "response": "ecuador",
      "accuracy": 1.0,
      "g_nll": 0.037901409900307215,
      "average_nll": 0.009475352475076804,
      "perplexity": 1.0095203857503439,
      "avg_token_probability": 0.990635076059982,
      "sequence_length": 4
    },
    {
      "example_id": "qf_1622--114/114_2391205.txt#0_0",
      "dataset_type": "short",
      "response": "fred trueman",
      "accuracy": 1.0,
      "g_nll": 0.06888998264639667,
      "average_nll": 0.013777996529279335,
      "perplexity": 1.0138733505493018,
      "avg_token_probability": 0.9864265058859178,
      "sequence_length": 5
    },
    {
      "example_id": "dpql_4731--151/151_679982.txt#0_0",
      "dataset_type": "short",
      "response": "honeycomb",
      "accuracy": 0.0,
      "g_nll": 0.3303392441011965,
      "average_nll": 0.1101130813670655,
      "perplexity": 1.1164043078465178,
      "avg_token_probability": 0.9050403024005385,
      "sequence_length": 3
    },
    {
      "example_id": "qw_10923--40/40_1181284.txt#0_1",
      "dataset_type": "short",
      "response": "adrian edmondson",
      "accuracy": 1.0,
      "g_nll": 0.0693696592196602,
      "average_nll": 0.011561609869943368,
      "perplexity": 1.01162870360254,
      "avg_token_probability": 0.9886026348415645,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_10035--99/99_1692646.txt#0_1",
      "dataset_type": "short",
      "response": "republican",
      "accuracy": 0.0,
      "g_nll": 3.076308488845825,
      "average_nll": 3.076308488845825,
      "perplexity": 21.67822908244065,
      "avg_token_probability": 0.04612922929253475,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_7787--39/39_1643620.txt#0_1",
      "dataset_type": "short",
      "response": "michael hordern",
      "accuracy": 1.0,
      "g_nll": 0.1427612962031617,
      "average_nll": 0.028552259240632337,
      "perplexity": 1.0289637822953848,
      "avg_token_probability": 0.9722439478294833,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_10817--189/189_1711448.txt#0_0",
      "dataset_type": "short",
      "response": "sculpture",
      "accuracy": 1.0,
      "g_nll": 0.04406167194247246,
      "average_nll": 0.02203083597123623,
      "perplexity": 1.0222753068363815,
      "avg_token_probability": 0.9782386659941158,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5141--181/181_554365.txt#0_0",
      "dataset_type": "short",
      "response": "st pauls",
      "accuracy": 0.0,
      "g_nll": 0.3427480049431324,
      "average_nll": 0.0856870012357831,
      "perplexity": 1.0894652736487607,
      "avg_token_probability": 0.9187872294775563,
      "sequence_length": 4
    },
    {
      "example_id": "jp_4045--160/160_2995038.txt#0_0",
      "dataset_type": "short",
      "response": "box jellyfish",
      "accuracy": 1.0,
      "g_nll": 0.2798911865102127,
      "average_nll": 0.06997279662755318,
      "perplexity": 1.0724790058115465,
      "avg_token_probability": 0.9379787939552954,
      "sequence_length": 4
    },
    {
      "example_id": "odql_10047--123/123_2824506.txt#0_1",
      "dataset_type": "short",
      "response": "Federal Reserve Board of Governors",
      "accuracy": 1.0,
      "g_nll": 3.8684462844394147,
      "average_nll": 0.6447410474065691,
      "perplexity": 1.9054935328864893,
      "avg_token_probability": 0.6778398495422496,
      "sequence_length": 6
    },
    {
      "example_id": "sfq_9872--171/171_1689203.txt#0_0",
      "dataset_type": "short",
      "response": "south african",
      "accuracy": 1.0,
      "g_nll": 0.18996460177004337,
      "average_nll": 0.06332153392334779,
      "perplexity": 1.0653693365486965,
      "avg_token_probability": 0.9391330851143059,
      "sequence_length": 3
    },
    {
      "example_id": "bb_1886--3/3_874010.txt#0_1",
      "dataset_type": "short",
      "response": "kyoto",
      "accuracy": 1.0,
      "g_nll": 3.124799013135089,
      "average_nll": 1.0415996710450297,
      "perplexity": 2.833746452729352,
      "avg_token_probability": 0.5371863050153327,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_12025--58/58_1700414.txt#0_2",
      "dataset_type": "short",
      "response": "127 hours",
      "accuracy": 1.0,
      "g_nll": 0.03813691812047182,
      "average_nll": 0.009534229530117955,
      "perplexity": 1.0095798250874504,
      "avg_token_probability": 0.9905973308406717,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_4010--66/66_323066.txt#0_0",
      "dataset_type": "short",
      "response": "archery",
      "accuracy": 1.0,
      "g_nll": 0.0038766610814491287,
      "average_nll": 0.001292220360483043,
      "perplexity": 1.0012930556369615,
      "avg_token_probability": 0.9987093744404087,
      "sequence_length": 3
    },
    {
      "example_id": "qf_3745--59/59_2512300.txt#0_2",
      "dataset_type": "short",
      "response": "roger kahn",
      "accuracy": 0.0,
      "g_nll": 0.7029524099052651,
      "average_nll": 0.140590481981053,
      "perplexity": 1.1509532153805961,
      "avg_token_probability": 0.8905666387693929,
      "sequence_length": 5
    },
    {
      "example_id": "bb_7851--165/165_1006156.txt#0_0",
      "dataset_type": "short",
      "response": "1967",
      "accuracy": 1.0,
      "g_nll": 0.02451034790101403,
      "average_nll": 0.006127586975253507,
      "perplexity": 1.0061463990408839,
      "avg_token_probability": 0.9939462238320073,
      "sequence_length": 4
    },
    {
      "example_id": "qb_7077--183/183_462795.txt#0_1",
      "dataset_type": "short",
      "response": "snooker",
      "accuracy": 1.0,
      "g_nll": 0.0164256218704395,
      "average_nll": 0.004106405467609875,
      "perplexity": 1.004114848303155,
      "avg_token_probability": 0.995910098863876,
      "sequence_length": 4
    },
    {
      "example_id": "odql_15005--151/151_2357401.txt#0_2",
      "dataset_type": "short",
      "response": "greece",
      "accuracy": 0.0,
      "g_nll": 0.7891147845657542,
      "average_nll": 0.26303826152191806,
      "perplexity": 1.300876491513924,
      "avg_token_probability": 0.8143477636569973,
      "sequence_length": 3
    },
    {
      "example_id": "qb_4987--156/156_405376.txt#0_2",
      "dataset_type": "short",
      "response": "cyprus",
      "accuracy": 1.0,
      "g_nll": 0.023604393935443113,
      "average_nll": 0.007868131311814372,
      "perplexity": 1.0078991663996355,
      "avg_token_probability": 0.992201515885962,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8918--129/129_512440.txt#0_1",
      "dataset_type": "short",
      "response": "the grapes of wrath",
      "accuracy": 1.0,
      "g_nll": 0.0710533613264488,
      "average_nll": 0.01421067226528976,
      "perplexity": 1.0143121238639103,
      "avg_token_probability": 0.9861068786677853,
      "sequence_length": 5
    },
    {
      "example_id": "bb_6062--81/81_2682584.txt#0_0",
      "dataset_type": "short",
      "response": "ozone",
      "accuracy": 1.0,
      "g_nll": 0.04911739006638527,
      "average_nll": 0.024558695033192635,
      "perplexity": 1.0248627436946114,
      "avg_token_probability": 0.9757893544420202,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_5545--199/199_1592717.txt#0_0",
      "dataset_type": "short",
      "response": "solzhenitsyn",
      "accuracy": 1.0,
      "g_nll": 0.2514153586107568,
      "average_nll": 0.04190255976845947,
      "perplexity": 1.042792863822053,
      "avg_token_probability": 0.9617262605987866,
      "sequence_length": 6
    },
    {
      "example_id": "qw_8941--0/0_1217055.txt#0_2",
      "dataset_type": "short",
      "response": "philadelphia eagles",
      "accuracy": 1.0,
      "g_nll": 0.7978501070754191,
      "average_nll": 0.15957002141508383,
      "perplexity": 1.173006394912845,
      "avg_token_probability": 0.8838097667739738,
      "sequence_length": 5
    },
    {
      "example_id": "qw_11443--84/84_2975478.txt#0_0",
      "dataset_type": "short",
      "response": "peach",
      "accuracy": 0.0,
      "g_nll": 0.03893212409457192,
      "average_nll": 0.01946606204728596,
      "perplexity": 1.0196567612104928,
      "avg_token_probability": 0.9808920006735982,
      "sequence_length": 2
    },
    {
      "example_id": "odql_1378--196/196_2103697.txt#0_2",
      "dataset_type": "short",
      "response": "stupa",
      "accuracy": 0.0,
      "g_nll": 0.9134873347356844,
      "average_nll": 0.30449577824522817,
      "perplexity": 1.3559411355915436,
      "avg_token_probability": 0.7990520466776795,
      "sequence_length": 3
    },
    {
      "example_id": "qg_633--116/116_2857569.txt#0_1",
      "dataset_type": "short",
      "response": "dvorak",
      "accuracy": 1.0,
      "g_nll": 1.4584479896566336,
      "average_nll": 0.3646119974141584,
      "perplexity": 1.4399551934069073,
      "avg_token_probability": 0.8021485456014368,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_4703--146/146_2653628.txt#0_0",
      "dataset_type": "short",
      "response": "the apprentice",
      "accuracy": 1.0,
      "g_nll": 0.06417490099556744,
      "average_nll": 0.021391633665189147,
      "perplexity": 1.0216220748987586,
      "avg_token_probability": 0.9790092315438573,
      "sequence_length": 3
    },
    {
      "example_id": "qb_4415--4/4_390363.txt#0_2",
      "dataset_type": "short",
      "response": "margot",
      "accuracy": 1.0,
      "g_nll": 0.02446400612097932,
      "average_nll": 0.008154668706993107,
      "perplexity": 1.0081880085814454,
      "avg_token_probability": 0.9918966567031196,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_80--166/166_2439219.txt#0_0",
      "dataset_type": "short",
      "response": "angela rippon",
      "accuracy": 1.0,
      "g_nll": 0.03195537137878546,
      "average_nll": 0.006391074275757092,
      "perplexity": 1.0064115407686838,
      "avg_token_probability": 0.9936777655004281,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_6611--38/38_1616659.txt#0_0",
      "dataset_type": "short",
      "response": "brothers",
      "accuracy": 0.0,
      "g_nll": 1.785440057516098,
      "average_nll": 0.892720028758049,
      "perplexity": 2.4417622903661464,
      "avg_token_probability": 0.46455900963871943,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2646--198/198_2188674.txt#0_0",
      "dataset_type": "short",
      "response": "mongols",
      "accuracy": 0.0,
      "g_nll": 0.08813976589590311,
      "average_nll": 0.029379921965301037,
      "perplexity": 1.0298157697935357,
      "avg_token_probability": 0.9712021786354502,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8597--105/105_3212945.txt#0_0",
      "dataset_type": "short",
      "response": "bassoon",
      "accuracy": 1.0,
      "g_nll": 0.4526617612573318,
      "average_nll": 0.15088725375244394,
      "perplexity": 1.1628655419642653,
      "avg_token_probability": 0.8777975826327059,
      "sequence_length": 3
    },
    {
      "example_id": "qg_1091--8/8_3215147.txt#0_2",
      "dataset_type": "short",
      "response": "[Wadsworth laughs evilly] [PAR] Professor Plum :   Oh, shucks. [PAR] Wadsworth :   He was expendable like all of you. I'm grateful to you all for disposing of my network of spies and informers. Saved me a lot of trouble. Now there's no evidence against me. [PAR] Wadsworth : \"Ours is not to reason why, ours is but to do and die.\" [PAR] Professor Plum :   Die? [PAR] Wadsworth :   Merely quoting, sir, from Alfred, Lord Tennyson. [PAR] Colonel Mustard :   Hm, I prefer Kipling, myself. \"The female of the species is",
      "accuracy": 0.0,
      "g_nll": 8.36924359238651,
      "average_nll": 0.05579495728257674,
      "perplexity": 1.0573808532641116,
      "avg_token_probability": 0.9747974535430414,
      "sequence_length": 150
    },
    {
      "example_id": "qw_2021--84/84_31075.txt#0_1",
      "dataset_type": "short",
      "response": "gagarin",
      "accuracy": 1.0,
      "g_nll": 0.19747789948087302,
      "average_nll": 0.049369474870218255,
      "perplexity": 1.0506084524603196,
      "avg_token_probability": 0.9523374609787677,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_3826--197/197_1325980.txt#0_0",
      "dataset_type": "short",
      "response": "dalziel and pascoe",
      "accuracy": 1.0,
      "g_nll": 0.13440728520436096,
      "average_nll": 0.022401214200726827,
      "perplexity": 1.022654005481117,
      "avg_token_probability": 0.9786734400031154,
      "sequence_length": 6
    },
    {
      "example_id": "qb_1253--27/27_299742.txt#0_0",
      "dataset_type": "short",
      "response": "al capone",
      "accuracy": 1.0,
      "g_nll": 0.7263246140419142,
      "average_nll": 0.18158115351047854,
      "perplexity": 1.1991118448968796,
      "avg_token_probability": 0.8474677185718302,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3121--176/176_546993.txt#0_1",
      "dataset_type": "short",
      "response": "netherlands",
      "accuracy": 1.0,
      "g_nll": 0.6327210420859046,
      "average_nll": 0.21090701402863488,
      "perplexity": 1.2347975308751102,
      "avg_token_probability": 0.8391908246639844,
      "sequence_length": 3
    },
    {
      "example_id": "jp_2191--74/74_392765.txt#0_0",
      "dataset_type": "short",
      "response": "edward viii",
      "accuracy": 1.0,
      "g_nll": 0.16422078444156796,
      "average_nll": 0.03284415688831359,
      "perplexity": 1.033389480059493,
      "avg_token_probability": 0.9684216941672691,
      "sequence_length": 5
    },
    {
      "example_id": "qw_5335--185/185_1151387.txt#0_1",
      "dataset_type": "short",
      "response": "charlie chan",
      "accuracy": 1.0,
      "g_nll": 0.0598262237617746,
      "average_nll": 0.01495655594044365,
      "perplexity": 1.0150689649412248,
      "avg_token_probability": 0.9851941744226986,
      "sequence_length": 4
    },
    {
      "example_id": "bt_1833--26/26_2841025.txt#0_0",
      "dataset_type": "short",
      "response": "lancashire",
      "accuracy": 1.0,
      "g_nll": 0.03208479456952773,
      "average_nll": 0.01069493152317591,
      "perplexity": 1.0107523267334435,
      "avg_token_probability": 0.9894386696061007,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_2269--148/148_613645.txt#0",
      "dataset_type": "short",
      "response": "drinking song",
      "accuracy": 1.0,
      "g_nll": 0.09232071647420526,
      "average_nll": 0.03077357215806842,
      "perplexity": 1.0312519732898062,
      "avg_token_probability": 0.9699678841769145,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9374--54/54_2690886.txt#0_2",
      "dataset_type": "short",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.11772603169083595,
      "average_nll": 0.058863015845417976,
      "perplexity": 1.060629941292693,
      "avg_token_probability": 0.9430806195204204,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5117--114/114_226336.txt#0_0",
      "dataset_type": "short",
      "response": "achille lauro",
      "accuracy": 1.0,
      "g_nll": 0.11582173960800901,
      "average_nll": 0.023164347921601804,
      "perplexity": 1.023434725096331,
      "avg_token_probability": 0.9776560382463557,
      "sequence_length": 5
    },
    {
      "example_id": "odql_11538--73/73_1736878.txt#0_0",
      "dataset_type": "short",
      "response": "marshalsea",
      "accuracy": 1.0,
      "g_nll": 0.20216132689211008,
      "average_nll": 0.05054033172302752,
      "perplexity": 1.0518392849902405,
      "avg_token_probability": 0.9534870029037211,
      "sequence_length": 4
    },
    {
      "example_id": "qw_8762--197/197_3207893.txt#0_1",
      "dataset_type": "short",
      "response": "a perpendicular expression of a horizontal desire",
      "accuracy": 1.0,
      "g_nll": 0.30552539229393005,
      "average_nll": 0.30552539229393005,
      "perplexity": 1.3573379506007424,
      "avg_token_probability": 0.7367361971699172,
      "sequence_length": 1
    }
  ]
}