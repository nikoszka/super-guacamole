# ðŸ“ nllSAR File Structure

Complete guide to the file and directory structure of the nllSAR project.

---

## Repository Root

```
nllSAR/
â”œâ”€â”€ src/                           # Source code (main package)
â”œâ”€â”€ config/                        # Configuration files
â”œâ”€â”€ experiments/                   # Experiment scripts
â”œâ”€â”€ results/                       # Analysis outputs
â”œâ”€â”€ scripts/                       # Utility scripts
â”œâ”€â”€ tests/                         # Test suite
â”œâ”€â”€ docs/                          # Additional documentation (optional)
â”œâ”€â”€ .github/                       # GitHub workflows (CI/CD)
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ nllsar.yml                     # Conda environment
â”œâ”€â”€ setup.py                       # Package setup
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â”œâ”€â”€ README.md                      # Project overview
â”œâ”€â”€ CODE_DOCUMENTATION.md          # Comprehensive code documentation
â”œâ”€â”€ API_REFERENCE.md               # API reference
â”œâ”€â”€ ARCHITECTURE_GUIDE.md          # Architecture details
â”œâ”€â”€ DEVELOPER_GUIDE.md             # Developer guide
â”œâ”€â”€ FILE_STRUCTURE.md              # This file
â”œâ”€â”€ QUICK_START.md                 # Quick start guide
â”œâ”€â”€ GENERATION_SETTINGS_GUIDE.md   # Generation parameters guide
â”œâ”€â”€ ANALYSIS_README.md             # Analysis pipeline guide
â”œâ”€â”€ CHANGES_SUMMARY.md             # Recent changes
â””â”€â”€ [other documentation files]    # Domain-specific guides
```

---

## Source Code (`src/`)

### Main Package Structure

```
src/
â”œâ”€â”€ __init__.py                    # Package initialization
â”œâ”€â”€ generate_answers.py            # Main answer generation script
â”œâ”€â”€ compute_uncertainty_measures.py # Uncertainty computation
â”œâ”€â”€ analyze_results.py             # Results analysis utilities
â”œâ”€â”€ models/                        # Model implementations
â”œâ”€â”€ uncertainty_measures/          # Uncertainty metrics
â”œâ”€â”€ data/                          # Data loading utilities
â”œâ”€â”€ analysis/                      # Analysis scripts
â”œâ”€â”€ utils/                         # Helper utilities
â””â”€â”€ analysis_notebooks/            # Jupyter notebooks for analysis
```

---

### Models Module (`src/models/`)

```
src/models/
â”œâ”€â”€ __init__.py                    # Models package init
â”œâ”€â”€ base_model.py                  # Abstract base class for all models
â”‚                                  # - BaseModel (ABC)
â”‚                                  # - STOP_SEQUENCES constants
â”‚                                  # - predict() and get_p_true() interface
â”‚
â””â”€â”€ huggingface_models.py          # HuggingFace model implementation
                                   # - HuggingfaceModel class
                                   # - get_hf_cache_dir()
                                   # - get_gpu_memory_dict()
                                   # - StoppingCriteriaSub class
                                   # - remove_split_layer()
                                   # Supports: Llama, Mistral, Falcon
```

**Key Files:**

#### `base_model.py`
- **Purpose:** Define interface for all language models
- **Key Components:**
  - `BaseModel` abstract class
  - Stop sequence constants
- **Dependencies:** None (abstract)
- **Usage:** Subclass to implement new models

#### `huggingface_models.py`
- **Purpose:** HuggingFace Transformers integration
- **Key Components:**
  - `HuggingfaceModel` class (629 lines)
  - Multi-GPU support
  - Token-based extraction
  - Stop sequence handling
  - Cache management
- **Dependencies:** transformers, torch, accelerate, huggingface_hub
- **Usage:** Primary model class for generation

---

### Uncertainty Measures Module (`src/uncertainty_measures/`)

```
src/uncertainty_measures/
â”œâ”€â”€ __init__.py                    # Package init
â”œâ”€â”€ rw_gnll.py                     # Relevance-Weighted G-NLL
â”‚                                  # - initialize_similarity_model()
â”‚                                  # - compute_similarity()
â”‚                                  # - remove_token_at_position()
â”‚                                  # - compute_token_relevance_weights()
â”‚                                  # - compute_rw_gnll()
â”‚
â”œâ”€â”€ sar.py                         # Shifting Attention to Relevance
â”‚                                  # - compute_sar_for_entry()
â”‚                                  # - compute_sar()
â”‚
â”œâ”€â”€ p_true.py                      # P(True) baseline
â”‚                                  # - get_p_true_from_pred()
â”‚
â”œâ”€â”€ p_ik.py                        # P_IK baseline
â”‚                                  # - compute_p_ik()
â”‚
â””â”€â”€ semantic_entropy.py            # Semantic entropy
                                   # - compute_semantic_entropy()
```

**Key Files:**

#### `rw_gnll.py` (265 lines)
- **Purpose:** Relevance-weighted uncertainty estimation
- **Algorithm:** Weight token log-likelihoods by semantic relevance
- **Key Functions:**
  - `initialize_similarity_model()` - Load cross-encoder
  - `compute_token_relevance_weights()` - Compute R_T(y_t) for each token
  - `compute_rw_gnll()` - Main RW-G-NLL computation
- **Dependencies:** sentence-transformers, numpy
- **Usage:** For single-sample uncertainty with relevance weighting

#### `sar.py` (188 lines)
- **Purpose:** Multi-sample uncertainty with relevance weighting
- **Algorithm:** Average RW-G-NLL across multiple samples
- **Key Functions:**
  - `compute_sar_for_entry()` - Compute SAR across samples
  - `compute_sar()` - Convenience wrapper
- **Dependencies:** rw_gnll, numpy
- **Usage:** For multi-sample uncertainty (requires num_generations > 1)

#### `p_true.py`
- **Purpose:** P(True) baseline uncertainty
- **Algorithm:** Log-probability of generating 'True' answer
- **Usage:** Simple baseline for comparison

#### `semantic_entropy.py`
- **Purpose:** Semantic entropy over multiple samples
- **Algorithm:** Cluster similar responses and compute entropy
- **Usage:** Multi-sample uncertainty based on semantic diversity

---

### Data Module (`src/data/`)

```
src/data/
â”œâ”€â”€ __init__.py                    # Package init
â””â”€â”€ data_utils.py                  # Dataset loading utilities
                                   # - load_ds() - Main loading function
                                   # Supports:
                                   #   - squad (SQuAD v2.0)
                                   #   - trivia_qa (TriviaQA)
                                   #   - svamp (Math problems)
                                   #   - nq (Natural Questions)
                                   #   - bioasq (BioASQ)
```

**Key Files:**

#### `data_utils.py` (104 lines)
- **Purpose:** Load and normalize datasets
- **Key Function:** `load_ds(dataset_name, seed, add_options)`
- **Output Format:**
  ```python
  {
      'question': str,
      'answers': {'text': List[str]},
      'context': str,
      'id': str
  }
  ```
- **Dependencies:** datasets, json
- **Usage:** Primary data loading interface

---

### Analysis Module (`src/analysis/`)

```
src/analysis/
â”œâ”€â”€ __init__.py                         # Package init
â”œâ”€â”€ phase1_baseline_metrics.py          # Baseline statistics
â”‚                                       # - Accuracy, NLL, token stats
â”‚                                       # Output: JSON, CSV
â”‚
â”œâ”€â”€ phase1_5_token_nll_analysis.py      # Token-level NLL analysis
â”‚                                       # - NLL by position
â”‚                                       # - NLL distributions
â”‚                                       # Output: PNG plots, JSON examples
â”‚
â”œâ”€â”€ phase1_6_prefix_nll_analysis.py     # Prefix-based NLL
â”‚                                       # - Early stopping analysis
â”‚                                       # - AUROC by prefix length
â”‚                                       # Output: AUROC curves
â”‚
â”œâ”€â”€ phase2_token_importance.py          # Token relevance analysis
â”‚                                       # - Relevance weights
â”‚                                       # - RW-G-NLL computation
â”‚                                       # Output: Relevance plots, JSON
â”‚
â”œâ”€â”€ phase5_comparative_analysis.py      # AUROC comparison
â”‚                                       # - Compare all uncertainty metrics
â”‚                                       # - ROC curves
â”‚                                       # Output: CSV, PNG
â”‚
â””â”€â”€ token_visualization_app.py          # Streamlit visualization app
                                        # - Interactive token highlighting
                                        # - Load JSON from analysis phases
```

**Key Files:**

#### `phase1_baseline_metrics.py`
- **Purpose:** Compute baseline metrics
- **Outputs:**
  - `baseline_metrics.json` - Accuracy, NLL stats
  - `token_statistics.json` - Token count distributions
  - `baseline_metrics.csv` - CSV export
- **Usage:** First step in analysis pipeline

#### `phase1_5_token_nll_analysis.py` (186 lines)
- **Purpose:** Detailed token-level NLL analysis
- **Outputs:**
  - `nll_vs_position.png` - NLL by token position
  - `nll_distribution.png` - Distribution histograms
  - `position_nll_heatmap.png` - Heatmap visualization
  - `sentence_level_nll_examples.json` - Detailed examples (for Streamlit)
  - `example_{01-10}_token_nlls.png` - Individual plots
- **Usage:** Understand token-level patterns

#### `phase2_token_importance.py`
- **Purpose:** Compute token relevance weights
- **Algorithm:** SAR-style relevance computation
- **Outputs:**
  - `relevance_vs_position.png` - Relevance by position
  - `token_importance_examples.json` - Examples with weights (for Streamlit)
- **Usage:** Analyze which tokens are most relevant

#### `phase5_comparative_analysis.py`
- **Purpose:** Compare AUROC of all uncertainty metrics
- **Metrics Compared:**
  - G-NLL (baseline)
  - RW-G-NLL
  - SAR (if multi-sample)
  - Semantic Entropy (if multi-sample)
  - Length (baseline)
- **Outputs:**
  - `auroc_comparison.csv` - AUROC scores
  - `roc_curves.png` - ROC curve plots
  - `cost_performance_plot.png` - Cost-benefit analysis
- **Usage:** Final analysis step

#### `token_visualization_app.py`
- **Purpose:** Interactive visualization with Streamlit
- **Features:**
  - Load JSON from phase 1.5 or phase 2
  - Interactive token highlighting
  - Color-coded by NLL or relevance
- **Usage:** `streamlit run src/analysis/token_visualization_app.py`

---

### Utilities Module (`src/utils/`)

```
src/utils/
â”œâ”€â”€ __init__.py                    # Package init
â”œâ”€â”€ utils.py                       # General utilities
â”‚                                  # - setup_logger()
â”‚                                  # - init_model()
â”‚                                  # - get_metric()
â”‚                                  # - construct_fewshot_prompt_from_indices()
â”‚                                  # - get_make_prompt()
â”‚                                  # - split_dataset()
â”‚                                  # - BRIEF_PROMPTS constant
â”‚
â”œâ”€â”€ eval_utils.py                  # Evaluation utilities
â”‚                                  # - compute_rouge_scores()
â”‚                                  # - compute_squad_metrics()
â”‚                                  # - llm_judge_evaluate()
â”‚
â””â”€â”€ openai.py                      # OpenAI API utilities
                                   # - call_openai_api()
                                   # - parse_llm_judge_response()
```

**Key Files:**

#### `utils.py`
- **Purpose:** General utility functions
- **Key Functions:**
  - Model initialization
  - Prompt construction
  - Dataset splitting
  - Logging setup
- **Usage:** Used throughout codebase

#### `eval_utils.py`
- **Purpose:** Answer evaluation metrics
- **Key Functions:**
  - ROUGE-L computation
  - SQuAD F1 and EM
  - LLM-as-a-judge
- **Usage:** Evaluate answer correctness

#### `openai.py`
- **Purpose:** OpenAI API integration
- **Key Functions:**
  - API calls with retry logic
  - Response parsing
- **Usage:** For LLM judge evaluation

---

### Analysis Notebooks (`src/analysis_notebooks/`)

```
src/analysis_notebooks/
â”œâ”€â”€ first_analysis_uncertainties.ipynb   # Initial uncertainty analysis
â”œâ”€â”€ gnll_baseline_analysis.ipynb         # G-NLL baseline exploration
â”œâ”€â”€ gnll_baseline_analysis_results.json  # Cached results
â”œâ”€â”€ analysis_llm_as_judge.ipynb          # LLM judge analysis
â”œâ”€â”€ llm_judge_results.pkl                # LLM judge cached results
â””â”€â”€ llm_judge_summary.json               # LLM judge summary
```

**Purpose:** Exploratory analysis and visualization in Jupyter notebooks.

**Key Notebooks:**

- `first_analysis_uncertainties.ipynb` - Initial exploration of uncertainty metrics
- `gnll_baseline_analysis.ipynb` - Comprehensive G-NLL analysis with plots
- `analysis_llm_as_judge.ipynb` - Analyze LLM judge performance

---

### Main Scripts (`src/`)

#### `generate_answers.py` (302+ lines)
- **Purpose:** Main answer generation pipeline
- **Key Function:** `main(args)`
- **Pipeline:**
  1. Load dataset
  2. Construct few-shot prompt
  3. Initialize model
  4. Generate answers with token info
  5. Evaluate answers
  6. Save to pickle
  7. Upload to Wandb
- **Output:** `validation_generations.pkl`
- **Usage:** Primary entry point for generation

#### `compute_uncertainty_measures.py`
- **Purpose:** Compute uncertainty measures from pickle
- **Key Function:** `main(args)`
- **Supported Metrics:**
  - G-NLL
  - RW-G-NLL
  - SAR
  - Semantic Entropy
  - P(True)
- **Usage:** Post-generation uncertainty computation

#### `analyze_results.py`
- **Purpose:** General results analysis utilities
- **Functions:** Helper functions for analysis scripts
- **Usage:** Imported by analysis scripts

---

## Top-Level Scripts

```
â”œâ”€â”€ run_generate_short_answers.py       # Short answer generation wrapper
â”œâ”€â”€ run_generate_long_answers.py        # Long answer generation wrapper
â”œâ”€â”€ run_gnll_baseline.py                # Full G-NLL baseline pipeline
â”œâ”€â”€ run_greedy_decoding.py              # Greedy decoding experiments
â”œâ”€â”€ run_greedy_decoding_short.py        # Short answer greedy decoding
â”œâ”€â”€ generate_gnll_answers.py            # G-NLL answer generation
â”œâ”€â”€ compute_gnll_auroc.py               # Standalone AUROC computation
â””â”€â”€ recompute_accuracy_with_judge.py    # Recompute with LLM judge
```

**Key Scripts:**

#### `run_generate_short_answers.py`
- **Purpose:** Convenient wrapper for short answer generation
- **Default Settings:**
  - `max_new_tokens=50`
  - `brief_prompt=short`
  - `metric=squad`
- **Usage:** Quick short answer experiments

#### `run_generate_long_answers.py`
- **Purpose:** Convenient wrapper for long answer generation
- **Default Settings:**
  - `max_new_tokens=200`
  - `brief_prompt=detailed`
  - `metric=llm_llama-3.1-70b`
- **Usage:** Quick long answer experiments

#### `run_gnll_baseline.py`
- **Purpose:** Full pipeline for G-NLL baseline
- **Steps:**
  1. Generate answers (short and long)
  2. Evaluate with LLM judge
  3. Compute G-NLL and RW-G-NLL
  4. Calculate AUROC
  5. Generate plots
- **Usage:** Complete G-NLL baseline evaluation

#### `compute_gnll_auroc.py`
- **Purpose:** Standalone AUROC computation from existing pickle
- **Usage:**
  ```bash
  python compute_gnll_auroc.py \
    --pickle_path path/to/pickle.pkl \
    --use_rw_gnll
  ```

---

## Configuration (`config/`)

```
config/
â”œâ”€â”€ default_config.yaml            # Default configuration
â”œâ”€â”€ model_configs/                 # Model-specific configs
â”‚   â”œâ”€â”€ llama_config.yaml
â”‚   â”œâ”€â”€ mistral_config.yaml
â”‚   â””â”€â”€ falcon_config.yaml
â””â”€â”€ dataset_configs/               # Dataset-specific configs
    â”œâ”€â”€ squad_config.yaml
    â”œâ”€â”€ trivia_qa_config.yaml
    â””â”€â”€ bioasq_config.yaml
```

**Purpose:** YAML configuration files for reproducible experiments.

**Usage:**
```bash
python -m src.generate_answers --config config/llama_config.yaml
```

---

## Experiments (`experiments/`)

```
experiments/
â”œâ”€â”€ experiment_001_baseline.sh     # Baseline experiments
â”œâ”€â”€ experiment_002_rw_gnll.sh      # RW-G-NLL experiments
â”œâ”€â”€ experiment_003_sar.sh          # SAR experiments
â””â”€â”€ [more experiment scripts]
```

**Purpose:** Shell scripts for running full experiments.

**Example:**
```bash
#!/bin/bash
# experiment_001_baseline.sh

# Generate short answers
python -m src.generate_answers \
  --model_name Llama-3.2-1B \
  --dataset trivia_qa \
  --num_samples 200 \
  --temperature 0.0 \
  --model_max_new_tokens 50 \
  --brief_prompt short \
  --metric squad \
  --project baseline_short

# Analyze results
python -m src.analysis.phase1_baseline_metrics \
  --short-pickle path/to/pickle.pkl \
  --output-dir results/baseline_short
```

---

## Results (`results/`)

```
results/
â”œâ”€â”€ phase1_short/                  # Phase 1 short answer results
â”‚   â”œâ”€â”€ baseline_metrics.json
â”‚   â”œâ”€â”€ token_statistics.json
â”‚   â””â”€â”€ baseline_metrics_short.csv
â”‚
â”œâ”€â”€ phase1_long/                   # Phase 1 long answer results
â”‚   â”œâ”€â”€ baseline_metrics.json
â”‚   â”œâ”€â”€ token_statistics.json
â”‚   â””â”€â”€ baseline_metrics_long.csv
â”‚
â”œâ”€â”€ phase1_5_short/                # Phase 1.5 short answer results
â”‚   â”œâ”€â”€ nll_vs_position.png
â”‚   â”œâ”€â”€ nll_distribution.png
â”‚   â”œâ”€â”€ position_nll_heatmap.png
â”‚   â”œâ”€â”€ sentence_level_nll_examples.json
â”‚   â”œâ”€â”€ token_nll_results.json
â”‚   â””â”€â”€ example_{01-10}_token_nlls.png
â”‚
â”œâ”€â”€ phase1_5_long/                 # Phase 1.5 long answer results
â”‚   â””â”€â”€ [same structure as phase1_5_short]
â”‚
â”œâ”€â”€ phase2_short/                  # Phase 2 short answer results
â”‚   â”œâ”€â”€ relevance_vs_position.png
â”‚   â””â”€â”€ token_importance_examples.json
â”‚
â”œâ”€â”€ phase2_long/                   # Phase 2 long answer results
â”‚   â””â”€â”€ [same structure as phase2_short]
â”‚
â””â”€â”€ phase5_long/                   # Phase 5 comparison results
    â”œâ”€â”€ auroc_comparison.csv
    â”œâ”€â”€ roc_curves.png
    â”œâ”€â”€ cost_performance_plot.png
    â””â”€â”€ analysis_summary.json
```

**Purpose:** Analysis outputs organized by phase and answer type.

---

## Scripts (`scripts/`)

```
scripts/
â”œâ”€â”€ setup_environment.sh           # Environment setup script
â”œâ”€â”€ download_models.sh             # Download model weights
â”œâ”€â”€ clean_cache.sh                 # Clean model cache
â”œâ”€â”€ run_tests.sh                   # Run test suite
â”œâ”€â”€ format_code.sh                 # Run code formatters
â””â”€â”€ setup_hf_token.ps1             # HuggingFace token setup (Windows)
```

**Purpose:** Utility scripts for environment setup and maintenance.

---

## Tests (`tests/`)

```
tests/
â”œâ”€â”€ __init__.py                    # Test package init
â”œâ”€â”€ conftest.py                    # Pytest configuration and fixtures
â”œâ”€â”€ test_models.py                 # Model tests
â”œâ”€â”€ test_uncertainty_measures.py   # Uncertainty metric tests
â”œâ”€â”€ test_data_utils.py             # Data loading tests
â”œâ”€â”€ test_analysis.py               # Analysis script tests
â”œâ”€â”€ test_integration.py            # End-to-end integration tests
â””â”€â”€ fixtures/                      # Test data and fixtures
    â”œâ”€â”€ sample_pickle.pkl          # Sample generation results
    â”œâ”€â”€ sample_dataset.json        # Sample dataset
    â””â”€â”€ sample_config.yaml         # Sample configuration
```

**Purpose:** Comprehensive test suite for all modules.

**Usage:**
```bash
# Run all tests
pytest tests/ -v

# Run specific test file
pytest tests/test_models.py -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

---

## Documentation Files

### Core Documentation

```
â”œâ”€â”€ README.md                      # Project overview and quick start
â”œâ”€â”€ CODE_DOCUMENTATION.md          # Comprehensive code documentation
â”œâ”€â”€ API_REFERENCE.md               # Complete API reference
â”œâ”€â”€ ARCHITECTURE_GUIDE.md          # Architecture and design patterns
â”œâ”€â”€ DEVELOPER_GUIDE.md             # Developer guide
â””â”€â”€ FILE_STRUCTURE.md              # This file
```

### User Guides

```
â”œâ”€â”€ QUICK_START.md                 # Quick start guide
â”œâ”€â”€ GENERATION_SETTINGS_GUIDE.md   # Generation parameter guide
â”œâ”€â”€ ANALYSIS_README.md             # Analysis pipeline guide
â”œâ”€â”€ ENVIRONMENT_SETUP.md           # Environment setup
â”œâ”€â”€ GPU_REQUIREMENTS.md            # GPU setup guide
â”œâ”€â”€ MULTI_GPU_GUIDE.md             # Multi-GPU usage
â””â”€â”€ MODEL_CACHE_GUIDE.md           # Model cache management
```

### Domain-Specific Guides

```
â”œâ”€â”€ GNLL_BASELINE_README.md        # G-NLL baseline guide
â”œâ”€â”€ GREEDY_DECODING_README.md      # Greedy decoding guide
â”œâ”€â”€ CLOUD_PLATFORMS.md             # Cloud platform setup
â””â”€â”€ IMPORTANT_CLARIFICATIONS.md    # Important clarifications
```

### Change Logs

```
â”œâ”€â”€ CHANGES_SUMMARY.md             # Recent changes summary
â”œâ”€â”€ ALIGNMENT_FIX_SUMMARY.md       # Token alignment fix
â”œâ”€â”€ ANSWER_EXTRACTION_FIX.md       # Answer extraction fix
â”œâ”€â”€ QUICK_FIX_SUMMARY.md           # Quick fixes
â”œâ”€â”€ SESSION_FIXES_SUMMARY.md       # Session-specific fixes
â””â”€â”€ STOP_SEQUENCE_FIX.md           # Stop sequence handling fix
```

---

## Hidden Files and Directories

```
â”œâ”€â”€ .gitignore                     # Git ignore patterns
â”œâ”€â”€ .github/                       # GitHub-specific files
â”‚   â”œâ”€â”€ workflows/                 # CI/CD workflows
â”‚   â”‚   â”œâ”€â”€ tests.yml              # Run tests on push
â”‚   â”‚   â”œâ”€â”€ lint.yml               # Linting checks
â”‚   â”‚   â””â”€â”€ docs.yml               # Documentation building
â”‚   â”œâ”€â”€ ISSUE_TEMPLATE.md          # Issue template
â”‚   â””â”€â”€ PULL_REQUEST_TEMPLATE.md   # PR template
â”‚
â”œâ”€â”€ .vscode/                       # VS Code settings
â”‚   â”œâ”€â”€ settings.json              # Editor settings
â”‚   â”œâ”€â”€ launch.json                # Debug configurations
â”‚   â””â”€â”€ extensions.json            # Recommended extensions
â”‚
â”œâ”€â”€ .env                           # Environment variables (gitignored)
â””â”€â”€ .pytest_cache/                 # Pytest cache (gitignored)
```

---

## Generated/Cached Directories (Gitignored)

```
â”œâ”€â”€ __pycache__/                   # Python bytecode cache
â”œâ”€â”€ .pytest_cache/                 # Pytest cache
â”œâ”€â”€ .ipynb_checkpoints/            # Jupyter notebook checkpoints
â”œâ”€â”€ wandb/                         # Weights & Biases logs
â”œâ”€â”€ .cache/                        # General cache
â”œâ”€â”€ models/                        # Downloaded model weights
â””â”€â”€ htmlcov/                       # Coverage reports
```

---

## Data Directories

```
src/boldis/uncertainty/wandb/      # User 'boldis' wandb runs
src/nikos/uncertainty/wandb/       # User 'nikos' wandb runs
  â””â”€â”€ run-<timestamp>-<id>/        # Individual run directory
      â”œâ”€â”€ files/
      â”‚   â”œâ”€â”€ validation_generations.pkl    # Main output
      â”‚   â”œâ”€â”€ train_generations.pkl
      â”‚   â”œâ”€â”€ experiment_details.pkl
      â”‚   â”œâ”€â”€ config.yaml
      â”‚   â””â”€â”€ wandb-summary.json
      â””â”€â”€ logs/
          â””â”€â”€ debug.log
```

**Purpose:** Weights & Biases experiment tracking data.

**Usage:** After generation, find pickle files here.

---

## File Naming Conventions

### Python Files
- **Modules:** lowercase with underscores (`data_utils.py`)
- **Classes:** PascalCase (`HuggingfaceModel`)
- **Functions:** lowercase with underscores (`compute_rw_gnll()`)
- **Constants:** UPPERCASE with underscores (`STOP_SEQUENCES`)

### Output Files
- **Metrics:** `*_metrics.json`, `*_metrics.csv`
- **Plots:** `*_vs_*.png`, `*_distribution.png`, `*_heatmap.png`
- **Examples:** `*_examples.json`, `example_01_*.png`
- **Results:** `*_results.json`, `*_analysis.json`

### Documentation Files
- **Guides:** `*_GUIDE.md`, `*_README.md`
- **Summaries:** `*_SUMMARY.md`
- **References:** `*_REFERENCE.md`

---

## File Size Expectations

### Code Files
- Small: < 200 lines (utilities, simple classes)
- Medium: 200-600 lines (complex classes, analysis scripts)
- Large: > 600 lines (main generation script, comprehensive models)

### Data Files
- Pickles: 10MB - 500MB (depending on num_samples)
- JSON: 1KB - 50MB (analysis results)
- PNG: 50KB - 5MB (plots)

### Model Files (in cache)
- 1B models: ~5GB
- 7B models: ~15GB
- 13B models: ~25GB
- 70B models: ~140GB

---

## Important Files to NOT Modify Directly

```
# Auto-generated files
wandb/                             # Managed by Weights & Biases
__pycache__/                       # Python bytecode
*.pyc                              # Compiled Python
.pytest_cache/                     # Pytest cache

# Downloaded models
models/                            # Model weights

# User-specific
.env                               # Environment variables
.vscode/                           # Editor settings (can be shared but user-specific)
```

---

## Quick Reference: Where to Find Things

### "I want to..."

**...generate answers:**
- Script: `src/generate_answers.py`
- Wrappers: `run_generate_short_answers.py`, `run_generate_long_answers.py`

**...compute uncertainty metrics:**
- RW-G-NLL: `src/uncertainty_measures/rw_gnll.py`
- SAR: `src/uncertainty_measures/sar.py`
- Compute from pickle: `src/compute_uncertainty_measures.py`

**...analyze results:**
- Phase 1: `src/analysis/phase1_baseline_metrics.py`
- Phase 1.5: `src/analysis/phase1_5_token_nll_analysis.py`
- Phase 2: `src/analysis/phase2_token_importance.py`
- Phase 5: `src/analysis/phase5_comparative_analysis.py`

**...visualize results:**
- Streamlit app: `src/analysis/token_visualization_app.py`
- Jupyter notebooks: `src/analysis_notebooks/`

**...add a new model:**
- Create: `src/models/my_model.py`
- Register: `src/utils/utils.py` (in `init_model()`)
- Test: `tests/test_my_model.py`

**...add a new dataset:**
- Implement: `src/data/data_utils.py` (in `load_ds()`)
- Test: `tests/test_data_utils.py`

**...add a new uncertainty metric:**
- Create: `src/uncertainty_measures/my_metric.py`
- Add to analysis: `src/analysis/phase5_comparative_analysis.py`
- Test: `tests/test_my_metric.py`

**...understand the architecture:**
- Read: `ARCHITECTURE_GUIDE.md`
- API details: `API_REFERENCE.md`
- Code overview: `CODE_DOCUMENTATION.md`

**...contribute to the project:**
- Read: `DEVELOPER_GUIDE.md`
- Submit PR: See `.github/PULL_REQUEST_TEMPLATE.md`

---

## File Creation Timeline

### Initial Setup (v1.0)
1. Core models (`base_model.py`, `huggingface_models.py`)
2. Data loading (`data_utils.py`)
3. Basic generation (`generate_answers.py`)
4. Uncertainty measures (`p_true.py`, `p_ik.py`)

### Token Alignment Update (v1.5)
1. Enhanced `huggingface_models.py` (token_ids, tokens storage)
2. Updated analysis scripts to use stored tokens
3. Documentation updates (`CHANGES_SUMMARY.md`, `ANSWER_EXTRACTION_FIX.md`)

### RW-G-NLL Addition (v1.8)
1. `rw_gnll.py` implementation
2. `phase2_token_importance.py` analysis
3. `token_visualization_app.py` Streamlit app

### SAR Implementation (v1.9)
1. `sar.py` implementation
2. Multi-sample support in generation
3. Updated Phase 5 analysis

### Comprehensive Documentation (v2.0)
1. `CODE_DOCUMENTATION.md`
2. `API_REFERENCE.md`
3. `ARCHITECTURE_GUIDE.md`
4. `DEVELOPER_GUIDE.md`
5. `FILE_STRUCTURE.md` (this file)

---

**Last Updated:** November 2025
**Version:** 2.0


