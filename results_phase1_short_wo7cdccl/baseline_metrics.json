{
  "short": [
    {
      "example_id": "qg_1091--8/8_3215147.txt#0_2",
      "dataset_type": "short",
      "response": "mr. green, mr. plum, miss scarlett, miss white",
      "accuracy": 0.0,
      "g_nll": 9.955717833803646,
      "average_nll": 0.7111227024145462,
      "perplexity": 2.036276107906893,
      "avg_token_probability": 0.6704612596434055,
      "sequence_length": 14
    },
    {
      "example_id": "wh_1864--103/103_769441.txt#0_0",
      "dataset_type": "short",
      "response": "ringway",
      "accuracy": 1.0,
      "g_nll": 0.009562393126543611,
      "average_nll": 0.004781196563271806,
      "perplexity": 1.0047926447215867,
      "avg_token_probability": 0.9952399422824882,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3826--197/197_1325980.txt#0_0",
      "dataset_type": "short",
      "response": "dalziel and pascoe",
      "accuracy": 1.0,
      "g_nll": 0.09864233591360971,
      "average_nll": 0.01972846718272194,
      "perplexity": 1.0199243594890621,
      "avg_token_probability": 0.9811584007469358,
      "sequence_length": 5
    },
    {
      "example_id": "qb_1253--27/27_299742.txt#0_0",
      "dataset_type": "short",
      "response": "al-capone",
      "accuracy": 0.0,
      "g_nll": 8.508776977647358,
      "average_nll": 2.8362589925491193,
      "perplexity": 17.051854950271164,
      "avg_token_probability": 0.5942305745998094,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3121--176/176_546993.txt#0_1",
      "dataset_type": "short",
      "response": "netherlands",
      "accuracy": 1.0,
      "g_nll": 0.5893439570209011,
      "average_nll": 0.29467197851045057,
      "perplexity": 1.3426858566284676,
      "avg_token_probability": 0.7771136450881106,
      "sequence_length": 2
    },
    {
      "example_id": "bb_7851--165/165_1006156.txt#0_0",
      "dataset_type": "short",
      "response": "1967",
      "accuracy": 1.0,
      "g_nll": 0.00011789557174779475,
      "average_nll": 3.929852391593158e-05,
      "perplexity": 1.0000392992961131,
      "avg_token_probability": 0.9999607022807829,
      "sequence_length": 3
    },
    {
      "example_id": "qw_5335--185/185_1151387.txt#0_1",
      "dataset_type": "short",
      "response": "charlie chan",
      "accuracy": 1.0,
      "g_nll": 0.04073321266332641,
      "average_nll": 0.013577737554442137,
      "perplexity": 1.0136703336401638,
      "avg_token_probability": 0.9865882780997538,
      "sequence_length": 3
    },
    {
      "example_id": "odql_13192--6/6_701893.txt#0_1",
      "dataset_type": "short",
      "response": "gettysburg",
      "accuracy": 1.0,
      "g_nll": 0.06448356448933623,
      "average_nll": 0.016120891122334058,
      "perplexity": 1.016251533768798,
      "avg_token_probability": 0.9843864887328615,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_20420--125/125_1922602.txt#0_1",
      "dataset_type": "short",
      "response": "wisconsin",
      "accuracy": 1.0,
      "g_nll": 0.00810222355357837,
      "average_nll": 0.004051111776789185,
      "perplexity": 1.0040593286221426,
      "avg_token_probability": 0.9959651099352412,
      "sequence_length": 2
    },
    {
      "example_id": "bb_9374--54/54_2690886.txt#0_2",
      "dataset_type": "short",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.10851778835058212,
      "average_nll": 0.10851778835058212,
      "perplexity": 1.1146247356975605,
      "avg_token_probability": 0.8971629356261993,
      "sequence_length": 1
    },
    {
      "example_id": "odql_11679--16/16_33164.txt#0_0",
      "dataset_type": "short",
      "response": "tasmania",
      "accuracy": 1.0,
      "g_nll": 0.2483789010293549,
      "average_nll": 0.12418945051467745,
      "perplexity": 1.1322303523047144,
      "avg_token_probability": 0.8900121719732498,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11538--73/73_1736878.txt#0_0",
      "dataset_type": "short",
      "response": "marshalsea",
      "accuracy": 1.0,
      "g_nll": 0.02830949977715136,
      "average_nll": 0.00943649992571712,
      "perplexity": 1.009481164071663,
      "avg_token_probability": 0.9906956778858363,
      "sequence_length": 3
    },
    {
      "example_id": "qw_8762--197/197_3207893.txt#0_1",
      "dataset_type": "short",
      "response": "a horizontal desire",
      "accuracy": 1.0,
      "g_nll": 6.200308933854103,
      "average_nll": 1.5500772334635258,
      "perplexity": 4.711834079803581,
      "avg_token_probability": 0.5860636691269702,
      "sequence_length": 4
    },
    {
      "example_id": "bb_439--197/197_197490.txt#0_0",
      "dataset_type": "short",
      "response": "general belgrano",
      "accuracy": 1.0,
      "g_nll": 0.650123893195989,
      "average_nll": 0.16253097329899724,
      "perplexity": 1.1764847574985284,
      "avg_token_probability": 0.879969109204743,
      "sequence_length": 4
    },
    {
      "example_id": "bb_7724--184/184_1002731.txt#0_0",
      "dataset_type": "short",
      "response": "albatross",
      "accuracy": 0.0,
      "g_nll": 0.0562746445632456,
      "average_nll": 0.0187582148544152,
      "perplexity": 1.0189352554224274,
      "avg_token_probability": 0.9817592152837312,
      "sequence_length": 3
    },
    {
      "example_id": "qw_439--187/187_1058077.txt#0_2",
      "dataset_type": "short",
      "response": "narcolepsy",
      "accuracy": 1.0,
      "g_nll": 0.0026625209084158996,
      "average_nll": 0.0008875069694719665,
      "perplexity": 1.0008879009203184,
      "avg_token_probability": 0.9991134910502879,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9384--195/195_1224988.txt#0_1",
      "dataset_type": "short",
      "response": "madagascar",
      "accuracy": 1.0,
      "g_nll": 0.002765143130091019,
      "average_nll": 0.0013825715650455095,
      "perplexity": 1.0013835277577292,
      "avg_token_probability": 0.9986192310940427,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_21423--144/144_2788272.txt#0_0",
      "dataset_type": "short",
      "response": "byward tower",
      "accuracy": 0.0,
      "g_nll": 0.16565692331641912,
      "average_nll": 0.05521897443880638,
      "perplexity": 1.0567719953959762,
      "avg_token_probability": 0.9480407672255522,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_25451--37/37_2031222.txt#0_1",
      "dataset_type": "short",
      "response": "dark rum",
      "accuracy": 1.0,
      "g_nll": 0.23216473730280995,
      "average_nll": 0.11608236865140498,
      "perplexity": 1.1230883755984162,
      "avg_token_probability": 0.896219864696142,
      "sequence_length": 2
    },
    {
      "example_id": "qf_545--48/48_2849118.txt#0_1",
      "dataset_type": "short",
      "response": "cairngorms",
      "accuracy": 1.0,
      "g_nll": 0.06904947610379963,
      "average_nll": 0.013809895220759927,
      "perplexity": 1.0139056922983385,
      "avg_token_probability": 0.986639407361538,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_668--20/20_1363097.txt#0_1",
      "dataset_type": "short",
      "response": "demi moore",
      "accuracy": 1.0,
      "g_nll": 1.3003440075550543,
      "average_nll": 0.4334480025183514,
      "perplexity": 1.542567139885704,
      "avg_token_probability": 0.7560821974085131,
      "sequence_length": 3
    },
    {
      "example_id": "qw_8999--56/56_1218189.txt#0_2",
      "dataset_type": "short",
      "response": "(you're the) devil in disguise",
      "accuracy": 0.0,
      "g_nll": 2.542369122398668,
      "average_nll": 0.3177961402998335,
      "perplexity": 1.3740961098517894,
      "avg_token_probability": 0.8822766835015922,
      "sequence_length": 8
    },
    {
      "example_id": "sfq_1377--91/91_1497797.txt#0_1",
      "dataset_type": "short",
      "response": "yarrow",
      "accuracy": 0.0,
      "g_nll": 0.054452978074493785,
      "average_nll": 0.027226489037246893,
      "perplexity": 1.0276005166606141,
      "avg_token_probability": 0.9735015093820655,
      "sequence_length": 2
    },
    {
      "example_id": "wh_4269--173/173_235742.txt#0_0",
      "dataset_type": "short",
      "response": "abraham lincoln",
      "accuracy": 1.0,
      "g_nll": 0.010671721637663723,
      "average_nll": 0.0026679304094159306,
      "perplexity": 1.0026714925028521,
      "avg_token_probability": 0.9973453623072974,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_15401--88/88_1813633.txt#0_1",
      "dataset_type": "short",
      "response": "Xenophon",
      "accuracy": 1.0,
      "g_nll": 0.05205560027388856,
      "average_nll": 0.02602780013694428,
      "perplexity": 1.0263694812889674,
      "avg_token_probability": 0.9746230206468401,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3917--169/169_1555303.txt#0_0",
      "dataset_type": "short",
      "response": "apollo",
      "accuracy": 1.0,
      "g_nll": 0.15365971335268114,
      "average_nll": 0.07682985667634057,
      "perplexity": 1.0798583300760787,
      "avg_token_probability": 0.9287684727900349,
      "sequence_length": 2
    },
    {
      "example_id": "wh_2414--73/73_48566.txt#0_0",
      "dataset_type": "short",
      "response": "Tiny Tim",
      "accuracy": 1.0,
      "g_nll": 2.947632819414139,
      "average_nll": 1.4738164097070694,
      "perplexity": 4.365865317872675,
      "avg_token_probability": 0.4038886021955225,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_11783--191/191_1732943.txt#0_1",
      "dataset_type": "short",
      "response": "chess",
      "accuracy": 1.0,
      "g_nll": 0.12315324693918228,
      "average_nll": 0.12315324693918228,
      "perplexity": 1.1310577388034446,
      "avg_token_probability": 0.8841281622438729,
      "sequence_length": 1
    },
    {
      "example_id": "qg_2887--Lacrosse.txt#0_0",
      "dataset_type": "short",
      "response": "lacrosse stick",
      "accuracy": 1.0,
      "g_nll": 0.3372591880252003,
      "average_nll": 0.11241972934173343,
      "perplexity": 1.1189824318512769,
      "avg_token_probability": 0.9018682119447079,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_16267--E._L._James.txt#0_2",
      "dataset_type": "short",
      "response": "grey",
      "accuracy": 0.0,
      "g_nll": 0.3573966920375824,
      "average_nll": 0.3573966920375824,
      "perplexity": 1.4296028694595466,
      "avg_token_probability": 0.6994949586090607,
      "sequence_length": 1
    },
    {
      "example_id": "jp_2940--195/195_1424532.txt#0_1",
      "dataset_type": "short",
      "response": "giant",
      "accuracy": 0.0,
      "g_nll": 0.25166556239128113,
      "average_nll": 0.25166556239128113,
      "perplexity": 1.2861658231266508,
      "avg_token_probability": 0.7775047214122158,
      "sequence_length": 1
    },
    {
      "example_id": "qb_5499--156/156_419027.txt#0_0",
      "dataset_type": "short",
      "response": "jennifer lopos",
      "accuracy": 1.0,
      "g_nll": 5.723758948282921,
      "average_nll": 1.1447517896565842,
      "perplexity": 3.141661467183165,
      "avg_token_probability": 0.6019600847744464,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_9644--123/123_186355.txt#0_2",
      "dataset_type": "short",
      "response": "iron",
      "accuracy": 1.0,
      "g_nll": 0.0013740155845880508,
      "average_nll": 0.0013740155845880508,
      "perplexity": 1.0013749599764883,
      "avg_token_probability": 0.9986269279426354,
      "sequence_length": 1
    },
    {
      "example_id": "qb_637--10/10_282457.txt#0_0",
      "dataset_type": "short",
      "response": "tiffany",
      "accuracy": 1.0,
      "g_nll": 0.028558790028910153,
      "average_nll": 0.014279395014455076,
      "perplexity": 1.014381832576834,
      "avg_token_probability": 0.9859215828413761,
      "sequence_length": 2
    },
    {
      "example_id": "qw_11443--84/84_2975478.txt#0_0",
      "dataset_type": "short",
      "response": "peach",
      "accuracy": 0.0,
      "g_nll": 4.34289401944261,
      "average_nll": 2.171447009721305,
      "perplexity": 8.77096653652416,
      "avg_token_probability": 0.5062058669673694,
      "sequence_length": 2
    },
    {
      "example_id": "jp_742--32/32_1368333.txt#0_2",
      "dataset_type": "short",
      "response": "allen",
      "accuracy": 0.0,
      "g_nll": 4.812769412994385,
      "average_nll": 4.812769412994385,
      "perplexity": 123.07198314259182,
      "avg_token_probability": 0.008125326125942044,
      "sequence_length": 1
    },
    {
      "example_id": "wh_3212--72/72_800524.txt#0_2",
      "dataset_type": "short",
      "response": "the left book club",
      "accuracy": 1.0,
      "g_nll": 1.0912114157690667,
      "average_nll": 0.27280285394226667,
      "perplexity": 1.313641240102936,
      "avg_token_probability": 0.8338960200267175,
      "sequence_length": 4
    },
    {
      "example_id": "qb_1502--134/134_2618044.txt#0_0",
      "dataset_type": "short",
      "response": "bats",
      "accuracy": 1.0,
      "g_nll": 0.03005516342818737,
      "average_nll": 0.03005516342818737,
      "perplexity": 1.0305113789260683,
      "avg_token_probability": 0.9703920019225161,
      "sequence_length": 1
    },
    {
      "example_id": "qb_2774--81/81_303935.txt#0_0",
      "dataset_type": "short",
      "response": "funny folks",
      "accuracy": 0.0,
      "g_nll": 1.31150104579865,
      "average_nll": 0.43716701526621665,
      "perplexity": 1.5483146476461849,
      "avg_token_probability": 0.6899385430439909,
      "sequence_length": 3
    },
    {
      "example_id": "bb_1997--80/80_876537.txt#0_1",
      "dataset_type": "short",
      "response": "five pointed star",
      "accuracy": 1.0,
      "g_nll": 2.148580383043736,
      "average_nll": 0.7161934610145787,
      "perplexity": 2.046627795767506,
      "avg_token_probability": 0.5765016609420265,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1811--193/193_2452704.txt#0_1",
      "dataset_type": "short",
      "response": "vince",
      "accuracy": 0.0,
      "g_nll": 3.453716702759266,
      "average_nll": 1.726858351379633,
      "perplexity": 5.622960763234079,
      "avg_token_probability": 0.5065878483834156,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_5662--54/54_415167.txt#0_2",
      "dataset_type": "short",
      "response": "puck",
      "accuracy": 1.0,
      "g_nll": 0.04865974606946111,
      "average_nll": 0.024329873034730554,
      "perplexity": 1.0246282593820575,
      "avg_token_probability": 0.9761188390321431,
      "sequence_length": 2
    },
    {
      "example_id": "odql_4122--97/97_2155160.txt#0_1",
      "dataset_type": "short",
      "response": "battle of la belle alliance",
      "accuracy": 0.0,
      "g_nll": 0.8117691071602167,
      "average_nll": 0.16235382143204333,
      "perplexity": 1.1762763594868881,
      "avg_token_probability": 0.8777492720218507,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_16378--17/17_461209.txt#0_0",
      "dataset_type": "short",
      "response": "john dryden",
      "accuracy": 0.0,
      "g_nll": 0.18892746278243067,
      "average_nll": 0.06297582092747689,
      "perplexity": 1.0650010881814402,
      "avg_token_probability": 0.9423935079778539,
      "sequence_length": 3
    },
    {
      "example_id": "qz_4241--129/129_203139.txt#0_0",
      "dataset_type": "short",
      "response": "dai green",
      "accuracy": 0.0,
      "g_nll": 2.5724565982818604,
      "average_nll": 0.8574855327606201,
      "perplexity": 2.3572260678979955,
      "avg_token_probability": 0.5198161513101361,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_9677--35/35_1731921.txt#0_0",
      "dataset_type": "short",
      "response": "stalingrad",
      "accuracy": 1.0,
      "g_nll": 0.35031944504498824,
      "average_nll": 0.11677314834832941,
      "perplexity": 1.1238644502635822,
      "avg_token_probability": 0.901485969437387,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_11145--8/8_2764204.txt#0_0",
      "dataset_type": "short",
      "response": "parking meter",
      "accuracy": 1.0,
      "g_nll": 0.18785597383975983,
      "average_nll": 0.09392798691987991,
      "perplexity": 1.0984806380946075,
      "avg_token_probability": 0.9108939486234292,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_13406--148/148_983957.txt#0_0",
      "dataset_type": "short",
      "response": "paul gauguin",
      "accuracy": 1.0,
      "g_nll": 0.03397089277802934,
      "average_nll": 0.008492723194507334,
      "perplexity": 1.0085288886767811,
      "avg_token_probability": 0.9915945549562906,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_2528--156/156_1523996.txt#0_1",
      "dataset_type": "short",
      "response": "las vegas",
      "accuracy": 0.0,
      "g_nll": 0.21248290460789576,
      "average_nll": 0.10624145230394788,
      "perplexity": 1.112090360871973,
      "avg_token_probability": 0.9042284743805692,
      "sequence_length": 2
    },
    {
      "example_id": "qf_2575--20/20_3214656.txt#0_0",
      "dataset_type": "short",
      "response": "meteor",
      "accuracy": 0.0,
      "g_nll": 0.03729324787855148,
      "average_nll": 0.03729324787855148,
      "perplexity": 1.0379973667373192,
      "avg_token_probability": 0.9633935807980378,
      "sequence_length": 1
    },
    {
      "example_id": "qf_478--194/194_393733.txt#0_1",
      "dataset_type": "short",
      "response": "james cameron",
      "accuracy": 1.0,
      "g_nll": 0.06087630846650427,
      "average_nll": 0.02029210282216809,
      "perplexity": 1.0204993872454549,
      "avg_token_probability": 0.9802316015559476,
      "sequence_length": 3
    },
    {
      "example_id": "qf_3745--59/59_2512300.txt#0_2",
      "dataset_type": "short",
      "response": "w somerset maugham",
      "accuracy": 0.0,
      "g_nll": 2.260408198375444,
      "average_nll": 0.3767346997292407,
      "perplexity": 1.4575175782654104,
      "avg_token_probability": 0.7746868829218633,
      "sequence_length": 6
    },
    {
      "example_id": "qf_1492--14/14_2475412.txt#0_0",
      "dataset_type": "short",
      "response": "at the alamo",
      "accuracy": 1.0,
      "g_nll": 1.142461818870288,
      "average_nll": 0.285615454717572,
      "perplexity": 1.3305806883332325,
      "avg_token_probability": 0.8275194285521745,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1391--61/61_1385217.txt#0_1",
      "dataset_type": "short",
      "response": "monotremes",
      "accuracy": 0.0,
      "g_nll": 0.03362580201610399,
      "average_nll": 0.011208600672034663,
      "perplexity": 1.0112716523901888,
      "avg_token_probability": 0.9889032230908121,
      "sequence_length": 3
    },
    {
      "example_id": "qz_2787--60/60_167773.txt#0_2",
      "dataset_type": "short",
      "response": "la toya",
      "accuracy": 1.0,
      "g_nll": 0.05651683546420827,
      "average_nll": 0.01883894515473609,
      "perplexity": 1.0190175176920893,
      "avg_token_probability": 0.9814786050717119,
      "sequence_length": 3
    },
    {
      "example_id": "qw_9299--131/131_1223632.txt#0_1",
      "dataset_type": "short",
      "response": "thor",
      "accuracy": 1.0,
      "g_nll": 0.0694378986954689,
      "average_nll": 0.0694378986954689,
      "perplexity": 1.071905492408375,
      "avg_token_probability": 0.9329180670146426,
      "sequence_length": 1
    },
    {
      "example_id": "bb_6957--37/37_265768.txt#0_0",
      "dataset_type": "short",
      "response": "40",
      "accuracy": 1.0,
      "g_nll": 0.017321198800345883,
      "average_nll": 0.008660599400172941,
      "perplexity": 1.008698210892104,
      "avg_token_probability": 0.991410610642825,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1811--193/193_2452704.txt#0_2",
      "dataset_type": "short",
      "response": "van gogh",
      "accuracy": 1.0,
      "g_nll": 0.030510157273965888,
      "average_nll": 0.010170052424655296,
      "perplexity": 1.0102219431691601,
      "avg_token_probability": 0.9899211160604806,
      "sequence_length": 3
    },
    {
      "example_id": "qg_3088--142/142_2565750.txt#0_1",
      "dataset_type": "short",
      "response": "tigon",
      "accuracy": 0.0,
      "g_nll": 0.09267380554229021,
      "average_nll": 0.046336902771145105,
      "perplexity": 1.0474272326575509,
      "avg_token_probability": 0.9555010287823873,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_80--166/166_2439219.txt#0_0",
      "dataset_type": "short",
      "response": "angela rippon",
      "accuracy": 1.0,
      "g_nll": 0.008122477787708249,
      "average_nll": 0.002030619446927062,
      "perplexity": 1.0020326825508195,
      "avg_token_probability": 0.9979774054558157,
      "sequence_length": 4
    },
    {
      "example_id": "qb_9469--195/195_527460.txt#0_2",
      "dataset_type": "short",
      "response": "lilac",
      "accuracy": 1.0,
      "g_nll": 0.09418383287265897,
      "average_nll": 0.047091916436329484,
      "perplexity": 1.0482183531473026,
      "avg_token_probability": 0.9550294388647373,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1414--178/178_269111.txt#0_1",
      "dataset_type": "short",
      "response": "bush fires",
      "accuracy": 0.0,
      "g_nll": 6.12580536538735,
      "average_nll": 3.062902682693675,
      "perplexity": 21.389554228475376,
      "avg_token_probability": 0.4994785112949404,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8182--60/60_558286.txt#0_0",
      "dataset_type": "short",
      "response": "smell",
      "accuracy": 1.0,
      "g_nll": 0.049424223601818085,
      "average_nll": 0.049424223601818085,
      "perplexity": 1.050665973515088,
      "avg_token_probability": 0.9517772776579212,
      "sequence_length": 1
    },
    {
      "example_id": "qw_5549--67/67_1155062.txt#0_1",
      "dataset_type": "short",
      "response": "weft",
      "accuracy": 0.0,
      "g_nll": 2.1099517185939476,
      "average_nll": 1.0549758592969738,
      "perplexity": 2.871905823238906,
      "avg_token_probability": 0.5602423862531134,
      "sequence_length": 2
    },
    {
      "example_id": "qw_1265--47/47_2693885.txt#0_1",
      "dataset_type": "short",
      "response": "jules verne",
      "accuracy": 1.0,
      "g_nll": 0.42031198709150885,
      "average_nll": 0.10507799677287721,
      "perplexity": 1.1107972455775035,
      "avg_token_probability": 0.9100480731070051,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3600--123/123_649287.txt#0_1",
      "dataset_type": "short",
      "response": "tara king",
      "accuracy": 1.0,
      "g_nll": 0.055907346089952625,
      "average_nll": 0.01863578202998421,
      "perplexity": 1.018810511937698,
      "avg_token_probability": 0.9816900504002679,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5877--148/148_240409.txt#0_2",
      "dataset_type": "short",
      "response": "tartar",
      "accuracy": 1.0,
      "g_nll": 0.6486141681648405,
      "average_nll": 0.32430708408242026,
      "perplexity": 1.3830719612980296,
      "avg_token_probability": 0.7613843609656651,
      "sequence_length": 2
    },
    {
      "example_id": "qw_15440--126/126_2983047.txt#0_0",
      "dataset_type": "short",
      "response": "rising fast",
      "accuracy": 0.0,
      "g_nll": 0.4914291850582231,
      "average_nll": 0.24571459252911154,
      "perplexity": 1.278534618131911,
      "avg_token_probability": 0.8058326437281451,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3129--69/69_2501904.txt#0_0",
      "dataset_type": "short",
      "response": "tom hanks",
      "accuracy": 1.0,
      "g_nll": 0.05070996728318278,
      "average_nll": 0.01690332242772759,
      "perplexity": 1.0170469919380782,
      "avg_token_probability": 0.9835137884016146,
      "sequence_length": 3
    },
    {
      "example_id": "bt_615--93/93_253274.txt#0_1",
      "dataset_type": "short",
      "response": "eddie",
      "accuracy": 1.0,
      "g_nll": 0.026583724655210972,
      "average_nll": 0.013291862327605486,
      "perplexity": 1.0133805918205743,
      "avg_token_probability": 0.986878962536961,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7241--59/59_280648.txt#0_2",
      "dataset_type": "short",
      "response": "mars",
      "accuracy": 1.0,
      "g_nll": 0.008503420278429985,
      "average_nll": 0.008503420278429985,
      "perplexity": 1.0085396770526436,
      "avg_token_probability": 0.9915326315394948,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_18907--144/144_1890291.txt#0_2",
      "dataset_type": "short",
      "response": "canale",
      "accuracy": 0.0,
      "g_nll": 1.1933012008666992,
      "average_nll": 0.5966506004333496,
      "perplexity": 1.8160260057679187,
      "avg_token_probability": 0.5777290260612244,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1509--135/135_2390355.txt#0_0",
      "dataset_type": "short",
      "response": "bean bag toss",
      "accuracy": 0.0,
      "g_nll": 1.2622922054724768,
      "average_nll": 0.4207640684908256,
      "perplexity": 1.5231248828617794,
      "avg_token_probability": 0.7450102257562472,
      "sequence_length": 3
    },
    {
      "example_id": "bt_2235--118/118_319041.txt#0_0",
      "dataset_type": "short",
      "response": "surtsey",
      "accuracy": 1.0,
      "g_nll": 1.1411130001069978,
      "average_nll": 0.38037100003566593,
      "perplexity": 1.4628271977166807,
      "avg_token_probability": 0.7724810399383188,
      "sequence_length": 3
    },
    {
      "example_id": "bb_4368--85/85_456496.txt#0_2",
      "dataset_type": "short",
      "response": "hindi",
      "accuracy": 1.0,
      "g_nll": 4.484057433903217,
      "average_nll": 2.2420287169516087,
      "perplexity": 9.41240704096747,
      "avg_token_probability": 0.48866917381785246,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2063--191/191_3215408.txt#0_0",
      "dataset_type": "short",
      "response": "function",
      "accuracy": 1.0,
      "g_nll": 0.005674681626260281,
      "average_nll": 0.005674681626260281,
      "perplexity": 1.0056908131313234,
      "avg_token_probability": 0.9943413889666503,
      "sequence_length": 1
    },
    {
      "example_id": "jp_3026--138/138_366909.txt#0_2",
      "dataset_type": "short",
      "response": "insects",
      "accuracy": 0.0,
      "g_nll": 0.04700924828648567,
      "average_nll": 0.04700924828648567,
      "perplexity": 1.048131702457091,
      "avg_token_probability": 0.9540785739575877,
      "sequence_length": 1
    },
    {
      "example_id": "bb_200--50/50_834132.txt#0_2",
      "dataset_type": "short",
      "response": "goldtrail",
      "accuracy": 1.0,
      "g_nll": 0.009609594329958782,
      "average_nll": 0.004804797164979391,
      "perplexity": 1.0048163587124246,
      "avg_token_probability": 0.9952166397675217,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2820--42/42_1531177.txt#0_1",
      "dataset_type": "short",
      "response": "Ali MacGraw",
      "accuracy": 1.0,
      "g_nll": 2.870082766177802,
      "average_nll": 0.7175206915444505,
      "perplexity": 2.049345946068354,
      "avg_token_probability": 0.7371225415422382,
      "sequence_length": 4
    },
    {
      "example_id": "odql_7759--63/63_1347430.txt#0_0",
      "dataset_type": "short",
      "response": "robert",
      "accuracy": 0.0,
      "g_nll": 0.05093517154455185,
      "average_nll": 0.05093517154455185,
      "perplexity": 1.0522546750266657,
      "avg_token_probability": 0.9503402776278077,
      "sequence_length": 1
    },
    {
      "example_id": "qw_2021--84/84_31075.txt#0_1",
      "dataset_type": "short",
      "response": "gagarin",
      "accuracy": 1.0,
      "g_nll": 0.27758748083442697,
      "average_nll": 0.09252916027814233,
      "perplexity": 1.0969451283189788,
      "avg_token_probability": 0.9138315678952535,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1103--13/13_2468995.txt#0_2",
      "dataset_type": "short",
      "response": "lucille ball",
      "accuracy": 1.0,
      "g_nll": 0.03828571463236585,
      "average_nll": 0.012761904877455285,
      "perplexity": 1.0128436855074177,
      "avg_token_probability": 0.9874283513778259,
      "sequence_length": 3
    },
    {
      "example_id": "qw_7276--142/142_1187158.txt#0_1",
      "dataset_type": "short",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 1.1367106437683105,
      "average_nll": 1.1367106437683105,
      "perplexity": 3.1165002072520633,
      "avg_token_probability": 0.3208727526065971,
      "sequence_length": 1
    },
    {
      "example_id": "wh_1706--59/59_2662426.txt#0_0",
      "dataset_type": "short",
      "response": "jarhead",
      "accuracy": 1.0,
      "g_nll": 0.015066083517467632,
      "average_nll": 0.007533041758733816,
      "perplexity": 1.0075614864980798,
      "avg_token_probability": 0.99252340233228,
      "sequence_length": 2
    },
    {
      "example_id": "qb_5952--33/33_419470.txt#0_1",
      "dataset_type": "short",
      "response": "3600",
      "accuracy": 1.0,
      "g_nll": 0.07334165871725418,
      "average_nll": 0.02444721957241806,
      "perplexity": 1.0247485030156618,
      "avg_token_probability": 0.9764040684351939,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_4557--72/72_1569959.txt#0_0",
      "dataset_type": "short",
      "response": "air traffic controller",
      "accuracy": 1.0,
      "g_nll": 1.799427853547968,
      "average_nll": 0.5998092845159894,
      "perplexity": 1.8217713272568743,
      "avg_token_probability": 0.7178610543216228,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_26163--17/17_422703.txt#0_0",
      "dataset_type": "short",
      "response": "ping pong",
      "accuracy": 0.0,
      "g_nll": 0.08203610545024276,
      "average_nll": 0.04101805272512138,
      "perplexity": 1.0418709139849294,
      "avg_token_probability": 0.9604844314943738,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_5913--103/103_1600716.txt#0_1",
      "dataset_type": "short",
      "response": "hard times",
      "accuracy": 1.0,
      "g_nll": 0.002540657762438059,
      "average_nll": 0.0012703288812190294,
      "perplexity": 1.0012711360907232,
      "avg_token_probability": 0.998730886439514,
      "sequence_length": 2
    },
    {
      "example_id": "odql_1809--193/193_609132.txt#0_0",
      "dataset_type": "short",
      "response": "calorie",
      "accuracy": 0.0,
      "g_nll": 4.095853805541992,
      "average_nll": 4.095853805541992,
      "perplexity": 60.0906229680462,
      "avg_token_probability": 0.01664153158358435,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_22957--6/6_1623582.txt#0_1",
      "dataset_type": "short",
      "response": "caravaggio",
      "accuracy": 0.0,
      "g_nll": 0.0195352013390675,
      "average_nll": 0.006511733779689166,
      "perplexity": 1.006532981212269,
      "avg_token_probability": 0.9935511360287731,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1031--61/61_537330.txt#0_0",
      "dataset_type": "short",
      "response": "jim hacker",
      "accuracy": 1.0,
      "g_nll": 0.9285671412944794,
      "average_nll": 0.4642835706472397,
      "perplexity": 1.5908740316619905,
      "avg_token_probability": 0.6367181073053112,
      "sequence_length": 2
    },
    {
      "example_id": "odql_8256--23/23_225768.txt#0_0",
      "dataset_type": "short",
      "response": "2004",
      "accuracy": 1.0,
      "g_nll": 0.0019129658676320105,
      "average_nll": 0.0006376552892106702,
      "perplexity": 1.0006378586345637,
      "avg_token_probability": 0.9993626575812193,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1299--110/110_771432.txt#0_1",
      "dataset_type": "short",
      "response": "extrusive",
      "accuracy": 0.0,
      "g_nll": 0.6779618858590766,
      "average_nll": 0.3389809429295383,
      "perplexity": 1.4035165980431625,
      "avg_token_probability": 0.7538223890636104,
      "sequence_length": 2
    },
    {
      "example_id": "qf_2195--13/13_325145.txt#0_2",
      "dataset_type": "short",
      "response": "andre agassi",
      "accuracy": 1.0,
      "g_nll": 0.14939270586819475,
      "average_nll": 0.04979756862273158,
      "perplexity": 1.0510583076583855,
      "avg_token_probability": 0.9535480321666475,
      "sequence_length": 3
    },
    {
      "example_id": "qb_1400--176/176_304165.txt#0",
      "dataset_type": "short",
      "response": "a. hailey",
      "accuracy": 1.0,
      "g_nll": 9.529192268818406,
      "average_nll": 2.3822980672046015,
      "perplexity": 10.829761809675501,
      "avg_token_probability": 0.42430083370067234,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_24400--153/153_1526690.txt#0_0",
      "dataset_type": "short",
      "response": "carousel",
      "accuracy": 1.0,
      "g_nll": 0.1877143681049347,
      "average_nll": 0.1877143681049347,
      "perplexity": 1.2064888544310066,
      "avg_token_probability": 0.8288514198265106,
      "sequence_length": 1
    },
    {
      "example_id": "odql_9310--183/183_763371.txt#0_0",
      "dataset_type": "short",
      "response": "tesco",
      "accuracy": 1.0,
      "g_nll": 0.015739412976017775,
      "average_nll": 0.007869706488008887,
      "perplexity": 1.0079007540196594,
      "avg_token_probability": 0.9921918853887501,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_3467--85/85_645957.txt#0_1",
      "dataset_type": "short",
      "response": "willie nelson",
      "accuracy": 1.0,
      "g_nll": 0.22475868072592675,
      "average_nll": 0.05618967018148169,
      "perplexity": 1.0577982975058438,
      "avg_token_probability": 0.9474690799205328,
      "sequence_length": 4
    },
    {
      "example_id": "tb_1113--48/48_567125.txt#0_1",
      "dataset_type": "short",
      "response": "man",
      "accuracy": 1.0,
      "g_nll": 0.008164010010659695,
      "average_nll": 0.008164010010659695,
      "perplexity": 1.008197426415774,
      "avg_token_probability": 0.991869225013878,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_18732--23/23_1886509.txt#0_0",
      "dataset_type": "short",
      "response": "spiez",
      "accuracy": 0.0,
      "g_nll": 0.7119055630173534,
      "average_nll": 0.3559527815086767,
      "perplexity": 1.427540140380676,
      "avg_token_probability": 0.7452861834400093,
      "sequence_length": 2
    },
    {
      "example_id": "bb_5507--150/150_954054.txt#0_1",
      "dataset_type": "short",
      "response": "neon",
      "accuracy": 1.0,
      "g_nll": 0.004549865610897541,
      "average_nll": 0.004549865610897541,
      "perplexity": 1.004560231965313,
      "avg_token_probability": 0.995460469347476,
      "sequence_length": 1
    },
    {
      "example_id": "odql_12482--151/151_684649.txt#0_2",
      "dataset_type": "short",
      "response": "philippines",
      "accuracy": 1.0,
      "g_nll": 0.3228824734687805,
      "average_nll": 0.3228824734687805,
      "perplexity": 1.3811030251192244,
      "avg_token_probability": 0.7240589455038479,
      "sequence_length": 1
    },
    {
      "example_id": "qb_7146--4/4_464898.txt#0_0",
      "dataset_type": "short",
      "response": "roshi",
      "accuracy": 1.0,
      "g_nll": 0.0863372600870207,
      "average_nll": 0.04316863004351035,
      "perplexity": 1.0441139489868345,
      "avg_token_probability": 0.9586350922325235,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8955--114/114_513485.txt#0_0",
      "dataset_type": "short",
      "response": "james mason",
      "accuracy": 1.0,
      "g_nll": 0.03137762270489475,
      "average_nll": 0.010459207568298249,
      "perplexity": 1.0105140962769998,
      "avg_token_probability": 0.9896342257644388,
      "sequence_length": 3
    },
    {
      "example_id": "jp_1630--10/10_1391268.txt#0_1",
      "dataset_type": "short",
      "response": "bobby fischer",
      "accuracy": 1.0,
      "g_nll": 0.18588597747293534,
      "average_nll": 0.046471494368233834,
      "perplexity": 1.0475682170490728,
      "avg_token_probability": 0.9575138746303223,
      "sequence_length": 4
    },
    {
      "example_id": "bb_5116--124/124_2680471.txt#0_0",
      "dataset_type": "short",
      "response": "splitting",
      "accuracy": 0.0,
      "g_nll": 0.1411260962486267,
      "average_nll": 0.1411260962486267,
      "perplexity": 1.151569847467794,
      "avg_token_probability": 0.8683798053578049,
      "sequence_length": 1
    },
    {
      "example_id": "odql_7016--55/55_1322268.txt#0_0",
      "dataset_type": "short",
      "response": "1960",
      "accuracy": 0.0,
      "g_nll": 0.08392337251643767,
      "average_nll": 0.027974457505479222,
      "perplexity": 1.02836941696576,
      "avg_token_probability": 0.9726109513896598,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3301--62/62_294292.txt#0_1",
      "dataset_type": "short",
      "response": "thor",
      "accuracy": 1.0,
      "g_nll": 0.06541314721107483,
      "average_nll": 0.06541314721107483,
      "perplexity": 1.067600009248451,
      "avg_token_probability": 0.9366803965316197,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_22949--160/160_1977438.txt#0_1",
      "dataset_type": "short",
      "response": "millemiglia",
      "accuracy": 0.0,
      "g_nll": 0.8621965052734595,
      "average_nll": 0.28739883509115316,
      "perplexity": 1.3329557369963823,
      "avg_token_probability": 0.7880270043446377,
      "sequence_length": 3
    },
    {
      "example_id": "qz_1958--123/123_146775.txt#0_1",
      "dataset_type": "short",
      "response": "hair and fur",
      "accuracy": 0.0,
      "g_nll": 1.435817095800303,
      "average_nll": 0.478605698600101,
      "perplexity": 1.613822677547781,
      "avg_token_probability": 0.72141459151196,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_2287--128/128_614230.txt#0_0",
      "dataset_type": "short",
      "response": "braffin",
      "accuracy": 0.0,
      "g_nll": 2.9674214015103644,
      "average_nll": 0.9891404671701215,
      "perplexity": 2.6889222618201614,
      "avg_token_probability": 0.6833424655366098,
      "sequence_length": 3
    },
    {
      "example_id": "odql_4611--0/0_1366344.txt#0_0",
      "dataset_type": "short",
      "response": "babylon",
      "accuracy": 1.0,
      "g_nll": 0.16067947819828987,
      "average_nll": 0.08033973909914494,
      "perplexity": 1.0836551651724613,
      "avg_token_probability": 0.9232694541618862,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3304--119/119_1433184.txt#0_0",
      "dataset_type": "short",
      "response": "clavicle",
      "accuracy": 0.0,
      "g_nll": 0.8588728295144392,
      "average_nll": 0.28629094317147974,
      "perplexity": 1.3314797838553294,
      "avg_token_probability": 0.8077901826350932,
      "sequence_length": 3
    },
    {
      "example_id": "odql_6502--195/195_349954.txt#0_0",
      "dataset_type": "short",
      "response": "trumpet",
      "accuracy": 1.0,
      "g_nll": 0.008140125311911106,
      "average_nll": 0.008140125311911106,
      "perplexity": 1.0081733462115403,
      "avg_token_probability": 0.9918929157944378,
      "sequence_length": 1
    },
    {
      "example_id": "jp_2490--7/7_1413067.txt#0_0",
      "dataset_type": "short",
      "response": "charles darwin",
      "accuracy": 1.0,
      "g_nll": 0.036793894681068195,
      "average_nll": 0.009198473670267049,
      "perplexity": 1.0092409096447967,
      "avg_token_probability": 0.9909646416723762,
      "sequence_length": 4
    },
    {
      "example_id": "qw_5145--123/123_1138870.txt#0_1",
      "dataset_type": "short",
      "response": "rowing",
      "accuracy": 1.0,
      "g_nll": 0.03084343997761607,
      "average_nll": 0.015421719988808036,
      "perplexity": 1.015541248366445,
      "avg_token_probability": 0.9848122968794764,
      "sequence_length": 2
    },
    {
      "example_id": "bb_3805--121/121_2943039.txt#0_2",
      "dataset_type": "short",
      "response": "coconut shy",
      "accuracy": 0.0,
      "g_nll": 0.04528878442943096,
      "average_nll": 0.02264439221471548,
      "perplexity": 1.0229027226912204,
      "avg_token_probability": 0.9776108689658342,
      "sequence_length": 2
    },
    {
      "example_id": "tc_1968--49/49_57917.txt#0_2",
      "dataset_type": "short",
      "response": "drag show",
      "accuracy": 0.0,
      "g_nll": 1.46624456346035,
      "average_nll": 0.733122281730175,
      "perplexity": 2.081569719089444,
      "avg_token_probability": 0.5389585895602407,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1080--112/112_854470.txt#0_0",
      "dataset_type": "short",
      "response": "peppers",
      "accuracy": 1.0,
      "g_nll": 0.4727100729942322,
      "average_nll": 0.4727100729942322,
      "perplexity": 1.6043361751578542,
      "avg_token_probability": 0.62331075960536,
      "sequence_length": 1
    },
    {
      "example_id": "qw_597--13/13_1061577.txt#0_2",
      "dataset_type": "short",
      "response": "pygmalian",
      "accuracy": 0.0,
      "g_nll": 2.4346321299672127,
      "average_nll": 0.8115440433224043,
      "perplexity": 2.251381534957094,
      "avg_token_probability": 0.5314624028416965,
      "sequence_length": 3
    },
    {
      "example_id": "qb_2850--83/83_346130.txt#0_1",
      "dataset_type": "short",
      "response": "the times",
      "accuracy": 1.0,
      "g_nll": 0.13275480226729997,
      "average_nll": 0.06637740113364998,
      "perplexity": 1.0686299432242468,
      "avg_token_probability": 0.9378246335162121,
      "sequence_length": 2
    },
    {
      "example_id": "qw_4081--32/32_1128800.txt#0_0",
      "dataset_type": "short",
      "response": "germany",
      "accuracy": 1.0,
      "g_nll": 0.026583611965179443,
      "average_nll": 0.026583611965179443,
      "perplexity": 1.0269401081526974,
      "avg_token_probability": 0.9737666218907757,
      "sequence_length": 1
    },
    {
      "example_id": "bt_2066--111/111_2841537.txt#0_2",
      "dataset_type": "short",
      "response": "the Hindenburg",
      "accuracy": 1.0,
      "g_nll": 2.582704989472404,
      "average_nll": 0.8609016631574681,
      "perplexity": 2.365292429547897,
      "avg_token_probability": 0.5946427922458618,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5143--23/23_1663094.txt#0_1",
      "dataset_type": "short",
      "response": "algeria",
      "accuracy": 0.0,
      "g_nll": 0.5999253682866765,
      "average_nll": 0.1999751227622255,
      "perplexity": 1.2213723734112825,
      "avg_token_probability": 0.8495783467912554,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3732--157/157_915760.txt#0_0",
      "dataset_type": "short",
      "response": "Project Glass",
      "accuracy": 1.0,
      "g_nll": 2.7478062212467194,
      "average_nll": 1.3739031106233597,
      "perplexity": 3.9507408197172236,
      "avg_token_probability": 0.4958038386929828,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2413--135/135_2861641.txt#0_1",
      "dataset_type": "short",
      "response": "machu picchu",
      "accuracy": 1.0,
      "g_nll": 0.15438843877564068,
      "average_nll": 0.03859710969391017,
      "perplexity": 1.0393516545784887,
      "avg_token_probability": 0.9639551167534735,
      "sequence_length": 4
    },
    {
      "example_id": "wh_2204--68/68_777602.txt#0_0",
      "dataset_type": "short",
      "response": "paddy ashdown",
      "accuracy": 1.0,
      "g_nll": 0.2805683596483277,
      "average_nll": 0.07014208991208193,
      "perplexity": 1.072660584674636,
      "avg_token_probability": 0.9386846333403487,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_10516--89/89_1704034.txt#0_0",
      "dataset_type": "short",
      "response": "jack wilshere",
      "accuracy": 1.0,
      "g_nll": 0.16628383786860468,
      "average_nll": 0.04157095946715117,
      "perplexity": 1.0424471307199759,
      "avg_token_probability": 0.9615673583133484,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_3579--194/194_311741.txt#0_1",
      "dataset_type": "short",
      "response": "the owl",
      "accuracy": 0.0,
      "g_nll": 1.687043309211731,
      "average_nll": 0.8435216546058655,
      "perplexity": 2.3245388019409896,
      "avg_token_probability": 0.46052344226558073,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_5497--118/118_700447.txt#0_0",
      "dataset_type": "short",
      "response": "#JeSuisCharlie",
      "accuracy": 1.0,
      "g_nll": 0.4252613253684103,
      "average_nll": 0.08505226507368206,
      "perplexity": 1.088773970063125,
      "avg_token_probability": 0.927643852587362,
      "sequence_length": 5
    },
    {
      "example_id": "tc_3129--45/45_95047.txt#0_1",
      "dataset_type": "short",
      "response": "jupiter",
      "accuracy": 0.0,
      "g_nll": 0.6543077222813736,
      "average_nll": 0.3271538611406868,
      "perplexity": 1.3870148684533785,
      "avg_token_probability": 0.7598915053602209,
      "sequence_length": 2
    },
    {
      "example_id": "qg_4076--107/107_3215880.txt#0_2",
      "dataset_type": "short",
      "response": "sesame street",
      "accuracy": 1.0,
      "g_nll": 0.022586396415135823,
      "average_nll": 0.011293198207567912,
      "perplexity": 1.0113572070985433,
      "avg_token_probability": 0.9888322569637036,
      "sequence_length": 2
    },
    {
      "example_id": "qb_8140--128/128_491508.txt#0_1",
      "dataset_type": "short",
      "response": "star",
      "accuracy": 0.0,
      "g_nll": 1.4554005861282349,
      "average_nll": 1.4554005861282349,
      "perplexity": 4.286200114054911,
      "avg_token_probability": 0.23330688567733748,
      "sequence_length": 1
    },
    {
      "example_id": "odql_4941--169/169_781368.txt#0_2",
      "dataset_type": "short",
      "response": "felix",
      "accuracy": 1.0,
      "g_nll": 0.23311196267604828,
      "average_nll": 0.11655598133802414,
      "perplexity": 1.123620410480584,
      "avg_token_probability": 0.8960324434276361,
      "sequence_length": 2
    },
    {
      "example_id": "qf_1776--73/73_1938527.txt#0_2",
      "dataset_type": "short",
      "response": "anthony joshua",
      "accuracy": 1.0,
      "g_nll": 0.3080462145952083,
      "average_nll": 0.061609242919041664,
      "perplexity": 1.0635466751261673,
      "avg_token_probability": 0.9455977577827103,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_19473--126/126_273221.txt#0_0",
      "dataset_type": "short",
      "response": "uranus",
      "accuracy": 1.0,
      "g_nll": 0.03978126123547554,
      "average_nll": 0.03978126123547554,
      "perplexity": 1.0405831334273752,
      "avg_token_probability": 0.9609996240341642,
      "sequence_length": 1
    },
    {
      "example_id": "qb_8937--45/45_513006.txt#0_0",
      "dataset_type": "short",
      "response": "tanger med",
      "accuracy": 0.0,
      "g_nll": 1.4652746617781531,
      "average_nll": 0.4884248872593844,
      "perplexity": 1.6297471616899648,
      "avg_token_probability": 0.6986333737268402,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_3949--142/142_1672963.txt#0_1",
      "dataset_type": "short",
      "response": "kipps",
      "accuracy": 1.0,
      "g_nll": 0.06605951779022234,
      "average_nll": 0.022019839263407448,
      "perplexity": 1.022264065235322,
      "avg_token_probability": 0.9786685977136044,
      "sequence_length": 3
    },
    {
      "example_id": "qw_2956--145/145_1107670.txt#0_1",
      "dataset_type": "short",
      "response": "surrealism",
      "accuracy": 0.0,
      "g_nll": 0.8938364882342285,
      "average_nll": 0.4469182441171142,
      "perplexity": 1.5634864698443627,
      "avg_token_probability": 0.7044998145642003,
      "sequence_length": 2
    },
    {
      "example_id": "tc_2506--24/24_75775.txt#0_0",
      "dataset_type": "short",
      "response": "finland",
      "accuracy": 0.0,
      "g_nll": 0.007187730623172683,
      "average_nll": 0.0035938653115863417,
      "perplexity": 1.003600330988796,
      "avg_token_probability": 0.9964190038814986,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_22883--94/94_491081.txt#0_2",
      "dataset_type": "short",
      "response": "marc",
      "accuracy": 1.0,
      "g_nll": 0.3389036953449249,
      "average_nll": 0.3389036953449249,
      "perplexity": 1.4034081839634156,
      "avg_token_probability": 0.7125510677697945,
      "sequence_length": 1
    },
    {
      "example_id": "dpql_5156--28/28_691416.txt#0_2",
      "dataset_type": "short",
      "response": "istanbul",
      "accuracy": 1.0,
      "g_nll": 0.0034356906144239474,
      "average_nll": 0.0011452302048079825,
      "perplexity": 1.001145886231329,
      "avg_token_probability": 0.9988565674730321,
      "sequence_length": 3
    },
    {
      "example_id": "wh_1415--64/64_759730.txt#0_0",
      "dataset_type": "short",
      "response": "the colossus of rhodes",
      "accuracy": 1.0,
      "g_nll": 1.1464001353924687,
      "average_nll": 0.1637714479132098,
      "perplexity": 1.177945062522612,
      "avg_token_probability": 0.8772394365024628,
      "sequence_length": 7
    },
    {
      "example_id": "wh_2443--99/99_783232.txt#0_2",
      "dataset_type": "short",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 0.013297210447490215,
      "average_nll": 0.013297210447490215,
      "perplexity": 1.0133860115159607,
      "avg_token_probability": 0.9867908068950586,
      "sequence_length": 1
    },
    {
      "example_id": "bb_6162--31/31_932368.txt#0_1",
      "dataset_type": "short",
      "response": "Thank you",
      "accuracy": 1.0,
      "g_nll": 1.673023629002273,
      "average_nll": 0.8365118145011365,
      "perplexity": 2.3083011348374742,
      "avg_token_probability": 0.5886725676503273,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7238--11/11_75755.txt#0_0",
      "dataset_type": "short",
      "response": "moscow",
      "accuracy": 1.0,
      "g_nll": 0.006150785367935896,
      "average_nll": 0.003075392683967948,
      "perplexity": 1.0030801265556424,
      "avg_token_probability": 0.9969321780528516,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_18399--42/42_156375.txt#0_0",
      "dataset_type": "short",
      "response": "spain",
      "accuracy": 0.0,
      "g_nll": 5.441743969905474,
      "average_nll": 2.720871984952737,
      "perplexity": 15.19356503044493,
      "avg_token_probability": 0.5021635277766548,
      "sequence_length": 2
    },
    {
      "example_id": "qw_8714--28/28_287982.txt#0_0",
      "dataset_type": "short",
      "response": "fool",
      "accuracy": 0.0,
      "g_nll": 0.30459216237068176,
      "average_nll": 0.30459216237068176,
      "perplexity": 1.3560718330905452,
      "avg_token_probability": 0.7374240623529195,
      "sequence_length": 1
    },
    {
      "example_id": "qb_3427--81/81_362513.txt#0_3",
      "dataset_type": "short",
      "response": "Wolfgang amadeus mozart",
      "accuracy": 1.0,
      "g_nll": 2.3441825968329795,
      "average_nll": 0.39069709947216325,
      "perplexity": 1.4780107553655475,
      "avg_token_probability": 0.77104222071732,
      "sequence_length": 6
    },
    {
      "example_id": "bt_2892--150/150_2107251.txt#0_0",
      "dataset_type": "short",
      "response": "bruno mars",
      "accuracy": 1.0,
      "g_nll": 0.10709991229623483,
      "average_nll": 0.035699970765411614,
      "perplexity": 1.036344866084709,
      "avg_token_probability": 0.9661271993258955,
      "sequence_length": 3
    },
    {
      "example_id": "qf_1386--60/60_3214498.txt#0_0",
      "dataset_type": "short",
      "response": "eat porridge (it\u2019s a spoon)",
      "accuracy": 1.0,
      "g_nll": 0.5483290072485261,
      "average_nll": 0.06854112590606576,
      "perplexity": 1.070944667615128,
      "avg_token_probability": 0.9380454979728462,
      "sequence_length": 8
    },
    {
      "example_id": "wh_1046--61/61_640960.txt#0_0",
      "dataset_type": "short",
      "response": "surrey",
      "accuracy": 0.0,
      "g_nll": 0.3598949015076869,
      "average_nll": 0.17994745075384344,
      "perplexity": 1.1971544519048756,
      "avg_token_probability": 0.8488742868678155,
      "sequence_length": 2
    },
    {
      "example_id": "qb_9573--143/143_499089.txt#0_0",
      "dataset_type": "short",
      "response": "15",
      "accuracy": 1.0,
      "g_nll": 0.7732120600994676,
      "average_nll": 0.3866060300497338,
      "perplexity": 1.4719764625299543,
      "avg_token_probability": 0.7306246655864819,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_901--24/24_504588.txt#0_1",
      "dataset_type": "short",
      "response": "fidelio",
      "accuracy": 0.0,
      "g_nll": 0.0670240586705404,
      "average_nll": 0.022341352890180133,
      "perplexity": 1.0225927899043539,
      "avg_token_probability": 0.9783897705728251,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9204--170/170_1040261.txt#0_2",
      "dataset_type": "short",
      "response": "snake",
      "accuracy": 1.0,
      "g_nll": 0.03944256529211998,
      "average_nll": 0.03944256529211998,
      "perplexity": 1.0402307518198382,
      "avg_token_probability": 0.9613251658351224,
      "sequence_length": 1
    },
    {
      "example_id": "tc_1250--67/67_36879.txt#0_0",
      "dataset_type": "short",
      "response": "martina hingis",
      "accuracy": 1.0,
      "g_nll": 0.3204739167410935,
      "average_nll": 0.06409478334821869,
      "perplexity": 1.0661934513575464,
      "avg_token_probability": 0.9413611401730723,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_16328--35/35_1834766.txt#0_0",
      "dataset_type": "short",
      "response": "bromley",
      "accuracy": 1.0,
      "g_nll": 0.09671691059907062,
      "average_nll": 0.04835845529953531,
      "perplexity": 1.0495468035181177,
      "avg_token_probability": 0.9539063985741547,
      "sequence_length": 2
    },
    {
      "example_id": "qz_3456--13/13_2880285.txt#0_1",
      "dataset_type": "short",
      "response": "colin",
      "accuracy": 0.0,
      "g_nll": 4.028110206127167,
      "average_nll": 2.0140551030635834,
      "perplexity": 7.493643314263213,
      "avg_token_probability": 0.27900220586589347,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1194--11/11_846390.txt#0_1",
      "dataset_type": "short",
      "response": "james Ramsay MacDonald",
      "accuracy": 0.0,
      "g_nll": 3.651339247822193,
      "average_nll": 0.9128348119555483,
      "perplexity": 2.491375112248364,
      "avg_token_probability": 0.557430547350376,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_24600--154/154_2012736.txt#0_1",
      "dataset_type": "short",
      "response": "walnut burl",
      "accuracy": 1.0,
      "g_nll": 1.3924155614949996,
      "average_nll": 0.4641385204983332,
      "perplexity": 1.5906432918816307,
      "avg_token_probability": 0.6661161579097491,
      "sequence_length": 3
    },
    {
      "example_id": "qg_2790--167/167_3215631.txt#0_0",
      "dataset_type": "short",
      "response": "gold",
      "accuracy": 1.0,
      "g_nll": 0.0038001956418156624,
      "average_nll": 0.0038001956418156624,
      "perplexity": 1.0038074255407161,
      "avg_token_probability": 0.9962070159635797,
      "sequence_length": 1
    },
    {
      "example_id": "qw_1239--16/16_1074565.txt#0_2",
      "dataset_type": "short",
      "response": "new york",
      "accuracy": 1.0,
      "g_nll": 0.009613548347260803,
      "average_nll": 0.0048067741736304015,
      "perplexity": 1.0048183452450221,
      "avg_token_probability": 0.9952145938169334,
      "sequence_length": 2
    },
    {
      "example_id": "qw_11125--81/81_1257240.txt#0_1",
      "dataset_type": "short",
      "response": "strychnine",
      "accuracy": 1.0,
      "g_nll": 0.11094074203853665,
      "average_nll": 0.027735185509634164,
      "perplexity": 1.028123386398183,
      "avg_token_probability": 0.9737409469566481,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_11236--88/88_1721040.txt#0_0",
      "dataset_type": "short",
      "response": "shark",
      "accuracy": 1.0,
      "g_nll": 0.9701576828956604,
      "average_nll": 0.9701576828956604,
      "perplexity": 2.6383604508716063,
      "avg_token_probability": 0.3790232679047478,
      "sequence_length": 1
    },
    {
      "example_id": "qw_12653--136/136_1283549.txt#0_1",
      "dataset_type": "short",
      "response": "endometriosis",
      "accuracy": 1.0,
      "g_nll": 0.06460288773678258,
      "average_nll": 0.016150721934195644,
      "perplexity": 1.01628184982928,
      "avg_token_probability": 0.9843587000218719,
      "sequence_length": 4
    },
    {
      "example_id": "qb_8836--4/4_176289.txt#0_1",
      "dataset_type": "short",
      "response": "tribbiani",
      "accuracy": 1.0,
      "g_nll": 0.11376117638428695,
      "average_nll": 0.03792039212809565,
      "perplexity": 1.038648544986915,
      "avg_token_probability": 0.9640862615586806,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_16447--2/2_201163.txt#0_1",
      "dataset_type": "short",
      "response": "the herald of free enterprise",
      "accuracy": 1.0,
      "g_nll": 0.588487368368078,
      "average_nll": 0.1176974736736156,
      "perplexity": 1.1249037468872485,
      "avg_token_probability": 0.9024569660243268,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_14271--117/117_152714.txt#0_1",
      "dataset_type": "short",
      "response": "kia",
      "accuracy": 1.0,
      "g_nll": 0.10853244364261627,
      "average_nll": 0.10853244364261627,
      "perplexity": 1.1146410709682695,
      "avg_token_probability": 0.8971497875377203,
      "sequence_length": 1
    },
    {
      "example_id": "qz_2423--179/179_157731.txt#0_0",
      "dataset_type": "short",
      "response": "MARILLION",
      "accuracy": 1.0,
      "g_nll": 3.9271322487047655,
      "average_nll": 1.3090440829015886,
      "perplexity": 3.7026326101586613,
      "avg_token_probability": 0.6732247535580015,
      "sequence_length": 3
    },
    {
      "example_id": "qb_482--134/134_278266.txt#0_0",
      "dataset_type": "short",
      "response": "brain",
      "accuracy": 1.0,
      "g_nll": 0.029601942747831345,
      "average_nll": 0.029601942747831345,
      "perplexity": 1.0300444356799576,
      "avg_token_probability": 0.9708319033245159,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_12025--58/58_1700414.txt#0_2",
      "dataset_type": "short",
      "response": "127 hours",
      "accuracy": 1.0,
      "g_nll": 0.006013165038893931,
      "average_nll": 0.002004388346297977,
      "perplexity": 1.0020063984754215,
      "avg_token_probability": 0.9980000441269601,
      "sequence_length": 3
    },
    {
      "example_id": "qw_3080--104/104_1110125.txt#0_1",
      "dataset_type": "short",
      "response": "Burma",
      "accuracy": 1.0,
      "g_nll": 1.301403284072876,
      "average_nll": 1.301403284072876,
      "perplexity": 3.674449347682947,
      "avg_token_probability": 0.2721496217196694,
      "sequence_length": 1
    },
    {
      "example_id": "bb_1614--48/48_867787.txt#0_1",
      "dataset_type": "short",
      "response": "norway",
      "accuracy": 1.0,
      "g_nll": 0.008053839954300201,
      "average_nll": 0.004026919977150101,
      "perplexity": 1.004035038913846,
      "avg_token_probability": 0.9959891758680701,
      "sequence_length": 2
    },
    {
      "example_id": "qg_248--100/100_282206.txt#0_1",
      "dataset_type": "short",
      "response": "kipling",
      "accuracy": 1.0,
      "g_nll": 3.3265528678712144,
      "average_nll": 1.1088509559570714,
      "perplexity": 3.03087378536648,
      "avg_token_probability": 0.6786366019698935,
      "sequence_length": 3
    },
    {
      "example_id": "odql_1018--140/140_2097132.txt#0_0",
      "dataset_type": "short",
      "response": "su hs4 (as a pair)",
      "accuracy": 0.0,
      "g_nll": 7.324450734560742,
      "average_nll": 1.0463501049372488,
      "perplexity": 2.847240002653485,
      "avg_token_probability": 0.6771649969517733,
      "sequence_length": 7
    },
    {
      "example_id": "sfq_21930--191/191_1362130.txt#0_0",
      "dataset_type": "short",
      "response": "the hesperides",
      "accuracy": 1.0,
      "g_nll": 1.692551650950918,
      "average_nll": 0.4231379127377295,
      "perplexity": 1.5267448390084013,
      "avg_token_probability": 0.7281423181310087,
      "sequence_length": 4
    },
    {
      "example_id": "tc_2250--43/43_67037.txt#0_0",
      "dataset_type": "short",
      "response": "earthquake",
      "accuracy": 1.0,
      "g_nll": 0.4262712299823761,
      "average_nll": 0.4262712299823761,
      "perplexity": 1.531536117318183,
      "avg_token_probability": 0.6529392214080224,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_11235--145/145_1446453.txt#0_0",
      "dataset_type": "short",
      "response": "atat\u00fcrk",
      "accuracy": 0.0,
      "g_nll": 0.14346729218959808,
      "average_nll": 0.07173364609479904,
      "perplexity": 1.0743691435331837,
      "avg_token_probability": 0.9307902851211247,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2300--133/133_1682374.txt#0_0",
      "dataset_type": "short",
      "response": "pascal",
      "accuracy": 1.0,
      "g_nll": 0.007752798046567477,
      "average_nll": 0.0038763990232837386,
      "perplexity": 1.0038839219754916,
      "avg_token_probability": 0.9961383557756571,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_9920--21/21_331455.txt#0_0",
      "dataset_type": "short",
      "response": "grand hotel",
      "accuracy": 1.0,
      "g_nll": 0.08903420769638615,
      "average_nll": 0.044517103848193074,
      "perplexity": 1.04552285902179,
      "avg_token_probability": 0.9574021351384552,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2116--1/1_3114498.txt#0_0",
      "dataset_type": "short",
      "response": "sliced bread",
      "accuracy": 1.0,
      "g_nll": 0.04873101832345128,
      "average_nll": 0.02436550916172564,
      "perplexity": 1.0246647738154442,
      "avg_token_probability": 0.9761662731391789,
      "sequence_length": 2
    },
    {
      "example_id": "jp_3234--33/33_1767595.txt#0_0",
      "dataset_type": "short",
      "response": "ireland",
      "accuracy": 0.0,
      "g_nll": 0.5562272071837242,
      "average_nll": 0.2781136035918621,
      "perplexity": 1.320636217705375,
      "avg_token_probability": 0.7866839921351163,
      "sequence_length": 2
    },
    {
      "example_id": "qw_2310--122/122_2696481.txt#0_0",
      "dataset_type": "short",
      "response": "myxomatosis",
      "accuracy": 1.0,
      "g_nll": 0.1328492620855286,
      "average_nll": 0.03321231552138215,
      "perplexity": 1.033770001359702,
      "avg_token_probability": 0.968669468681653,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_7345--158/158_612044.txt#0_2",
      "dataset_type": "short",
      "response": "5",
      "accuracy": 0.0,
      "g_nll": 1.397133013466373,
      "average_nll": 0.6985665067331865,
      "perplexity": 2.0108680745681817,
      "avg_token_probability": 0.6232094275620389,
      "sequence_length": 2
    },
    {
      "example_id": "odql_548--127/127_413228.txt#0_1",
      "dataset_type": "short",
      "response": "los angeles",
      "accuracy": 1.0,
      "g_nll": 0.38902924000284855,
      "average_nll": 0.12967641333428284,
      "perplexity": 1.1384599332612164,
      "avg_token_probability": 0.8923872281017794,
      "sequence_length": 3
    },
    {
      "example_id": "odql_206--73/73_813141.txt#0_1",
      "dataset_type": "short",
      "response": "neptune",
      "accuracy": 1.0,
      "g_nll": 0.4698669033241458,
      "average_nll": 0.2349334516620729,
      "perplexity": 1.26482459395758,
      "avg_token_probability": 0.8124223094688527,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_5388--155/155_1723803.txt#0_0",
      "dataset_type": "short",
      "response": "m69",
      "accuracy": 1.0,
      "g_nll": 0.2277602408430539,
      "average_nll": 0.11388012042152695,
      "perplexity": 1.1206177776447552,
      "avg_token_probability": 0.8980601645656783,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1833--26/26_2841025.txt#0_0",
      "dataset_type": "short",
      "response": "lancashire",
      "accuracy": 1.0,
      "g_nll": 0.004740688600577414,
      "average_nll": 0.002370344300288707,
      "perplexity": 1.0023731557872981,
      "avg_token_probability": 0.9976339489425692,
      "sequence_length": 2
    },
    {
      "example_id": "qz_6268--46/46_250126.txt#0_1",
      "dataset_type": "short",
      "response": "manchester",
      "accuracy": 0.0,
      "g_nll": 0.01754200974937703,
      "average_nll": 0.008771004874688515,
      "perplexity": 1.0088095828446442,
      "avg_token_probability": 0.9913052253038849,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5984--118/118_142159.txt#0_0",
      "dataset_type": "short",
      "response": "ravens",
      "accuracy": 1.0,
      "g_nll": 0.08685930073258419,
      "average_nll": 0.043429650366292094,
      "perplexity": 1.0443865195184965,
      "avg_token_probability": 0.9584030303339355,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_3072--53/53_1526395.txt#0_2",
      "dataset_type": "short",
      "response": "Yves Saint Laurent",
      "accuracy": 1.0,
      "g_nll": 0.43578395538497716,
      "average_nll": 0.10894598884624429,
      "perplexity": 1.11510212076283,
      "avg_token_probability": 0.9079240804627011,
      "sequence_length": 4
    },
    {
      "example_id": "qb_2271--145/145_2891786.txt#0_1",
      "dataset_type": "short",
      "response": "purple",
      "accuracy": 1.0,
      "g_nll": 0.004701512400060892,
      "average_nll": 0.004701512400060892,
      "perplexity": 1.0047125818504055,
      "avg_token_probability": 0.9953095224091589,
      "sequence_length": 1
    },
    {
      "example_id": "qw_3199--48/48_2960705.txt#0_2",
      "dataset_type": "short",
      "response": "atlantic ocean",
      "accuracy": 1.0,
      "g_nll": 1.5240103378246204,
      "average_nll": 0.5080034459415401,
      "perplexity": 1.6619696679595557,
      "avg_token_probability": 0.7352106703865452,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_13847--137/137_1778867.txt#0_1",
      "dataset_type": "short",
      "response": "helly hansen",
      "accuracy": 1.0,
      "g_nll": 0.8560498914507662,
      "average_nll": 0.21401247286269154,
      "perplexity": 1.2386381040601002,
      "avg_token_probability": 0.8408323967590734,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_1224--17/17_461209.txt#0_1",
      "dataset_type": "short",
      "response": "thomas shadwell",
      "accuracy": 0.0,
      "g_nll": 0.31146888522016525,
      "average_nll": 0.06229377704403305,
      "perplexity": 1.064274958357736,
      "avg_token_probability": 0.9464399051508432,
      "sequence_length": 5
    },
    {
      "example_id": "qw_12099--173/173_1273928.txt#0_0",
      "dataset_type": "short",
      "response": "new zealand",
      "accuracy": 1.0,
      "g_nll": 0.24257679842401103,
      "average_nll": 0.08085893280800367,
      "perplexity": 1.0842179381982566,
      "avg_token_probability": 0.9281059704306598,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_24481--77/77_2010082.txt#0_1",
      "dataset_type": "short",
      "response": "mary quant",
      "accuracy": 1.0,
      "g_nll": 0.021727064158767462,
      "average_nll": 0.010863532079383731,
      "perplexity": 1.0109227545046238,
      "avg_token_probability": 0.9892191512679214,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12424--70/70_2829432.txt#0_0",
      "dataset_type": "short",
      "response": "tuna",
      "accuracy": 1.0,
      "g_nll": 0.19592343270778656,
      "average_nll": 0.19592343270778656,
      "perplexity": 1.216433762729154,
      "avg_token_probability": 0.8220751763387677,
      "sequence_length": 1
    },
    {
      "example_id": "qb_404--89/89_2634070.txt#0_2",
      "dataset_type": "short",
      "response": "mallard",
      "accuracy": 1.0,
      "g_nll": 0.06426227069459856,
      "average_nll": 0.03213113534729928,
      "perplexity": 1.0326529137249902,
      "avg_token_probability": 0.9688788390326178,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1626--74/74_774368.txt#0_0",
      "dataset_type": "short",
      "response": "daniel defoe",
      "accuracy": 1.0,
      "g_nll": 2.4749579285617074,
      "average_nll": 0.6187394821404268,
      "perplexity": 1.8565863060598904,
      "avg_token_probability": 0.7701767132152963,
      "sequence_length": 4
    },
    {
      "example_id": "odql_11066--48/48_1410700.txt#0_2",
      "dataset_type": "short",
      "response": "cello",
      "accuracy": 1.0,
      "g_nll": 0.1334017813205719,
      "average_nll": 0.06670089066028595,
      "perplexity": 1.0689756897384024,
      "avg_token_probability": 0.9375567086204382,
      "sequence_length": 2
    },
    {
      "example_id": "bb_320--142/142_2934494.txt#0_0",
      "dataset_type": "short",
      "response": "valley",
      "accuracy": 1.0,
      "g_nll": 0.4430118501186371,
      "average_nll": 0.4430118501186371,
      "perplexity": 1.5573907894980512,
      "avg_token_probability": 0.6420995980862974,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_25624--186/186_3210436.txt#0_1",
      "dataset_type": "short",
      "response": "jenni murray",
      "accuracy": 1.0,
      "g_nll": 0.1044226296899069,
      "average_nll": 0.02088452593798138,
      "perplexity": 1.0211041337873608,
      "avg_token_probability": 0.9796145843840487,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_901--24/24_504588.txt#0_0",
      "dataset_type": "short",
      "response": "leonore",
      "accuracy": 1.0,
      "g_nll": 1.9052434409531998,
      "average_nll": 0.6350811469843999,
      "perplexity": 1.8871752738280438,
      "avg_token_probability": 0.7159564612634872,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_376--165/165_2911834.txt#0_2",
      "dataset_type": "short",
      "response": "Gargantua",
      "accuracy": 1.0,
      "g_nll": 2.210508246876998,
      "average_nll": 0.5526270617192495,
      "perplexity": 1.7378123667436796,
      "avg_token_probability": 0.7759524447373746,
      "sequence_length": 4
    },
    {
      "example_id": "odql_8725--117/117_1962716.txt#0_0",
      "dataset_type": "short",
      "response": "essex county cricket club",
      "accuracy": 0.0,
      "g_nll": 0.8627094859250519,
      "average_nll": 0.1725418971850104,
      "perplexity": 1.1883216069022446,
      "avg_token_probability": 0.8755553648538278,
      "sequence_length": 5
    },
    {
      "example_id": "dpql_2064--32/32_2647072.txt#0_0",
      "dataset_type": "short",
      "response": "dye",
      "accuracy": 1.0,
      "g_nll": 0.30081281065940857,
      "average_nll": 0.30081281065940857,
      "perplexity": 1.350956433223981,
      "avg_token_probability": 0.7402163203838903,
      "sequence_length": 1
    },
    {
      "example_id": "bb_6913--109/109_984790.txt#0_0",
      "dataset_type": "short",
      "response": "blackfriars",
      "accuracy": 1.0,
      "g_nll": 0.0030475214912257798,
      "average_nll": 0.0007618803728064449,
      "perplexity": 1.0007621706773788,
      "avg_token_probability": 0.9992389389267304,
      "sequence_length": 4
    },
    {
      "example_id": "odql_13282--196/196_2145719.txt#0_2",
      "dataset_type": "short",
      "response": "nouakchott",
      "accuracy": 1.0,
      "g_nll": 0.042622059147106484,
      "average_nll": 0.010655514786776621,
      "perplexity": 1.0107124869605977,
      "avg_token_probability": 0.9895644967024755,
      "sequence_length": 4
    },
    {
      "example_id": "qw_6502--97/97_1172999.txt#0_0",
      "dataset_type": "short",
      "response": "china",
      "accuracy": 1.0,
      "g_nll": 0.06367821991443634,
      "average_nll": 0.06367821991443634,
      "perplexity": 1.0657494066451887,
      "avg_token_probability": 0.9383068794266022,
      "sequence_length": 1
    },
    {
      "example_id": "wh_495--37/37_2926588.txt#0_1",
      "dataset_type": "short",
      "response": "tommy roe",
      "accuracy": 1.0,
      "g_nll": 0.13430695600888498,
      "average_nll": 0.033576739002221245,
      "perplexity": 1.034146800074964,
      "avg_token_probability": 0.9677901299377972,
      "sequence_length": 4
    },
    {
      "example_id": "jp_1058--65/65_66511.txt#0_2",
      "dataset_type": "short",
      "response": "spartacus",
      "accuracy": 1.0,
      "g_nll": 0.2084144349610142,
      "average_nll": 0.06947147832033806,
      "perplexity": 1.0719414871970478,
      "avg_token_probability": 0.9372875878043918,
      "sequence_length": 3
    },
    {
      "example_id": "odql_2399--38/38_2808267.txt#0_0",
      "dataset_type": "short",
      "response": "lanzarote",
      "accuracy": 1.0,
      "g_nll": 0.020546124877000693,
      "average_nll": 0.006848708292333565,
      "perplexity": 1.00687221432632,
      "avg_token_probability": 0.9932201506128263,
      "sequence_length": 3
    },
    {
      "example_id": "qz_5580--182/182_233604.txt#0_1",
      "dataset_type": "short",
      "response": "Nero",
      "accuracy": 0.0,
      "g_nll": 0.419251948595047,
      "average_nll": 0.419251948595047,
      "perplexity": 1.5208234758628196,
      "avg_token_probability": 0.6575385084930142,
      "sequence_length": 1
    },
    {
      "example_id": "odql_12354--Lord_Snooty.txt#0_1",
      "dataset_type": "short",
      "response": "lord snooty",
      "accuracy": 1.0,
      "g_nll": 0.18261003151565092,
      "average_nll": 0.04565250787891273,
      "perplexity": 1.0467106240591386,
      "avg_token_probability": 0.9582207295549523,
      "sequence_length": 4
    },
    {
      "example_id": "qw_1849--186/186_2958618.txt#0_0",
      "dataset_type": "short",
      "response": "perigee",
      "accuracy": 1.0,
      "g_nll": 0.7305353936972097,
      "average_nll": 0.24351179789906988,
      "perplexity": 1.2757213685838282,
      "avg_token_probability": 0.8271031021604421,
      "sequence_length": 3
    },
    {
      "example_id": "tb_83--165/165_2048554.txt#0_0",
      "dataset_type": "short",
      "response": "cyclops",
      "accuracy": 1.0,
      "g_nll": 0.03049279935657978,
      "average_nll": 0.01524639967828989,
      "perplexity": 1.0153632189659783,
      "avg_token_probability": 0.9848710862044561,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_13030--New_York_Stadium.txt#0_0",
      "dataset_type": "short",
      "response": "rotherham united",
      "accuracy": 1.0,
      "g_nll": 0.022217675948922988,
      "average_nll": 0.005554418987230747,
      "perplexity": 1.0055698733725014,
      "avg_token_probability": 0.9944903812413862,
      "sequence_length": 4
    },
    {
      "example_id": "qw_8760--191/191_551701.txt#0_1",
      "dataset_type": "short",
      "response": "ronald searle",
      "accuracy": 1.0,
      "g_nll": 0.8288001933742635,
      "average_nll": 0.1657600386748527,
      "perplexity": 1.180289843824183,
      "avg_token_probability": 0.8758042525506635,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_9896--145/145_1575967.txt#0_1",
      "dataset_type": "short",
      "response": "saint cecilia",
      "accuracy": 1.0,
      "g_nll": 0.0928718178181498,
      "average_nll": 0.02321795445453745,
      "perplexity": 1.0234895893541573,
      "avg_token_probability": 0.9773130168784756,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_9525--59/59_129915.txt#0_1",
      "dataset_type": "short",
      "response": "Henri Kontinen",
      "accuracy": 0.0,
      "g_nll": 0.8026419753150549,
      "average_nll": 0.2675473251050183,
      "perplexity": 1.3067554707126483,
      "avg_token_probability": 0.8145278193607052,
      "sequence_length": 3
    },
    {
      "example_id": "odql_3814--123/123_2148942.txt#0_1",
      "dataset_type": "short",
      "response": "3000m",
      "accuracy": 0.0,
      "g_nll": 0.6612921645854613,
      "average_nll": 0.1653230411463653,
      "perplexity": 1.1797741727612292,
      "avg_token_probability": 0.8602266686970919,
      "sequence_length": 4
    },
    {
      "example_id": "qz_2223--157/157_12133.txt#0_0",
      "dataset_type": "short",
      "response": "matthew",
      "accuracy": 1.0,
      "g_nll": 0.04461246661958285,
      "average_nll": 0.022306233309791423,
      "perplexity": 1.0225568775052818,
      "avg_token_probability": 0.97817975067587,
      "sequence_length": 2
    },
    {
      "example_id": "qz_5877--148/148_240409.txt#0_1",
      "dataset_type": "short",
      "response": "tartar",
      "accuracy": 1.0,
      "g_nll": 0.0009635933429308352,
      "average_nll": 0.0004817966714654176,
      "perplexity": 1.0004819127541238,
      "avg_token_probability": 0.9995184292649493,
      "sequence_length": 2
    },
    {
      "example_id": "qw_11886--United_Nations_peacekeeping.txt#0_0",
      "dataset_type": "short",
      "response": "UN peacekeeping",
      "accuracy": 0.0,
      "g_nll": 2.550679426640272,
      "average_nll": 0.8502264755467573,
      "perplexity": 2.340176784732146,
      "avg_token_probability": 0.5199320793928829,
      "sequence_length": 3
    },
    {
      "example_id": "qw_3650--35/35_1121378.txt#0_1",
      "dataset_type": "short",
      "response": "risky business",
      "accuracy": 0.0,
      "g_nll": 0.7059438749420224,
      "average_nll": 0.3529719374710112,
      "perplexity": 1.4232912017225638,
      "avg_token_probability": 0.7467901597950612,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12631--117/117_2313442.txt#0_0",
      "dataset_type": "short",
      "response": "feet",
      "accuracy": 0.0,
      "g_nll": 0.9220975637435913,
      "average_nll": 0.9220975637435913,
      "perplexity": 2.5145593104969,
      "avg_token_probability": 0.3976839980769397,
      "sequence_length": 1
    },
    {
      "example_id": "qf_3656--12/12_934124.txt#0_1",
      "dataset_type": "short",
      "response": "los angeles",
      "accuracy": 1.0,
      "g_nll": 0.3804588541095768,
      "average_nll": 0.1268196180365256,
      "perplexity": 1.1352122275012484,
      "avg_token_probability": 0.8944870496076316,
      "sequence_length": 3
    },
    {
      "example_id": "qf_3202--166/166_863009.txt#0_0",
      "dataset_type": "short",
      "response": "dry ice",
      "accuracy": 1.0,
      "g_nll": 0.013702464231755584,
      "average_nll": 0.006851232115877792,
      "perplexity": 1.0068747554973272,
      "avg_token_probability": 0.993192217267262,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_893--144/144_1486574.txt#0_1",
      "dataset_type": "short",
      "response": "tenerife",
      "accuracy": 1.0,
      "g_nll": 0.06601376085518496,
      "average_nll": 0.03300688042759248,
      "perplexity": 1.0335576505354183,
      "avg_token_probability": 0.9680588503356149,
      "sequence_length": 2
    },
    {
      "example_id": "bb_7225--113/113_991549.txt#0_0",
      "dataset_type": "short",
      "response": "fallopian tube",
      "accuracy": 1.0,
      "g_nll": 0.13851851214008093,
      "average_nll": 0.03462962803502023,
      "perplexity": 1.035236215315105,
      "avg_token_probability": 0.9667174555227855,
      "sequence_length": 4
    },
    {
      "example_id": "qb_5053--95/95_407079.txt#0_0",
      "dataset_type": "short",
      "response": "shoji",
      "accuracy": 1.0,
      "g_nll": 0.00019787258133874275,
      "average_nll": 9.893629066937137e-05,
      "perplexity": 1.0000989411850256,
      "avg_token_probability": 0.9999010711265763,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12695--170/170_2314497.txt#0_0",
      "dataset_type": "short",
      "response": "Hercule Poirot",
      "accuracy": 1.0,
      "g_nll": 0.5929693873565611,
      "average_nll": 0.11859387747131223,
      "perplexity": 1.1259125669655239,
      "avg_token_probability": 0.9096248455933139,
      "sequence_length": 5
    },
    {
      "example_id": "odql_8598--108/108_3212953.txt#0_1",
      "dataset_type": "short",
      "response": "verbal kint and keyser soze",
      "accuracy": 0.0,
      "g_nll": 2.629621309742106,
      "average_nll": 0.29218014552690064,
      "perplexity": 1.3393442727878462,
      "avg_token_probability": 0.8384993635903587,
      "sequence_length": 9
    },
    {
      "example_id": "qg_3271--66/66_2533261.txt#0_0",
      "dataset_type": "short",
      "response": "kentucky derby",
      "accuracy": 1.0,
      "g_nll": 0.05985994477305212,
      "average_nll": 0.019953314924350707,
      "perplexity": 1.0201537129617644,
      "avg_token_probability": 0.980521242635873,
      "sequence_length": 3
    },
    {
      "example_id": "odql_450--14/14_979444.txt#0_1",
      "dataset_type": "short",
      "response": "the blue boy",
      "accuracy": 1.0,
      "g_nll": 0.012783225560269784,
      "average_nll": 0.004261075186756595,
      "perplexity": 1.0042701664759326,
      "avg_token_probability": 0.9957656599900956,
      "sequence_length": 3
    },
    {
      "example_id": "qb_84--151/151_266387.txt#0_0",
      "dataset_type": "short",
      "response": "bees",
      "accuracy": 0.0,
      "g_nll": 0.1250808835029602,
      "average_nll": 0.1250808835029602,
      "perplexity": 1.1332401097897928,
      "avg_token_probability": 0.8824255260303946,
      "sequence_length": 1
    },
    {
      "example_id": "dpql_4205--53/53_60605.txt#0_0",
      "dataset_type": "short",
      "response": "donegal",
      "accuracy": 1.0,
      "g_nll": 3.792566299438363,
      "average_nll": 1.8962831497191814,
      "perplexity": 6.661090099214647,
      "avg_token_probability": 0.511268611511578,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_19313--55/55_1898648.txt#0_1",
      "dataset_type": "short",
      "response": "eros",
      "accuracy": 0.0,
      "g_nll": 0.29375553131103516,
      "average_nll": 0.29375553131103516,
      "perplexity": 1.3414559196077178,
      "avg_token_probability": 0.7454587104825854,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_25425--97/97_2030678.txt#0_2",
      "dataset_type": "short",
      "response": "luxembourg",
      "accuracy": 1.0,
      "g_nll": 0.0298610296449624,
      "average_nll": 0.0149305148224812,
      "perplexity": 1.0150425317547451,
      "avg_token_probability": 0.9852789820772778,
      "sequence_length": 2
    },
    {
      "example_id": "qb_9568--153/153_530377.txt#0_1",
      "dataset_type": "short",
      "response": "george foreman",
      "accuracy": 1.0,
      "g_nll": 0.1794144957360686,
      "average_nll": 0.04485362393401715,
      "perplexity": 1.0458747576712,
      "avg_token_probability": 0.9571167739370735,
      "sequence_length": 4
    },
    {
      "example_id": "tb_2199--49/49_2803159.txt#0_2",
      "dataset_type": "short",
      "response": "chicago cubs",
      "accuracy": 1.0,
      "g_nll": 0.22122091054916382,
      "average_nll": 0.07374030351638794,
      "perplexity": 1.076527198863304,
      "avg_token_probability": 0.9302102096497501,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_971--13/13_577956.txt#0_0",
      "dataset_type": "short",
      "response": "secretary",
      "accuracy": 1.0,
      "g_nll": 0.030980058014392853,
      "average_nll": 0.030980058014392853,
      "perplexity": 1.0314649342225508,
      "avg_token_probability": 0.9694949065367239,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_8322--184/184_1656059.txt#0_0",
      "dataset_type": "short",
      "response": "bud flanagan",
      "accuracy": 1.0,
      "g_nll": 0.09685730932142178,
      "average_nll": 0.03228576977380726,
      "perplexity": 1.0328126097630204,
      "avg_token_probability": 0.9684835142435398,
      "sequence_length": 3
    },
    {
      "example_id": "qb_388--149/149_18617.txt#0_2",
      "dataset_type": "short",
      "response": "Maria Bueno",
      "accuracy": 1.0,
      "g_nll": 2.5338044157560944,
      "average_nll": 0.8446014719186982,
      "perplexity": 2.327050234884034,
      "avg_token_probability": 0.6931015800684919,
      "sequence_length": 3
    },
    {
      "example_id": "qb_1227--60/60_298938.txt#0_0",
      "dataset_type": "short",
      "response": "colette",
      "accuracy": 1.0,
      "g_nll": 0.15602083503563335,
      "average_nll": 0.07801041751781668,
      "perplexity": 1.0811339213433062,
      "avg_token_probability": 0.927770424169473,
      "sequence_length": 2
    },
    {
      "example_id": "qw_5900--159/159_1161564.txt#0_0",
      "dataset_type": "short",
      "response": "australia",
      "accuracy": 1.0,
      "g_nll": 0.24854758381843567,
      "average_nll": 0.24854758381843567,
      "perplexity": 1.28216183107356,
      "avg_token_probability": 0.779932747773887,
      "sequence_length": 1
    },
    {
      "example_id": "bb_1106--42/42_855085.txt#0_0",
      "dataset_type": "short",
      "response": "language",
      "accuracy": 1.0,
      "g_nll": 0.3079836070537567,
      "average_nll": 0.3079836070537567,
      "perplexity": 1.360678683221814,
      "avg_token_probability": 0.7349273655351172,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_5173--26/26_1584174.txt#0_2",
      "dataset_type": "short",
      "response": "iwo jima",
      "accuracy": 1.0,
      "g_nll": 0.07842151982592327,
      "average_nll": 0.019605379956480817,
      "perplexity": 1.0197988275545242,
      "avg_token_probability": 0.9811228019014698,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_2287--128/128_614230.txt#0_1",
      "dataset_type": "short",
      "response": "a crab apple",
      "accuracy": 0.0,
      "g_nll": 0.48435784317553043,
      "average_nll": 0.16145261439184347,
      "perplexity": 1.1752167684776509,
      "avg_token_probability": 0.8564369256419172,
      "sequence_length": 3
    },
    {
      "example_id": "qb_1429--139/139_304888.txt#0_0",
      "dataset_type": "short",
      "response": "la boheme",
      "accuracy": 1.0,
      "g_nll": 0.23546890541911125,
      "average_nll": 0.07848963513970375,
      "perplexity": 1.0816521439308144,
      "avg_token_probability": 0.9258406390248798,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_25263--68/68_2027371.txt#0_0",
      "dataset_type": "short",
      "response": "warbler",
      "accuracy": 1.0,
      "g_nll": 2.038386717438698,
      "average_nll": 1.019193358719349,
      "perplexity": 2.7709586925633936,
      "avg_token_probability": 0.5108255968981572,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14642--115/115_2771986.txt#0_1",
      "dataset_type": "short",
      "response": "charlie brooker",
      "accuracy": 1.0,
      "g_nll": 0.11131409283552784,
      "average_nll": 0.02782852320888196,
      "perplexity": 1.0282193535482191,
      "avg_token_probability": 0.9736381968389582,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1122--112/112_553537.txt#0_3",
      "dataset_type": "short",
      "response": "sardinia",
      "accuracy": 1.0,
      "g_nll": 0.09895932475956215,
      "average_nll": 0.03298644158652072,
      "perplexity": 1.0335365260307414,
      "avg_token_probability": 0.9683499544433106,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_2291--49/49_614335.txt#0_0",
      "dataset_type": "short",
      "response": "hydrochloric acid",
      "accuracy": 1.0,
      "g_nll": 0.10587668559583108,
      "average_nll": 0.02646917139895777,
      "perplexity": 1.0268225912697222,
      "avg_token_probability": 0.9742411840296034,
      "sequence_length": 4
    },
    {
      "example_id": "qw_15997--140/140_2983879.txt#0_1",
      "dataset_type": "short",
      "response": "eric coates",
      "accuracy": 1.0,
      "g_nll": 0.08712708851453499,
      "average_nll": 0.021781772128633747,
      "perplexity": 1.0220207267249295,
      "avg_token_probability": 0.9790467313832517,
      "sequence_length": 4
    },
    {
      "example_id": "odql_9298--52/52_868598.txt#0_0",
      "dataset_type": "short",
      "response": "Volkswagen",
      "accuracy": 1.0,
      "g_nll": 2.086963176727295,
      "average_nll": 2.086963176727295,
      "perplexity": 8.060399948805278,
      "avg_token_probability": 0.12406332270748191,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_11961--3/3_2765982.txt#0_1",
      "dataset_type": "short",
      "response": "les invalides",
      "accuracy": 1.0,
      "g_nll": 0.022969971048496518,
      "average_nll": 0.007656657016165506,
      "perplexity": 1.0076860441690672,
      "avg_token_probability": 0.9924233436982767,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_1626--73/73_1503331.txt#0_1",
      "dataset_type": "short",
      "response": "millais",
      "accuracy": 1.0,
      "g_nll": 0.9275367123918841,
      "average_nll": 0.46376835619594203,
      "perplexity": 1.590054601479946,
      "avg_token_probability": 0.6977135527309388,
      "sequence_length": 2
    },
    {
      "example_id": "odql_5527--13/13_90964.txt#0_1",
      "dataset_type": "short",
      "response": "thylacine",
      "accuracy": 0.0,
      "g_nll": 0.8874106183679942,
      "average_nll": 0.22185265459199854,
      "perplexity": 1.2483874201581875,
      "avg_token_probability": 0.8520756793881253,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_1192--97/97_1493497.txt#0_2",
      "dataset_type": "short",
      "response": "keane",
      "accuracy": 1.0,
      "g_nll": 0.18131561652990058,
      "average_nll": 0.09065780826495029,
      "perplexity": 1.0948942773752586,
      "avg_token_probability": 0.9170614814673899,
      "sequence_length": 2
    },
    {
      "example_id": "qw_4782--187/187_1142011.txt#0_1",
      "dataset_type": "short",
      "response": "wooden clog",
      "accuracy": 1.0,
      "g_nll": 0.10756640788167715,
      "average_nll": 0.035855469293892384,
      "perplexity": 1.036506028716335,
      "avg_token_probability": 0.9652492993944289,
      "sequence_length": 3
    },
    {
      "example_id": "bt_1538--42/42_1542499.txt#0_1",
      "dataset_type": "short",
      "response": "quincy",
      "accuracy": 1.0,
      "g_nll": 0.017044400941813365,
      "average_nll": 0.008522200470906682,
      "perplexity": 1.008558617799754,
      "avg_token_probability": 0.9915461351434702,
      "sequence_length": 2
    },
    {
      "example_id": "bt_3044--15/15_470066.txt#0_1",
      "dataset_type": "short",
      "response": "supertramp",
      "accuracy": 1.0,
      "g_nll": 0.013530982895645138,
      "average_nll": 0.0045103276318817125,
      "perplexity": 1.0045205144691216,
      "avg_token_probability": 0.9955198335798466,
      "sequence_length": 3
    },
    {
      "example_id": "qw_16785--192/192_1349169.txt#0_1",
      "dataset_type": "short",
      "response": "1880",
      "accuracy": 0.0,
      "g_nll": 0.18232857927068835,
      "average_nll": 0.06077619309022945,
      "perplexity": 1.0626610566838819,
      "avg_token_probability": 0.9424974136243834,
      "sequence_length": 3
    },
    {
      "example_id": "jp_3059--26/26_1399947.txt#0_1",
      "dataset_type": "short",
      "response": "judas iscariot",
      "accuracy": 1.0,
      "g_nll": 1.8153765202559953,
      "average_nll": 0.3025627533759992,
      "perplexity": 1.3533225993184683,
      "avg_token_probability": 0.8211048385395269,
      "sequence_length": 6
    },
    {
      "example_id": "qf_3128--50/50_3337.txt#0_0",
      "dataset_type": "short",
      "response": "robin williams",
      "accuracy": 1.0,
      "g_nll": 0.04623152763815597,
      "average_nll": 0.015410509212718656,
      "perplexity": 1.0155298634247172,
      "avg_token_probability": 0.9849321831548187,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_455--142/142_545804.txt#0_0",
      "dataset_type": "short",
      "response": "cato",
      "accuracy": 1.0,
      "g_nll": 0.09669073670625039,
      "average_nll": 0.048345368353125195,
      "perplexity": 1.0495330682452217,
      "avg_token_probability": 0.9539182188267143,
      "sequence_length": 2
    },
    {
      "example_id": "odql_7977--27/27_2226595.txt#0_0",
      "dataset_type": "short",
      "response": "gollum",
      "accuracy": 0.0,
      "g_nll": 0.09077773051831173,
      "average_nll": 0.03025924350610391,
      "perplexity": 1.0307217072297516,
      "avg_token_probability": 0.9708222448183372,
      "sequence_length": 3
    },
    {
      "example_id": "bb_618--61/61_2669551.txt#0_1",
      "dataset_type": "short",
      "response": "elephants",
      "accuracy": 1.0,
      "g_nll": 0.00862787663936615,
      "average_nll": 0.00862787663936615,
      "perplexity": 1.0086652040418636,
      "avg_token_probability": 0.9914092366752211,
      "sequence_length": 1
    },
    {
      "example_id": "odql_14838--54/54_107845.txt#0_2",
      "dataset_type": "short",
      "response": "wednesday",
      "accuracy": 1.0,
      "g_nll": 1.9965439985971898,
      "average_nll": 0.9982719992985949,
      "perplexity": 2.713588691592553,
      "avg_token_probability": 0.5671708149945892,
      "sequence_length": 2
    },
    {
      "example_id": "qb_6250--52/52_439893.txt#0_0",
      "dataset_type": "short",
      "response": "saint moritz",
      "accuracy": 1.0,
      "g_nll": 0.6513184046634706,
      "average_nll": 0.21710613488782352,
      "perplexity": 1.2424759651871937,
      "avg_token_probability": 0.8401153651406331,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2359--127/127_71063.txt#0_0",
      "dataset_type": "short",
      "response": "john buchan",
      "accuracy": 1.0,
      "g_nll": 0.08010495861890377,
      "average_nll": 0.026701652872967923,
      "perplexity": 1.0270613362501049,
      "avg_token_probability": 0.9740264227104473,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9139--113/113_1038621.txt#0_1",
      "dataset_type": "short",
      "response": "desmond tutu",
      "accuracy": 1.0,
      "g_nll": 0.05567728412154338,
      "average_nll": 0.013919321030385845,
      "perplexity": 1.0140166458200812,
      "avg_token_probability": 0.9864544625264952,
      "sequence_length": 4
    },
    {
      "example_id": "qw_1719--97/97_1084406.txt#0_1",
      "dataset_type": "short",
      "response": "france",
      "accuracy": 1.0,
      "g_nll": 0.026294874027371407,
      "average_nll": 0.026294874027371407,
      "perplexity": 1.0266436343872887,
      "avg_token_probability": 0.974047825852259,
      "sequence_length": 1
    },
    {
      "example_id": "qb_2899--155/155_2893117.txt#0_2",
      "dataset_type": "short",
      "response": "Louis XV",
      "accuracy": 1.0,
      "g_nll": 4.335478834807873,
      "average_nll": 2.1677394174039364,
      "perplexity": 8.738507577852724,
      "avg_token_probability": 0.46757475470684456,
      "sequence_length": 2
    },
    {
      "example_id": "bt_2646--138/138_2413763.txt#0_2",
      "dataset_type": "short",
      "response": "ambergris",
      "accuracy": 1.0,
      "g_nll": 0.00039522548286186066,
      "average_nll": 0.00013174182762062023,
      "perplexity": 1.0001317505059564,
      "avg_token_probability": 0.9998682823400907,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_17367--193/193_1858126.txt#0_0",
      "dataset_type": "short",
      "response": "franklin d. roosevelt",
      "accuracy": 0.0,
      "g_nll": 0.7016414731187979,
      "average_nll": 0.10023449615982827,
      "perplexity": 1.105430106800083,
      "avg_token_probability": 0.916747214748942,
      "sequence_length": 7
    },
    {
      "example_id": "dpql_151--178/178_231745.txt#0_2",
      "dataset_type": "short",
      "response": "denali",
      "accuracy": 1.0,
      "g_nll": 0.546026587464894,
      "average_nll": 0.273013293732447,
      "perplexity": 1.313917711579155,
      "avg_token_probability": 0.7896220359207307,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_2345--98/98_1465999.txt#0_0",
      "dataset_type": "short",
      "response": "jaguar",
      "accuracy": 0.0,
      "g_nll": 0.059699451845517615,
      "average_nll": 0.029849725922758807,
      "perplexity": 1.030299694983721,
      "avg_token_probability": 0.9710221889386483,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11051--136/136_235136.txt#0_2",
      "dataset_type": "short",
      "response": "$100",
      "accuracy": 0.0,
      "g_nll": 0.12316697090864182,
      "average_nll": 0.06158348545432091,
      "perplexity": 1.0635192812130043,
      "avg_token_probability": 0.9405957042765967,
      "sequence_length": 2
    },
    {
      "example_id": "tb_47--Hansel_and_Gretel.txt#0_0",
      "dataset_type": "short",
      "response": "user-interface",
      "accuracy": 0.0,
      "g_nll": 4.387370228767395,
      "average_nll": 2.1936851143836975,
      "perplexity": 8.968201142064567,
      "avg_token_probability": 0.14293157108532487,
      "sequence_length": 2
    },
    {
      "example_id": "qb_7858--119/119_484438.txt#0_2",
      "dataset_type": "short",
      "response": "yahoo",
      "accuracy": 1.0,
      "g_nll": 0.03113757073879242,
      "average_nll": 0.03113757073879242,
      "perplexity": 1.0316274158705905,
      "avg_token_probability": 0.9693422107787819,
      "sequence_length": 1
    },
    {
      "example_id": "qg_1094--4/4_3215155.txt#0_0",
      "dataset_type": "short",
      "response": "eat me",
      "accuracy": 1.0,
      "g_nll": 0.23534753895364702,
      "average_nll": 0.11767376947682351,
      "perplexity": 1.124877082263493,
      "avg_token_probability": 0.8950246036765916,
      "sequence_length": 2
    },
    {
      "example_id": "bb_1960--194/194_648540.txt#0_1",
      "dataset_type": "short",
      "response": "the church",
      "accuracy": 0.0,
      "g_nll": 0.9821865633130074,
      "average_nll": 0.4910932816565037,
      "perplexity": 1.6341017772169188,
      "avg_token_probability": 0.6593329510585978,
      "sequence_length": 2
    },
    {
      "example_id": "odql_11893--64/64_768.txt#0_0",
      "dataset_type": "short",
      "response": "joseph goebbels",
      "accuracy": 1.0,
      "g_nll": 0.5709959003143013,
      "average_nll": 0.11419918006286026,
      "perplexity": 1.1209753785959442,
      "avg_token_probability": 0.909899323109167,
      "sequence_length": 5
    },
    {
      "example_id": "jp_4375--83/83_2736108.txt#0_0",
      "dataset_type": "short",
      "response": "yosemite",
      "accuracy": 1.0,
      "g_nll": 0.0192353246093262,
      "average_nll": 0.0096176623046631,
      "perplexity": 1.0096640606473366,
      "avg_token_probability": 0.990472990683891,
      "sequence_length": 2
    },
    {
      "example_id": "jp_553--17/17_1364047.txt#0_2",
      "dataset_type": "short",
      "response": "chechnya",
      "accuracy": 1.0,
      "g_nll": 0.10390004218788818,
      "average_nll": 0.03463334739596272,
      "perplexity": 1.035240065739411,
      "avg_token_probability": 0.9668209255973341,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_14830--197/197_1800498.txt#0_1",
      "dataset_type": "short",
      "response": "the bWF world cup",
      "accuracy": 0.0,
      "g_nll": 4.666713178157806,
      "average_nll": 0.9333426356315613,
      "perplexity": 2.54299529339854,
      "avg_token_probability": 0.5809549065697135,
      "sequence_length": 5
    },
    {
      "example_id": "sfq_2452--77/77_2081353.txt#0_2",
      "dataset_type": "short",
      "response": "toyo",
      "accuracy": 0.0,
      "g_nll": 4.50820004940033,
      "average_nll": 2.254100024700165,
      "perplexity": 9.526715642114585,
      "avg_token_probability": 0.13989536446123504,
      "sequence_length": 2
    },
    {
      "example_id": "qw_1155--186/186_1072791.txt#0_0",
      "dataset_type": "short",
      "response": "albatross",
      "accuracy": 0.0,
      "g_nll": 0.6071838140445607,
      "average_nll": 0.20239460468152023,
      "perplexity": 1.2243310395617961,
      "avg_token_probability": 0.8482938584005054,
      "sequence_length": 3
    },
    {
      "example_id": "qb_250--Peter_Blake_(artist).txt#0_0",
      "dataset_type": "short",
      "response": "peter blake",
      "accuracy": 1.0,
      "g_nll": 0.03269174785418727,
      "average_nll": 0.01089724928472909,
      "perplexity": 1.0109568405693659,
      "avg_token_probability": 0.9892195249506234,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23994--31/31_726041.txt#0_0",
      "dataset_type": "short",
      "response": "hugo von hofmannsthal",
      "accuracy": 0.0,
      "g_nll": 3.0556815675449798,
      "average_nll": 0.38196019594312247,
      "perplexity": 1.465153764908484,
      "avg_token_probability": 0.8660502604818595,
      "sequence_length": 8
    },
    {
      "example_id": "qw_12027--75/75_2714729.txt#0_0",
      "dataset_type": "short",
      "response": "alzheimer's disease",
      "accuracy": 0.0,
      "g_nll": 0.1565234325826168,
      "average_nll": 0.0391308581456542,
      "perplexity": 1.0399065549903816,
      "avg_token_probability": 0.96211632453476,
      "sequence_length": 4
    },
    {
      "example_id": "qb_7632--22/22_167620.txt#0_1",
      "dataset_type": "short",
      "response": "Leroy",
      "accuracy": 1.0,
      "g_nll": 2.860902190208428,
      "average_nll": 1.430451095104214,
      "perplexity": 4.18058460788969,
      "avg_token_probability": 0.5286085019310544,
      "sequence_length": 2
    },
    {
      "example_id": "qz_2001--6/6_147605.txt#0_1",
      "dataset_type": "short",
      "response": "pablo neruda",
      "accuracy": 1.0,
      "g_nll": 0.028748393580826814,
      "average_nll": 0.007187098395206704,
      "perplexity": 1.0072129875724012,
      "avg_token_probability": 0.9928672116471504,
      "sequence_length": 4
    },
    {
      "example_id": "qw_2616--7/7_311565.txt#0_0",
      "dataset_type": "short",
      "response": "ryder cup",
      "accuracy": 1.0,
      "g_nll": 1.7550525973856566,
      "average_nll": 0.5850175324618855,
      "perplexity": 1.795022456526821,
      "avg_token_probability": 0.7241719551439609,
      "sequence_length": 3
    },
    {
      "example_id": "jp_2085--58/58_325202.txt#0_1",
      "dataset_type": "short",
      "response": "Sweeney Todd",
      "accuracy": 1.0,
      "g_nll": 1.6465674028731883,
      "average_nll": 0.5488558009577295,
      "perplexity": 1.7312709655717484,
      "avg_token_probability": 0.6880043440123985,
      "sequence_length": 3
    },
    {
      "example_id": "qb_6583--36/36_449032.txt#0_2",
      "dataset_type": "short",
      "response": "football",
      "accuracy": 1.0,
      "g_nll": 0.02367105521261692,
      "average_nll": 0.02367105521261692,
      "perplexity": 1.0239534383402082,
      "avg_token_probability": 0.9766069066782609,
      "sequence_length": 1
    },
    {
      "example_id": "bb_8929--172/172_1033394.txt#0_0",
      "dataset_type": "short",
      "response": "william bligh",
      "accuracy": 1.0,
      "g_nll": 1.0475030690166705,
      "average_nll": 0.2618757672541676,
      "perplexity": 1.2993651087066187,
      "avg_token_probability": 0.8369364477876402,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_1783--160/160_2646247.txt#0_0",
      "dataset_type": "short",
      "response": "smallpox",
      "accuracy": 1.0,
      "g_nll": 0.010078913619508967,
      "average_nll": 0.0033596378731696555,
      "perplexity": 1.003365287781933,
      "avg_token_probability": 0.9966560337843852,
      "sequence_length": 3
    },
    {
      "example_id": "qb_3092--129/129_139741.txt#0_0",
      "dataset_type": "short",
      "response": "22 months",
      "accuracy": 1.0,
      "g_nll": 1.3845301860710606,
      "average_nll": 0.4615100620236869,
      "perplexity": 1.5864678419432765,
      "avg_token_probability": 0.7433763950389363,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_20082--77/77_957747.txt#0_1",
      "dataset_type": "short",
      "response": "volkswagen",
      "accuracy": 0.0,
      "g_nll": 0.7373145569581538,
      "average_nll": 0.2457715189860513,
      "perplexity": 1.2786074026494587,
      "avg_token_probability": 0.8256422547524479,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_4588--56/56_232596.txt#0_1",
      "dataset_type": "short",
      "response": "swift",
      "accuracy": 0.0,
      "g_nll": 0.9463073015213013,
      "average_nll": 0.9463073015213013,
      "perplexity": 2.576179020907507,
      "avg_token_probability": 0.38817178149666454,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_1303--47/47_1508907.txt#0_1",
      "dataset_type": "short",
      "response": "cranmer",
      "accuracy": 1.0,
      "g_nll": 2.355347394942555,
      "average_nll": 1.1776736974712776,
      "perplexity": 3.2468123429492293,
      "avg_token_probability": 0.5474297338634102,
      "sequence_length": 2
    },
    {
      "example_id": "odql_3698--78/78_264520.txt#0_1",
      "dataset_type": "short",
      "response": "mares tails",
      "accuracy": 0.0,
      "g_nll": 0.6622739135864322,
      "average_nll": 0.22075797119547738,
      "perplexity": 1.2470215788951853,
      "avg_token_probability": 0.8122976472687918,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_1807--198/198_1507629.txt#0_1",
      "dataset_type": "short",
      "response": "kempton park",
      "accuracy": 1.0,
      "g_nll": 0.1143415030092001,
      "average_nll": 0.03811383433640003,
      "perplexity": 1.0388494828894186,
      "avg_token_probability": 0.9632833528155252,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_20006--59/59_2784672.txt#0_0",
      "dataset_type": "short",
      "response": "colchester",
      "accuracy": 1.0,
      "g_nll": 4.874771726312929,
      "average_nll": 1.6249239087709764,
      "perplexity": 5.078032628735378,
      "avg_token_probability": 0.6672286635983498,
      "sequence_length": 3
    },
    {
      "example_id": "bb_9463--32/32_2691228.txt#0_2",
      "dataset_type": "short",
      "response": "fruits and seeds",
      "accuracy": 1.0,
      "g_nll": 0.1888254885780043,
      "average_nll": 0.06294182952600143,
      "perplexity": 1.0649648879171323,
      "avg_token_probability": 0.940788407505528,
      "sequence_length": 3
    },
    {
      "example_id": "bb_2276--42/42_883302.txt#0_1",
      "dataset_type": "short",
      "response": "scotland",
      "accuracy": 1.0,
      "g_nll": 0.004846216179430485,
      "average_nll": 0.0024231080897152424,
      "perplexity": 1.0024260461887537,
      "avg_token_probability": 0.9975802819270281,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_25007--68/68_2021815.txt#0_1",
      "dataset_type": "short",
      "response": "faversham",
      "accuracy": 1.0,
      "g_nll": 0.501298305164255,
      "average_nll": 0.16709943505475167,
      "perplexity": 1.1818717789508264,
      "avg_token_probability": 0.8685702406904655,
      "sequence_length": 3
    },
    {
      "example_id": "qb_574--35/35_280986.txt#0_1",
      "dataset_type": "short",
      "response": "japan",
      "accuracy": 0.0,
      "g_nll": 2.288820266723633,
      "average_nll": 2.288820266723633,
      "perplexity": 9.86329475771113,
      "avg_token_probability": 0.10138599976627481,
      "sequence_length": 1
    },
    {
      "example_id": "qw_6881--133/133_2704872.txt#0_2",
      "dataset_type": "short",
      "response": "kopassus",
      "accuracy": 0.0,
      "g_nll": 0.032278492018349425,
      "average_nll": 0.010759497339449808,
      "perplexity": 1.0108175888892992,
      "avg_token_probability": 0.9894117965468898,
      "sequence_length": 3
    },
    {
      "example_id": "bb_7700--19/19_416201.txt#0_1",
      "dataset_type": "short",
      "response": "victor hugo",
      "accuracy": 1.0,
      "g_nll": 1.75021609889518,
      "average_nll": 0.437554024723795,
      "perplexity": 1.5489139760235258,
      "avg_token_probability": 0.7578334306197134,
      "sequence_length": 4
    },
    {
      "example_id": "qb_3569--170/170_366697.txt#0_0",
      "dataset_type": "short",
      "response": "popeye's father",
      "accuracy": 0.0,
      "g_nll": 1.654901369009167,
      "average_nll": 0.41372534225229174,
      "perplexity": 1.5124416660107058,
      "avg_token_probability": 0.7034598306217762,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_8812--24/24_1020332.txt#0_0",
      "dataset_type": "short",
      "response": "capricorn",
      "accuracy": 1.0,
      "g_nll": 0.014391468836379318,
      "average_nll": 0.004797156278793106,
      "perplexity": 1.0048086810543218,
      "avg_token_probability": 0.9952371869981865,
      "sequence_length": 3
    },
    {
      "example_id": "qw_597--13/13_1061577.txt#0_1",
      "dataset_type": "short",
      "response": "pygmalion",
      "accuracy": 1.0,
      "g_nll": 0.02098834139860628,
      "average_nll": 0.00524708534965157,
      "perplexity": 1.0052608754106438,
      "avg_token_probability": 0.9947818961972292,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_16522--56/56_1003309.txt#0_0",
      "dataset_type": "short",
      "response": "sir edwin landseer",
      "accuracy": 1.0,
      "g_nll": 0.2592289188978043,
      "average_nll": 0.04320481981630072,
      "perplexity": 1.0441517359171617,
      "avg_token_probability": 0.9611842927069368,
      "sequence_length": 6
    },
    {
      "example_id": "jp_3307--148/148_1403703.txt#0_0",
      "dataset_type": "short",
      "response": "subfamily",
      "accuracy": 0.0,
      "g_nll": 0.7161413431167603,
      "average_nll": 0.3580706715583801,
      "perplexity": 1.4305667172867989,
      "avg_token_probability": 0.6997422724043529,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_10250--131/131_472528.txt#0_0",
      "dataset_type": "short",
      "response": "rat",
      "accuracy": 0.0,
      "g_nll": 0.03370201960206032,
      "average_nll": 0.03370201960206032,
      "perplexity": 1.0342763667223234,
      "avg_token_probability": 0.9668595669154203,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_22454--52/52_1966691.txt#0_0",
      "dataset_type": "short",
      "response": "para handy",
      "accuracy": 1.0,
      "g_nll": 6.355922102928162,
      "average_nll": 2.1186407009760537,
      "perplexity": 8.319820673841823,
      "avg_token_probability": 0.37102739115032346,
      "sequence_length": 3
    },
    {
      "example_id": "tc_69--158/158_2486.txt#0_1",
      "dataset_type": "short",
      "response": "octopussy",
      "accuracy": 1.0,
      "g_nll": 0.9021190278981521,
      "average_nll": 0.3007063426327174,
      "perplexity": 1.3508126072149613,
      "avg_token_probability": 0.8018648841190418,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8861--190/190_510847.txt#0_1",
      "dataset_type": "short",
      "response": "litas",
      "accuracy": 1.0,
      "g_nll": 0.09407021072729549,
      "average_nll": 0.047035105363647745,
      "perplexity": 1.048158804429785,
      "avg_token_probability": 0.9551082815662839,
      "sequence_length": 2
    },
    {
      "example_id": "qf_530--164/164_2459775.txt#0_0",
      "dataset_type": "short",
      "response": "tetanus",
      "accuracy": 1.0,
      "g_nll": 0.002629497721500229,
      "average_nll": 0.0013147488607501145,
      "perplexity": 1.0013156135219294,
      "avg_token_probability": 0.9986869276141243,
      "sequence_length": 2
    },
    {
      "example_id": "wh_299--Pocketful_of_Miracles.txt#0_1",
      "dataset_type": "short",
      "response": "peter falk",
      "accuracy": 1.0,
      "g_nll": 0.08891008228965802,
      "average_nll": 0.029636694096552674,
      "perplexity": 1.0300802317353173,
      "avg_token_probability": 0.9711576063112325,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_22936--27/27_1976951.txt#0_0",
      "dataset_type": "short",
      "response": "cthulhu",
      "accuracy": 0.0,
      "g_nll": 5.241392929165158,
      "average_nll": 1.3103482322912896,
      "perplexity": 3.7074645463159337,
      "avg_token_probability": 0.6756679909603278,
      "sequence_length": 4
    },
    {
      "example_id": "dpql_4975--25/25_686725.txt#0_0",
      "dataset_type": "short",
      "response": "costa brava",
      "accuracy": 1.0,
      "g_nll": 0.026761399931103824,
      "average_nll": 0.008920466643701275,
      "perplexity": 1.0089603725778626,
      "avg_token_probability": 0.9911842696676691,
      "sequence_length": 3
    },
    {
      "example_id": "odql_7656--38/38_144986.txt#0_0",
      "dataset_type": "short",
      "response": "bellona",
      "accuracy": 1.0,
      "g_nll": 5.609603639584748,
      "average_nll": 1.8698678798615826,
      "perplexity": 6.487439221294829,
      "avg_token_probability": 0.656661069737247,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_5542--101/101_701718.txt#0_2",
      "dataset_type": "short",
      "response": "Annie",
      "accuracy": 1.0,
      "g_nll": 0.47154927253723145,
      "average_nll": 0.47154927253723145,
      "perplexity": 1.602474941462005,
      "avg_token_probability": 0.6240347191249419,
      "sequence_length": 1
    },
    {
      "example_id": "qg_1795--111/111_375646.txt#0_1",
      "dataset_type": "short",
      "response": "benjamin franklin",
      "accuracy": 1.0,
      "g_nll": 0.12537266884396558,
      "average_nll": 0.031343167210991396,
      "perplexity": 1.0318395366328101,
      "avg_token_probability": 0.9704554246462848,
      "sequence_length": 4
    },
    {
      "example_id": "qw_13284--106/106_96340.txt#0_1",
      "dataset_type": "short",
      "response": "canada",
      "accuracy": 0.0,
      "g_nll": 6.4991302490234375,
      "average_nll": 6.4991302490234375,
      "perplexity": 664.5633769653165,
      "avg_token_probability": 0.001504747379499653,
      "sequence_length": 1
    },
    {
      "example_id": "qw_11437--137/137_654439.txt#0_0",
      "dataset_type": "short",
      "response": "groucho marx",
      "accuracy": 0.0,
      "g_nll": 2.5161186157959037,
      "average_nll": 0.6290296539489759,
      "perplexity": 1.8757895309198152,
      "avg_token_probability": 0.7372520745761826,
      "sequence_length": 4
    },
    {
      "example_id": "jp_2683--186/186_1418025.txt#0_2",
      "dataset_type": "short",
      "response": "Kix",
      "accuracy": 1.0,
      "g_nll": 2.6342280209064484,
      "average_nll": 1.3171140104532242,
      "perplexity": 3.732633476668724,
      "avg_token_probability": 0.4673665976904581,
      "sequence_length": 2
    },
    {
      "example_id": "bt_1265--Alan_Lake.txt#0_0",
      "dataset_type": "short",
      "response": "diana dors",
      "accuracy": 1.0,
      "g_nll": 1.7858400125187472,
      "average_nll": 0.4464600031296868,
      "perplexity": 1.5627701803897194,
      "avg_token_probability": 0.773483844698372,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_18204--53/53_1875473.txt#0_1",
      "dataset_type": "short",
      "response": "nicolas poussin",
      "accuracy": 1.0,
      "g_nll": 0.10279495047029741,
      "average_nll": 0.02055899009405948,
      "perplexity": 1.0207717818905988,
      "avg_token_probability": 0.9804559530247147,
      "sequence_length": 5
    },
    {
      "example_id": "qz_3752--50/50_191237.txt#0_0",
      "dataset_type": "short",
      "response": "michael holding",
      "accuracy": 1.0,
      "g_nll": 0.02912757203739602,
      "average_nll": 0.01456378601869801,
      "perplexity": 1.0146703546694853,
      "avg_token_probability": 0.9856438359824521,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_1447--149/149_591689.txt#0_0",
      "dataset_type": "short",
      "response": "cadbury",
      "accuracy": 0.0,
      "g_nll": 2.38366585271433,
      "average_nll": 1.191832926357165,
      "perplexity": 3.293111710211714,
      "avg_token_probability": 0.5431922652075439,
      "sequence_length": 2
    },
    {
      "example_id": "qf_3026--170/170_1354094.txt#0_0",
      "dataset_type": "short",
      "response": "2",
      "accuracy": 0.0,
      "g_nll": 0.7081725299358368,
      "average_nll": 0.3540862649679184,
      "perplexity": 1.424878098241755,
      "avg_token_probability": 0.713168561104166,
      "sequence_length": 2
    },
    {
      "example_id": "qf_220--100/100_733704.txt#0_0",
      "dataset_type": "short",
      "response": "michael miles",
      "accuracy": 1.0,
      "g_nll": 0.07560286112129688,
      "average_nll": 0.03780143056064844,
      "perplexity": 1.0385249930770866,
      "avg_token_probability": 0.9630201178622992,
      "sequence_length": 2
    },
    {
      "example_id": "qb_804--33/33_287054.txt#0_0",
      "dataset_type": "short",
      "response": "leprosy",
      "accuracy": 1.0,
      "g_nll": 0.10148259039124241,
      "average_nll": 0.03382753013041414,
      "perplexity": 1.0344061874423378,
      "avg_token_probability": 0.9678233389320935,
      "sequence_length": 3
    },
    {
      "example_id": "odql_12312--173/173_2307375.txt#0_0",
      "dataset_type": "short",
      "response": "turkey",
      "accuracy": 1.0,
      "g_nll": 0.004220388829708099,
      "average_nll": 0.004220388829708099,
      "perplexity": 1.0042293072125792,
      "avg_token_probability": 0.9957885044957329,
      "sequence_length": 1
    },
    {
      "example_id": "bb_7976--4/4_843121.txt#0_0",
      "dataset_type": "short",
      "response": "E",
      "accuracy": 1.0,
      "g_nll": 1.9221630096435547,
      "average_nll": 1.9221630096435547,
      "perplexity": 6.835728236053583,
      "avg_token_probability": 0.14629019256876163,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_13707--150/150_14358.txt#0_0",
      "dataset_type": "short",
      "response": "ambulance driver",
      "accuracy": 1.0,
      "g_nll": 0.12393288407474756,
      "average_nll": 0.06196644203737378,
      "perplexity": 1.0639266409185104,
      "avg_token_probability": 0.9413279508857544,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12262--143/143_282014.txt#0_1",
      "dataset_type": "short",
      "response": "cold comfort farm",
      "accuracy": 0.0,
      "g_nll": 0.12609070591861382,
      "average_nll": 0.04203023530620461,
      "perplexity": 1.0429260114613654,
      "avg_token_probability": 0.9604917099362259,
      "sequence_length": 3
    },
    {
      "example_id": "qb_8177--179/179_12992.txt#0_1",
      "dataset_type": "short",
      "response": "Gerald Ford",
      "accuracy": 0.0,
      "g_nll": 1.0532936453819275,
      "average_nll": 0.5266468226909637,
      "perplexity": 1.6932450279108973,
      "avg_token_probability": 0.6121026503897967,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24363--72/72_2007380.txt#0_0",
      "dataset_type": "short",
      "response": "john constable",
      "accuracy": 1.0,
      "g_nll": 0.019016988546354696,
      "average_nll": 0.006338996182118232,
      "perplexity": 1.006359130138959,
      "avg_token_probability": 0.9936935057651802,
      "sequence_length": 3
    },
    {
      "example_id": "odql_4497--97/97_2162315.txt#0_1",
      "dataset_type": "short",
      "response": "1960",
      "accuracy": 1.0,
      "g_nll": 0.06836565397679806,
      "average_nll": 0.022788551325599354,
      "perplexity": 1.0230501940676602,
      "avg_token_probability": 0.9777802300862094,
      "sequence_length": 3
    },
    {
      "example_id": "odql_1097--173/173_3062526.txt#0_0",
      "dataset_type": "short",
      "response": "james hanratty",
      "accuracy": 0.0,
      "g_nll": 0.043150092447376664,
      "average_nll": 0.010787523111844166,
      "perplexity": 1.0108459182299516,
      "avg_token_probability": 0.9893981125010576,
      "sequence_length": 4
    },
    {
      "example_id": "sfq_2358--33/33_1520459.txt#0_0",
      "dataset_type": "short",
      "response": "environment",
      "accuracy": 0.0,
      "g_nll": 1.4616612195968628,
      "average_nll": 1.4616612195968628,
      "perplexity": 4.313118617460176,
      "avg_token_probability": 0.2318507995471871,
      "sequence_length": 1
    },
    {
      "example_id": "qb_4017--123/123_379315.txt#0_2",
      "dataset_type": "short",
      "response": "privet",
      "accuracy": 1.0,
      "g_nll": 0.0012421458850440104,
      "average_nll": 0.0006210729425220052,
      "perplexity": 1.000621265848256,
      "avg_token_probability": 0.9993792918392461,
      "sequence_length": 2
    },
    {
      "example_id": "qg_3182--3/3_2567423.txt#0_0",
      "dataset_type": "short",
      "response": "pirate day",
      "accuracy": 1.0,
      "g_nll": 1.5245026238262653,
      "average_nll": 0.7622513119131327,
      "perplexity": 2.1430955701196663,
      "avg_token_probability": 0.601905171168249,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1819--183/183_300683.txt#0_0",
      "dataset_type": "short",
      "response": "gordon ramsay",
      "accuracy": 1.0,
      "g_nll": 0.06751842555263465,
      "average_nll": 0.013503685110526931,
      "perplexity": 1.0135952716539294,
      "avg_token_probability": 0.9868756858855742,
      "sequence_length": 5
    },
    {
      "example_id": "bb_3277--172/172_905779.txt#0_0",
      "dataset_type": "short",
      "response": "zygote",
      "accuracy": 1.0,
      "g_nll": 0.028560068284605222,
      "average_nll": 0.009520022761535074,
      "perplexity": 1.0095654823223918,
      "avg_token_probability": 0.990614398084359,
      "sequence_length": 3
    },
    {
      "example_id": "sfq_23794--112/112_1995157.txt#0_0",
      "dataset_type": "short",
      "response": "speedway",
      "accuracy": 1.0,
      "g_nll": 0.024320451491803396,
      "average_nll": 0.012160225745901698,
      "perplexity": 1.0122344618952681,
      "avg_token_probability": 0.9879851834494173,
      "sequence_length": 2
    },
    {
      "example_id": "jp_1511--132/132_1375093.txt#0_0",
      "dataset_type": "short",
      "response": "negro",
      "accuracy": 0.0,
      "g_nll": 0.6150028109550476,
      "average_nll": 0.6150028109550476,
      "perplexity": 1.8496617988671895,
      "avg_token_probability": 0.5406393755941988,
      "sequence_length": 1
    },
    {
      "example_id": "qz_127--19/19_99793.txt#0_2",
      "dataset_type": "short",
      "response": "red",
      "accuracy": 0.0,
      "g_nll": 2.1736600399017334,
      "average_nll": 2.1736600399017334,
      "perplexity": 8.79039844394916,
      "avg_token_probability": 0.11376048610041636,
      "sequence_length": 1
    },
    {
      "example_id": "qb_5539--18/18_3229.txt#0_0",
      "dataset_type": "short",
      "response": "bahrain",
      "accuracy": 1.0,
      "g_nll": 0.022386854512433274,
      "average_nll": 0.011193427256216637,
      "perplexity": 1.0112563080613153,
      "avg_token_probability": 0.9889309133662342,
      "sequence_length": 2
    },
    {
      "example_id": "odql_10409--184/184_5652.txt#0",
      "dataset_type": "short",
      "response": "burkina faso",
      "accuracy": 1.0,
      "g_nll": 0.045199779029637455,
      "average_nll": 0.00903995580592749,
      "perplexity": 1.0090809396105849,
      "avg_token_probability": 0.9910735213111519,
      "sequence_length": 5
    },
    {
      "example_id": "bb_1013--62/62_852710.txt#0_0",
      "dataset_type": "short",
      "response": "Concepcion",
      "accuracy": 1.0,
      "g_nll": 0.22901933046523482,
      "average_nll": 0.11450966523261741,
      "perplexity": 1.1213234788638502,
      "avg_token_probability": 0.8975173352824715,
      "sequence_length": 2
    },
    {
      "example_id": "odql_2420--173/173_2123595.txt#0_1",
      "dataset_type": "short",
      "response": "maiquetia",
      "accuracy": 0.0,
      "g_nll": 0.21871384649421088,
      "average_nll": 0.0729046154980703,
      "perplexity": 1.0756279337866812,
      "avg_token_probability": 0.9333117390616844,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_5351--130/130_201583.txt#0_0",
      "dataset_type": "short",
      "response": "the four yorkshiremen",
      "accuracy": 0.0,
      "g_nll": 1.2811290877580177,
      "average_nll": 0.25622581755160356,
      "perplexity": 1.2920444613160231,
      "avg_token_probability": 0.8237174812583546,
      "sequence_length": 5
    },
    {
      "example_id": "odql_4457--196/196_990714.txt#0_1",
      "dataset_type": "short",
      "response": "wynonie harris",
      "accuracy": 0.0,
      "g_nll": 1.1711389591469015,
      "average_nll": 0.19518982652448358,
      "perplexity": 1.2155417066481728,
      "avg_token_probability": 0.8627341397011518,
      "sequence_length": 6
    },
    {
      "example_id": "qg_2992--93/93_3215677.txt#0_1",
      "dataset_type": "short",
      "response": "62",
      "accuracy": 0.0,
      "g_nll": 0.6169398799538612,
      "average_nll": 0.3084699399769306,
      "perplexity": 1.36134058700308,
      "avg_token_probability": 0.7598973652587501,
      "sequence_length": 2
    },
    {
      "example_id": "qg_2077--57/57_1351839.txt#0_1",
      "dataset_type": "short",
      "response": "bosnia and herzegovina",
      "accuracy": 0.0,
      "g_nll": 0.07358610245864838,
      "average_nll": 0.010512300351235484,
      "perplexity": 1.0105677487068343,
      "avg_token_probability": 0.9896910646708419,
      "sequence_length": 7
    },
    {
      "example_id": "sfq_1811--116/116_151264.txt#0_0",
      "dataset_type": "short",
      "response": "call my bluff",
      "accuracy": 1.0,
      "g_nll": 0.04185977244924288,
      "average_nll": 0.01395325748308096,
      "perplexity": 1.0140510585319336,
      "avg_token_probability": 0.9863331763764295,
      "sequence_length": 3
    },
    {
      "example_id": "odql_8247--176/176_1183968.txt#0_0",
      "dataset_type": "short",
      "response": "baseball",
      "accuracy": 1.0,
      "g_nll": 0.019572461023926735,
      "average_nll": 0.019572461023926735,
      "perplexity": 1.0197652574182507,
      "avg_token_probability": 0.9806178360417076,
      "sequence_length": 1
    },
    {
      "example_id": "sfq_8507--177/177_73240.txt#0_0",
      "dataset_type": "short",
      "response": "tunis",
      "accuracy": 0.0,
      "g_nll": 2.0764839886687696,
      "average_nll": 1.0382419943343848,
      "perplexity": 2.8242476042165254,
      "avg_token_probability": 0.5615415949699871,
      "sequence_length": 2
    },
    {
      "example_id": "odql_8017--6/6_2227411.txt#0_1",
      "dataset_type": "short",
      "response": "belfast",
      "accuracy": 0.0,
      "g_nll": 0.027170348912477493,
      "average_nll": 0.013585174456238747,
      "perplexity": 1.013677872234921,
      "avg_token_probability": 0.9865977223087288,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_24528--186/186_2010986.txt#0_0",
      "dataset_type": "short",
      "response": "viscount",
      "accuracy": 0.0,
      "g_nll": 0.25714597048499854,
      "average_nll": 0.12857298524249927,
      "perplexity": 1.1372044174026081,
      "avg_token_probability": 0.886623828838752,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_14796--60/60_1860775.txt#0_1",
      "dataset_type": "short",
      "response": "massachusetts",
      "accuracy": 1.0,
      "g_nll": 0.06565790920285508,
      "average_nll": 0.03282895460142754,
      "perplexity": 1.0333737702955645,
      "avg_token_probability": 0.9682030166287483,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_7117--62/62_1628292.txt#0_0",
      "dataset_type": "short",
      "response": "greyfriars bobby",
      "accuracy": 1.0,
      "g_nll": 0.06185252626161741,
      "average_nll": 0.010308754376936236,
      "perplexity": 1.010362072642806,
      "avg_token_probability": 0.9898487014152776,
      "sequence_length": 6
    },
    {
      "example_id": "odql_8059--174/174_3212822.txt#0_1",
      "dataset_type": "short",
      "response": "el hierro",
      "accuracy": 1.0,
      "g_nll": 1.4521221481263638,
      "average_nll": 0.48404071604212123,
      "perplexity": 1.6226177108783577,
      "avg_token_probability": 0.6573439623878062,
      "sequence_length": 3
    },
    {
      "example_id": "dpql_4709--117/117_679477.txt#0_0",
      "dataset_type": "short",
      "response": "hampshire",
      "accuracy": 0.0,
      "g_nll": 4.166012708952621,
      "average_nll": 1.388670902984207,
      "perplexity": 4.009517472313446,
      "avg_token_probability": 0.6710503040558673,
      "sequence_length": 3
    },
    {
      "example_id": "qw_11850--49/49_1269426.txt#0_0",
      "dataset_type": "short",
      "response": "venice",
      "accuracy": 1.0,
      "g_nll": 0.02001153117771537,
      "average_nll": 0.010005765588857685,
      "perplexity": 1.0100559906349451,
      "avg_token_probability": 0.9900936380377912,
      "sequence_length": 2
    },
    {
      "example_id": "qg_1429--123/123_375481.txt#0_2",
      "dataset_type": "short",
      "response": "sherlock holmes",
      "accuracy": 1.0,
      "g_nll": 0.0055404958338840515,
      "average_nll": 0.0013851239584710129,
      "perplexity": 1.0013860836857238,
      "avg_token_probability": 0.9986174599725359,
      "sequence_length": 4
    },
    {
      "example_id": "qw_10270--138/138_49533.txt#0_0",
      "dataset_type": "short",
      "response": "1982",
      "accuracy": 1.0,
      "g_nll": 0.01831435109488666,
      "average_nll": 0.006104783698295553,
      "perplexity": 1.0061234558674772,
      "avg_token_probability": 0.9939222692134058,
      "sequence_length": 3
    },
    {
      "example_id": "tc_2833--59/59_86475.txt#0_0",
      "dataset_type": "short",
      "response": "cellulose",
      "accuracy": 1.0,
      "g_nll": 0.03793161486100871,
      "average_nll": 0.018965807430504356,
      "perplexity": 1.0191468007738311,
      "avg_token_probability": 0.9813855855210648,
      "sequence_length": 2
    },
    {
      "example_id": "qb_9685--66/66_533419.txt#0_1",
      "dataset_type": "short",
      "response": "failure",
      "accuracy": 1.0,
      "g_nll": 0.03201263025403023,
      "average_nll": 0.03201263025403023,
      "perplexity": 1.0325305463458567,
      "avg_token_probability": 0.9684943496722853,
      "sequence_length": 1
    },
    {
      "example_id": "odql_6714--124/124_3212527.txt#0_0",
      "dataset_type": "short",
      "response": "oasis",
      "accuracy": 1.0,
      "g_nll": 0.09052792191505432,
      "average_nll": 0.09052792191505432,
      "perplexity": 1.0947520747893358,
      "avg_token_probability": 0.9134488283042816,
      "sequence_length": 1
    },
    {
      "example_id": "qw_15940--120/120_2983777.txt#0_0",
      "dataset_type": "short",
      "response": "primal scream",
      "accuracy": 1.0,
      "g_nll": 0.01420618677548191,
      "average_nll": 0.007103093387740955,
      "perplexity": 1.0071283801916322,
      "avg_token_probability": 0.9929470267194502,
      "sequence_length": 2
    },
    {
      "example_id": "bb_5924--97/97_919135.txt#0_2",
      "dataset_type": "short",
      "response": "the snakes",
      "accuracy": 0.0,
      "g_nll": 9.59486198425293,
      "average_nll": 4.797430992126465,
      "perplexity": 121.19865692858123,
      "avg_token_probability": 0.03882243810214441,
      "sequence_length": 2
    },
    {
      "example_id": "qb_2710--0/0_341544.txt#0_2",
      "dataset_type": "short",
      "response": "mercury",
      "accuracy": 1.0,
      "g_nll": 0.005512275733053684,
      "average_nll": 0.005512275733053684,
      "perplexity": 1.0055274962786958,
      "avg_token_probability": 0.9945028889819999,
      "sequence_length": 1
    },
    {
      "example_id": "dpql_5912--30/30_711491.txt#0_0",
      "dataset_type": "short",
      "response": "rutland",
      "accuracy": 0.0,
      "g_nll": 3.53531783958897,
      "average_nll": 1.767658919794485,
      "perplexity": 5.857125298307049,
      "avg_token_probability": 0.5134037682481855,
      "sequence_length": 2
    },
    {
      "example_id": "jp_2368--22/22_1179904.txt#0_1",
      "dataset_type": "short",
      "response": "rio grande",
      "accuracy": 1.0,
      "g_nll": 0.13785661244764924,
      "average_nll": 0.06892830622382462,
      "perplexity": 1.0713593965941053,
      "avg_token_probability": 0.9352437747707425,
      "sequence_length": 2
    },
    {
      "example_id": "dpql_2455--24/24_81962.txt#0_0",
      "dataset_type": "short",
      "response": "mata hari",
      "accuracy": 1.0,
      "g_nll": 0.01574857532978058,
      "average_nll": 0.00787428766489029,
      "perplexity": 1.0079053714018689,
      "avg_token_probability": 0.9921595596297951,
      "sequence_length": 2
    },
    {
      "example_id": "wh_1013--32/32_749045.txt#0_1",
      "dataset_type": "short",
      "response": "leeds",
      "accuracy": 1.0,
      "g_nll": 0.03996327120148635,
      "average_nll": 0.019981635600743175,
      "perplexity": 1.020182604814056,
      "avg_token_probability": 0.9804119114930083,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_12851--98/98_131425.txt#0_0",
      "dataset_type": "short",
      "response": "33",
      "accuracy": 1.0,
      "g_nll": 0.003185091307386756,
      "average_nll": 0.001592545653693378,
      "perplexity": 1.0015938144279606,
      "avg_token_probability": 0.9984095673503832,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_12855--189/189_1756387.txt#0_1",
      "dataset_type": "short",
      "response": "the humber",
      "accuracy": 1.0,
      "g_nll": 0.4648066163049407,
      "average_nll": 0.15493553876831356,
      "perplexity": 1.1675826948606673,
      "avg_token_probability": 0.867059025044926,
      "sequence_length": 3
    },
    {
      "example_id": "jp_3645--146/146_1169963.txt#0_0",
      "dataset_type": "short",
      "response": "taiwan",
      "accuracy": 1.0,
      "g_nll": 0.049096362930868054,
      "average_nll": 0.024548181465434027,
      "perplexity": 1.0248519687873536,
      "avg_token_probability": 0.9760433375740998,
      "sequence_length": 2
    },
    {
      "example_id": "odql_12368--150/150_2308380.txt#0_0",
      "dataset_type": "short",
      "response": "oxford",
      "accuracy": 0.0,
      "g_nll": 3.2077713906764984,
      "average_nll": 1.6038856953382492,
      "perplexity": 4.972315839872843,
      "avg_token_probability": 0.4065505142907201,
      "sequence_length": 2
    },
    {
      "example_id": "qb_6543--195/195_826037.txt#0_0",
      "dataset_type": "short",
      "response": "john f. kennedy international airport",
      "accuracy": 0.0,
      "g_nll": 0.49408273956532867,
      "average_nll": 0.07058324850933266,
      "perplexity": 1.073133902509937,
      "avg_token_probability": 0.9344509685412571,
      "sequence_length": 7
    },
    {
      "example_id": "odql_6620--156/156_349458.txt#0_0",
      "dataset_type": "short",
      "response": "Sisyphus",
      "accuracy": 1.0,
      "g_nll": 4.408843474814148,
      "average_nll": 1.102210868703537,
      "perplexity": 3.010815188095073,
      "avg_token_probability": 0.7516193932100301,
      "sequence_length": 4
    },
    {
      "example_id": "jp_3697--38/38_1394540.txt#0_1",
      "dataset_type": "short",
      "response": "albania",
      "accuracy": 0.0,
      "g_nll": 0.005858831445941348,
      "average_nll": 0.0019529438153137828,
      "perplexity": 1.0019548520521109,
      "avg_token_probability": 0.9980526990923456,
      "sequence_length": 3
    },
    {
      "example_id": "wh_585--16/16_792479.txt#0_1",
      "dataset_type": "short",
      "response": "lowestoft",
      "accuracy": 1.0,
      "g_nll": 0.8628811835915258,
      "average_nll": 0.4314405917957629,
      "perplexity": 1.5394736800298359,
      "avg_token_probability": 0.7109699717897158,
      "sequence_length": 2
    },
    {
      "example_id": "bt_2178--180/180_535996.txt#0_2",
      "dataset_type": "short",
      "response": "bedser",
      "accuracy": 1.0,
      "g_nll": 0.02871800845969119,
      "average_nll": 0.014359004229845596,
      "perplexity": 1.014462589933104,
      "avg_token_probability": 0.9858445450251181,
      "sequence_length": 2
    },
    {
      "example_id": "sfq_16988--36/36_2777799.txt#0_0",
      "dataset_type": "short",
      "response": "TS Eliot",
      "accuracy": 1.0,
      "g_nll": 0.3268291503113687,
      "average_nll": 0.10894305010378957,
      "perplexity": 1.1150988437697016,
      "avg_token_probability": 0.9031350116165374,
      "sequence_length": 3
    },
    {
      "example_id": "bb_3892--168/168_918990.txt#0_0",
      "dataset_type": "short",
      "response": "honeybees",
      "accuracy": 0.0,
      "g_nll": 0.10388363217134611,
      "average_nll": 0.0346278773904487,
      "perplexity": 1.0352344029860308,
      "avg_token_probability": 0.9670182931843843,
      "sequence_length": 3
    },
    {
      "example_id": "odql_2538--189/189_425837.txt#0_1",
      "dataset_type": "short",
      "response": "volleyball",
      "accuracy": 1.0,
      "g_nll": 0.0061305807903409,
      "average_nll": 0.0061305807903409,
      "perplexity": 1.0061494112616625,
      "avg_token_probability": 0.9938881728768777,
      "sequence_length": 1
    },
    {
      "example_id": "qf_1723--128/128_76305.txt#0_1",
      "dataset_type": "short",
      "response": "the cardinals",
      "accuracy": 1.0,
      "g_nll": 1.6793567463628278,
      "average_nll": 0.5597855821209426,
      "perplexity": 1.750297165052395,
      "avg_token_probability": 0.7136401141461453,
      "sequence_length": 3
    },
    {
      "example_id": "qz_6774--13/13_1663667.txt#0_0",
      "dataset_type": "short",
      "response": "lithium",
      "accuracy": 1.0,
      "g_nll": 0.006800601724535227,
      "average_nll": 0.006800601724535227,
      "perplexity": 1.0068237783249314,
      "avg_token_probability": 0.9932224700371257,
      "sequence_length": 1
    }
  ]
}