[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
2026-02-19 16:25:08 INFO     Finished wandb init.
2026-02-19 16:25:12 INFO     Train dataset: Dataset({
    features: ['id', 'question', 'context', 'answers'],
    num_rows: 12294
})
2026-02-19 16:25:14 INFO     Prompt is:
2026-02-19 16:25:14 INFO     Using HuggingFace model cache directory: /system/user/studentwork/boldis/huggingface-cache/hub
2026-02-19 16:25:15 INFO     Detected model size: 8b
Initializing model:  Llama-3.1-8B and base: meta-llama
2026-02-19 16:25:15 INFO     Using device_map="auto" - will distribute across 2 GPU(s)
2026-02-19 16:25:15 INFO     Max memory per GPU: {0: '11GiB', 1: '11GiB'}
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:09<00:00,  2.31s/it]
2026-02-19 16:25:26 WARNING  Some parameters are on the meta device because they were offloaded to the disk.
2026-02-19 16:25:26 INFO     ================================================================================
2026-02-19 16:25:26 INFO     Generating answers:
2026-02-19 16:25:26 INFO     ================================================================================
2026-02-19 16:25:26 INFO     xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
2026-02-19 16:25:26 INFO     Starting with dataset_split train.
2026-02-19 16:25:26 INFO     xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
2026-02-19 16:25:26 INFO     Skip training data.
2026-02-19 16:25:26 INFO     xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
2026-02-19 16:25:26 INFO     Starting with dataset_split validation.
2026-02-19 16:25:26 INFO     xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
  0%|                                                                                                                                                        | 0/400 [00:00<?, ?it/s]2026-02-19 16:25:26 INFO     Current input: Answer the following question in one complete sentence.

Context: The Battle of Waterloo was fought on Sunday, 18 June 1815 near Waterloo in present-day Belgium. A French army under Napoleon Bonaparte was defeated by two armies of the Seventh Coalition, including a British-led allied army under the Duke of Wellington.
Question: Who commanded the British forces at the Battle of Waterloo?
Answer: The Duke of Wellington commanded the British-led allied forces at the Battle of Waterloo in 1815.

Context: The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci. Considered an archetypal masterpiece of the Italian Renaissance, it has been described as the best known, most visited, most written about, and most parodied work of art in the world.
Question: Who painted the Mona Lisa?
Answer: Leonardo da Vinci painted the Mona Lisa, which is considered a masterpiece of the Italian Renaissance.

Context: Mount Everest is Earth's highest mountain above sea level, located in the Mahalangur Himal sub-range of the Himalayas. Its elevation of 8,848.86 m was most recently established in 2020 by the Chinese and Nepali authorities.
Question: What is the height of Mount Everest?
Answer: Mount Everest stands at an elevation of 8,848.86 meters above sea level, making it Earth's highest mountain.

Context: [DOC] [TLE] International Bureau of Weights and MeasuresThe International Bureau of Weights and Measures (), is an international standards organisation, one of three such organisations established to maintain the International System of Units (SI) under the terms of the Metre Convention (Convention du MÃ¨tre). The organisation is usually referred to by its French initialism, BIPM. [PAR] The other organisations that maintain the SI system, also known by their French initialisms, are the General Conference on Weights and Measures () (CGPM) and the International Committee for Weights and Measures () (CIPM). [PAR] History [PAR] The BIPM was created on 20 May 1875, following the signing of the Metre Convention, a treaty among 51 nations ().  It is based at the Pavillon de Breteuil in SÃ¨vres, France, a 4.35Â ha site (originally 2.52Â ha) granted to the Bureau by the French Government in 1876, where it enjoys extraterritorial status,  a status that was clarified by the French decree No 70-820 of 9 September 1970. [PAR] Function [PAR] Under the authority of the Metric Convention, the BIPM helps to ensure uniformity of SI weights and measures around the world.  It does so through a series of consultative committees, whose members are the national metrology laboratories of the Convention's member states, and through its own laboratory work. [PAR] The BIPM carries out measurement-related research. It takes part in and organises international comparisons of national measurement standards and performs calibrations for member states. [PAR] The BIPM has an important role in maintaining accurate worldwide time of day.
Question: The International Bureau of Weights and Measures is based in which European country?
Answer:
The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
  0%|                                                                                                                                                        | 0/400 [00:05<?, ?it/s]
Traceback (most recent call last):
  File "/system/user/studentwork/boldis/super-guacamole/src/generate_answers.py", line 294, in <module>
    main(args)
  File "/system/user/studentwork/boldis/super-guacamole/src/generate_answers.py", line 191, in main
    predicted_answer, token_log_likelihoods, embedding, token_ids, tokens = model.predict(
                                                                            ^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/models/huggingface_models.py", line 543, in predict
    outputs = self.model.generate(
              ^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/generation/utils.py", line 2564, in generate
    result = decoding_method(
             ^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/generation/utils.py", line 2784, in _sample
    outputs = self(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/accelerate/hooks.py", line 175, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/utils/generic.py", line 918, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 473, in forward
    logits = self.lm_head(hidden_states[:, slice_indices, :])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    args, kwargs = module._hf_hook.pre_forward(module, *args, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/accelerate/hooks.py", line 341, in pre_forward
    value = self.weights_map[name]
            ~~~~~~~~~~~~~~~~^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/accelerate/utils/offload.py", line 118, in __getitem__
    return self.dataset[f"{self.prefix}{key}"]
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/accelerate/utils/offload.py", line 178, in __getitem__
    tensor = tensor.to(getattr(torch, weight_info["dtype"]))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 11.77 GiB of which 1.31 GiB is free. Process 1271900 has 306.00 MiB memory in use. Including non-PyTorch memory, this process has 10.15 GiB memory in use. Of the allocated memory 9.74 GiB is allocated by PyTorch, and 47.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
