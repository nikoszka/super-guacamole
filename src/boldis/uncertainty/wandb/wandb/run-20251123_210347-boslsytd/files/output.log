[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-23 21:03:49 WARNING  Recompute accuracy enabled. This does not apply to precomputed p_true!
2025-11-23 21:03:49 WARNING  70B models require significant GPU memory (~35GB with 4-bit quantization). Attempting to load Llama-3.1-70B with 4-bit quantization...
2025-11-23 21:03:49 INFO     Using HuggingFace model cache directory: /system/user/studentwork/boldis/huggingface-cache/hub
2025-11-23 21:03:50 INFO     Detected model size: 70b
2025-11-23 21:03:50 INFO     Redirecting to pre-quantized 4-bit model to save disk space...
Initializing model:  Meta-Llama-3.1-70B-Instruct-bnb-4bit and base: unsloth
2025-11-23 21:03:50 WARNING  Loading 70B model with quantization. This may still require significant GPU memory.
2025-11-23 21:03:50 INFO     Using max_memory per GPU for quantized model: {0: '11GiB', 1: '11GiB', 2: '11GiB', 3: '11GiB', 'cpu': '100GiB'}
Traceback (most recent call last):
  File "/system/user/studentwork/boldis/super-guacamole/src/compute_uncertainty_measures.py", line 397, in <module>
    main(args)
  File "/system/user/studentwork/boldis/super-guacamole/src/compute_uncertainty_measures.py", line 197, in main
    metric = utils.get_metric(args.metric)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/utils/utils.py", line 427, in get_metric
    metric = get_llama_metric(metric, max_new_tokens=50)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/utils/utils.py", line 338, in get_llama_metric
    judge_model = HuggingfaceModel(
                  ^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/models/huggingface_models.py", line 284, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 549, in from_pretrained
    config, kwargs = AutoConfig.from_pretrained(
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1372, in from_pretrained
    return config_class.from_dict(config_dict, **unused_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/configuration_utils.py", line 839, in from_dict
    logger.info(f"Model config {config}")
                ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/configuration_utils.py", line 873, in __repr__
    return f"{self.__class__.__name__} {self.to_json_string()}"
                                        ^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/configuration_utils.py", line 985, in to_json_string
    config_dict = self.to_diff_dict()
                  ^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/configuration_utils.py", line 887, in to_diff_dict
    config_dict = self.to_dict()
                  ^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/configuration_utils.py", line 964, in to_dict
    self.quantization_config.to_dict()
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'to_dict'
