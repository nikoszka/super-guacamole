[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-23 18:52:46 WARNING  Recompute accuracy enabled. This does not apply to precomputed p_true!
2025-11-23 18:52:46 WARNING  70B models require significant GPU memory (~35GB with 4-bit quantization). Attempting to load Llama-3.1-70B with 4-bit quantization...
2025-11-23 18:52:46 INFO     Using HuggingFace model cache directory: /system/user/studentwork/boldis/huggingface-cache/hub
Traceback (most recent call last):
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/importlib/metadata/__init__.py", line 563, in from_name
    return next(cls.discover(name=name))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/system/user/studentwork/boldis/super-guacamole/src/compute_uncertainty_measures.py", line 397, in <module>
    main(args)
  File "/system/user/studentwork/boldis/super-guacamole/src/compute_uncertainty_measures.py", line 197, in main
    metric = utils.get_metric(args.metric)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/utils/utils.py", line 427, in get_metric
    metric = get_llama_metric(metric, max_new_tokens=50)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/utils/utils.py", line 338, in get_llama_metric
    judge_model = HuggingfaceModel(
                  ^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/models/huggingface_models.py", line 190, in __init__
    kwargs = {'quantization_config': BitsAndBytesConfig(
                                     ^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/utils/quantization_config.py", line 510, in __init__
    self.post_init()
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/utils/quantization_config.py", line 568, in post_init
    if self.load_in_4bit and not version.parse(importlib.metadata.version("bitsandbytes")) >= version.parse(
                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/importlib/metadata/__init__.py", line 1009, in version
    return distribution(distribution_name).version
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/importlib/metadata/__init__.py", line 982, in distribution
    return Distribution.from_name(distribution_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/importlib/metadata/__init__.py", line 565, in from_name
    raise PackageNotFoundError(name)
importlib.metadata.PackageNotFoundError: No package metadata was found for bitsandbytes
