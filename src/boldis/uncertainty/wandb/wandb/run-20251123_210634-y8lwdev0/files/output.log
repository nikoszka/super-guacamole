[34m[1mwandb[0m: Detected [openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
2025-11-23 21:06:36 WARNING  Recompute accuracy enabled. This does not apply to precomputed p_true!
2025-11-23 21:06:36 WARNING  70B models require significant GPU memory (~35GB with 4-bit quantization). Attempting to load Llama-3.1-70B with 4-bit quantization...
2025-11-23 21:06:36 INFO     Using HuggingFace model cache directory: /system/user/studentwork/boldis/huggingface-cache/hub
2025-11-23 21:06:37 INFO     Detected model size: 70b
2025-11-23 21:06:37 INFO     Redirecting to pre-quantized 4-bit model to save disk space...
Initializing model:  Meta-Llama-3.1-70B-Instruct-bnb-4bit and base: unsloth
2025-11-23 21:06:37 WARNING  Loading 70B model with quantization. This may still require significant GPU memory.
2025-11-23 21:06:37 INFO     Using max_memory per GPU for quantized model: {0: '11GiB', 1: '11GiB', 2: '11GiB', 3: '11GiB', 'cpu': '100GiB'}
Traceback (most recent call last):
  File "/system/user/studentwork/boldis/super-guacamole/src/compute_uncertainty_measures.py", line 397, in <module>
    main(args)
  File "/system/user/studentwork/boldis/super-guacamole/src/compute_uncertainty_measures.py", line 197, in main
    metric = utils.get_metric(args.metric)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/utils/utils.py", line 427, in get_metric
    metric = get_llama_metric(metric, max_new_tokens=50)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/utils/utils.py", line 338, in get_llama_metric
    judge_model = HuggingfaceModel(
                  ^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/super-guacamole/src/models/huggingface_models.py", line 295, in __init__
    self.model = AutoModelForCausalLM.from_pretrained(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 604, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/modeling_utils.py", line 277, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/modeling_utils.py", line 5029, in from_pretrained
    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/modeling_utils.py", line 1365, in _get_device_map
    hf_quantizer.validate_environment(device_map=device_map)
  File "/system/user/studentwork/boldis/nllSAR/lib/python3.11/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 127, in validate_environment
    raise ValueError(
ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details.
