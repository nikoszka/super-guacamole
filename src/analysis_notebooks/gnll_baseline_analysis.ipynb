{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# G-NLL Baseline Analysis Notebook\n",
        "# This notebook analyzes the G-NLL (Greedy Negative Log-Likelihood) baseline results\n",
        "# Includes: AUROC analysis, ROC curves, sample answers, ROUGE scores, and LLM judge metrics\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from rouge_score import rouge_scorer\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "Set the paths to your validation_generations.pkl files. You can either:\n",
        "1. Use the latest run automatically\n",
        "2. Specify a specific run directory\n",
        "3. Provide a direct path to the pickle file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration: Set paths to your pickle files\n",
        "# Option 1: Use the latest run automatically\n",
        "wandb_dir = \"../nikos/uncertainty/wandb\"\n",
        "\n",
        "# For short answers\n",
        "short_run_dirs = glob.glob(os.path.join(wandb_dir, \"run-*\"))\n",
        "if short_run_dirs:\n",
        "    latest_short_run = max(short_run_dirs, key=os.path.getmtime)\n",
        "    short_run_name = os.path.basename(latest_short_run)\n",
        "    print(f\"Using latest run for short answers: {short_run_name}\")\n",
        "else:\n",
        "    short_run_name = None\n",
        "    print(\"No wandb runs found for short answers\")\n",
        "\n",
        "# For long answers (you may need to specify manually)\n",
        "long_run_name = None  # Set this manually if different from short\n",
        "\n",
        "# Option 2: Specify specific runs (uncomment and modify)\n",
        "# short_run_name = \"run-20250414_225134-35cwfjoe\"  # Example\n",
        "# long_run_name = \"run-20250414_230000-abc123\"     # Example\n",
        "\n",
        "# Option 3: Direct paths (uncomment and set)\n",
        "# short_pickle_path = \"path/to/short/validation_generations.pkl\"\n",
        "# long_pickle_path = \"path/to/long/validation_generations.pkl\"\n",
        "\n",
        "# Construct paths\n",
        "if short_run_name:\n",
        "    short_pickle_path = os.path.join(wandb_dir, short_run_name, \"files\", \"validation_generations.pkl\")\n",
        "    if not os.path.exists(short_pickle_path):\n",
        "        print(f\"⚠️ Warning: {short_pickle_path} not found\")\n",
        "        short_pickle_path = None\n",
        "    else:\n",
        "        print(f\"✅ Short answers pickle: {short_pickle_path}\")\n",
        "else:\n",
        "    short_pickle_path = None\n",
        "\n",
        "if long_run_name:\n",
        "    long_pickle_path = os.path.join(wandb_dir, long_run_name, \"files\", \"validation_generations.pkl\")\n",
        "    if not os.path.exists(long_pickle_path):\n",
        "        print(f\"⚠️ Warning: {long_pickle_path} not found\")\n",
        "        long_pickle_path = None\n",
        "    else:\n",
        "        print(f\"✅ Long answers pickle: {long_pickle_path}\")\n",
        "else:\n",
        "    long_pickle_path = None\n",
        "\n",
        "# ROUGE threshold for correctness (for short answers)\n",
        "ROUGE_THRESHOLD = 0.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Core Functions\n",
        "\n",
        "All functions from `run_gnll_baseline.py` for computing AUROC and analyzing results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gnll_auroc(pickle_path, use_rouge=False, rouge_threshold=0.3, return_details=False):\n",
        "    \"\"\"\n",
        "    Compute AUROC for G-NLL baseline.\n",
        "    \n",
        "    Args:\n",
        "        pickle_path: Path to validation_generations.pkl\n",
        "        use_rouge: If True, use ROUGE scores for correctness (for short answers)\n",
        "                   If False, use LLM judge accuracy (for long answers)\n",
        "        rouge_threshold: ROUGE-L threshold for correctness (if use_rouge=True)\n",
        "        return_details: If True, return detailed data including all examples\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with results, and optionally detailed data\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Computing G-NLL AUROC...\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Loading generations from: {pickle_path}\")\n",
        "    \n",
        "    with open(pickle_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(data)} examples\")\n",
        "    \n",
        "    y_true = []\n",
        "    gnll_uncertainties = []\n",
        "    example_ids = []\n",
        "    responses = []\n",
        "    rouge_scores = []\n",
        "    judge_accuracies = []\n",
        "    questions = []\n",
        "    references = []\n",
        "    \n",
        "    if use_rouge:\n",
        "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "        print(f\"Using ROUGE-L threshold: {rouge_threshold}\")\n",
        "    else:\n",
        "        print(\"Using LLM judge accuracy (from 'accuracy' field)\")\n",
        "    \n",
        "    for example_id, entry in data.items():\n",
        "        if 'most_likely_answer' not in entry:\n",
        "            continue\n",
        "            \n",
        "        mla = entry['most_likely_answer']\n",
        "        pred_answer = mla.get('response', '').strip()\n",
        "        \n",
        "        if use_rouge:\n",
        "            # Use ROUGE score for correctness\n",
        "            if 'reference' in entry and 'answers' in entry['reference']:\n",
        "                true_answers = entry['reference']['answers']['text']\n",
        "                best_rougeL = 0.0\n",
        "                best_rouge1 = 0.0\n",
        "                best_rouge2 = 0.0\n",
        "                for ref in true_answers:\n",
        "                    score = scorer.score(ref.strip(), pred_answer)\n",
        "                    best_rougeL = max(best_rougeL, score['rougeL'].fmeasure)\n",
        "                    best_rouge1 = max(best_rouge1, score['rouge1'].fmeasure)\n",
        "                    best_rouge2 = max(best_rouge2, score['rouge2'].fmeasure)\n",
        "                is_correct = int(best_rougeL >= rouge_threshold)\n",
        "                rouge_scores.append({\n",
        "                    'rouge1': best_rouge1,\n",
        "                    'rouge2': best_rouge2,\n",
        "                    'rougeL': best_rougeL\n",
        "                })\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            # Use LLM judge accuracy\n",
        "            judge_acc = mla.get('accuracy', 0.0)\n",
        "            is_correct = int(judge_acc > 0.5)\n",
        "            judge_accuracies.append(judge_acc)\n",
        "            rouge_scores.append(None)\n",
        "        \n",
        "        # Compute G-NLL (sum of token log-probs, negated)\n",
        "        if 'token_log_likelihoods' in mla:\n",
        "            token_log_likelihoods = mla['token_log_likelihoods']\n",
        "            sequence_nll = -sum(token_log_likelihoods)  # Negative log likelihood\n",
        "            gnll_uncertainties.append(sequence_nll)\n",
        "            y_true.append(is_correct)\n",
        "            example_ids.append(example_id)\n",
        "            responses.append(pred_answer)\n",
        "            questions.append(entry.get('question', ''))\n",
        "            if 'reference' in entry and 'answers' in entry['reference']:\n",
        "                references.append(entry['reference']['answers']['text'])\n",
        "            else:\n",
        "                references.append([])\n",
        "        else:\n",
        "            print(f\"Warning: No token_log_likelihoods for example {example_id}\")\n",
        "    \n",
        "    if len(y_true) == 0:\n",
        "        print(\"❌ No valid examples found!\")\n",
        "        return None\n",
        "    \n",
        "    # Compute AUROC\n",
        "    try:\n",
        "        # For AUROC, higher uncertainty (higher NLL) should predict incorrect answers\n",
        "        # roc_auc_score expects higher scores to predict positive class (correct=1)\n",
        "        # So we negate G-NLL: higher confidence (lower NLL) should predict correct answers\n",
        "        auroc = roc_auc_score(y_true, -np.array(gnll_uncertainties))\n",
        "        \n",
        "        # Also compute ROC curve for visualization\n",
        "        fpr, tpr, thresholds = roc_curve(y_true, -np.array(gnll_uncertainties))\n",
        "        \n",
        "        accuracy = sum(y_true) / len(y_true) if len(y_true) > 0 else 0\n",
        "        \n",
        "        results = {\n",
        "            'G-NLL_AUROC': auroc,\n",
        "            'Accuracy': accuracy,\n",
        "            'Num_examples': len(y_true),\n",
        "            'Num_correct': int(sum(y_true)),\n",
        "            'Num_incorrect': int(len(y_true) - sum(y_true)),\n",
        "            'Mean_G-NLL': np.mean(gnll_uncertainties),\n",
        "            'Std_G-NLL': np.std(gnll_uncertainties),\n",
        "            'Min_G-NLL': np.min(gnll_uncertainties),\n",
        "            'Max_G-NLL': np.max(gnll_uncertainties),\n",
        "            'ROC_curve': {\n",
        "                'fpr': fpr,\n",
        "                'tpr': tpr,\n",
        "                'thresholds': thresholds\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nResults:\")\n",
        "        print(f\"  G-NLL AUROC: {auroc:.4f}\")\n",
        "        print(f\"  Accuracy: {accuracy:.4f} ({int(sum(y_true))}/{len(y_true)})\")\n",
        "        print(f\"  Number of examples: {len(y_true)}\")\n",
        "        print(f\"  Mean G-NLL: {np.mean(gnll_uncertainties):.4f}\")\n",
        "        print(f\"  Std G-NLL: {np.std(gnll_uncertainties):.4f}\")\n",
        "        print(f\"  Min G-NLL: {np.min(gnll_uncertainties):.4f}\")\n",
        "        print(f\"  Max G-NLL: {np.max(gnll_uncertainties):.4f}\")\n",
        "        \n",
        "        if return_details:\n",
        "            details = {\n",
        "                'example_ids': example_ids,\n",
        "                'y_true': y_true,\n",
        "                'gnll_uncertainties': gnll_uncertainties,\n",
        "                'responses': responses,\n",
        "                'questions': questions,\n",
        "                'references': references,\n",
        "                'rouge_scores': rouge_scores,\n",
        "                'judge_accuracies': judge_accuracies if not use_rouge else None\n",
        "            }\n",
        "            return results, details\n",
        "        else:\n",
        "            return results\n",
        "        \n",
        "    except ValueError as e:\n",
        "        print(f\"❌ Error computing AUROC: {e}\")\n",
        "        print(f\"This might happen if all labels are the same (all correct or all incorrect)\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze short answers (if available)\n",
        "short_results_rouge = None\n",
        "short_results_judge = None\n",
        "short_details_rouge = None\n",
        "short_details_judge = None\n",
        "\n",
        "if short_pickle_path and os.path.exists(short_pickle_path):\n",
        "    print(\"=\"*80)\n",
        "    print(\"ANALYZING SHORT ANSWERS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # With ROUGE\n",
        "    print(\"\\n--- Short Answers: ROUGE-based correctness ---\")\n",
        "    result = compute_gnll_auroc(short_pickle_path, use_rouge=True, rouge_threshold=ROUGE_THRESHOLD, return_details=True)\n",
        "    if result:\n",
        "        short_results_rouge, short_details_rouge = result\n",
        "    \n",
        "    # With LLM Judge\n",
        "    print(\"\\n--- Short Answers: LLM Judge-based correctness ---\")\n",
        "    result = compute_gnll_auroc(short_pickle_path, use_rouge=False, return_details=True)\n",
        "    if result:\n",
        "        short_results_judge, short_details_judge = result\n",
        "else:\n",
        "    print(\"⚠️ Short answers pickle file not found. Set short_pickle_path manually.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze long answers (if available)\n",
        "long_results_judge = None\n",
        "long_details_judge = None\n",
        "\n",
        "if long_pickle_path and os.path.exists(long_pickle_path):\n",
        "    print(\"=\"*80)\n",
        "    print(\"ANALYZING LONG ANSWERS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    print(\"\\n--- Long Answers: LLM Judge-based correctness ---\")\n",
        "    result = compute_gnll_auroc(long_pickle_path, use_rouge=False, return_details=True)\n",
        "    if result:\n",
        "        long_results_judge, long_details_judge = result\n",
        "else:\n",
        "    print(\"⚠️ Long answers pickle file not found. Set long_pickle_path manually.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizations\n",
        "\n",
        "### 1. ROC Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_roc_curve(results_dict, title_suffix=\"\"):\n",
        "    \"\"\"Plot ROC curve from results dictionary.\"\"\"\n",
        "    if not results_dict or 'ROC_curve' not in results_dict:\n",
        "        print(\"No ROC curve data available\")\n",
        "        return\n",
        "    \n",
        "    fpr = results_dict['ROC_curve']['fpr']\n",
        "    tpr = results_dict['ROC_curve']['tpr']\n",
        "    auroc = results_dict['G-NLL_AUROC']\n",
        "    \n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.plot(fpr, tpr, label=f'G-NLL (AUROC = {auroc:.4f})', linewidth=2)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random (AUROC = 0.5)', linewidth=1)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title(f'ROC Curve - G-NLL Baseline{title_suffix}', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot ROC curves\n",
        "if short_results_rouge:\n",
        "    plot_roc_curve(short_results_rouge, \" (Short Answers - ROUGE)\")\n",
        "\n",
        "if short_results_judge:\n",
        "    plot_roc_curve(short_results_judge, \" (Short Answers - LLM Judge)\")\n",
        "\n",
        "if long_results_judge:\n",
        "    plot_roc_curve(long_results_judge, \" (Long Answers - LLM Judge)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. G-NLL Distribution: Correct vs Incorrect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gnll_distribution(details, title_suffix=\"\"):\n",
        "    \"\"\"Plot G-NLL distribution for correct vs incorrect answers.\"\"\"\n",
        "    if not details:\n",
        "        print(\"No details data available\")\n",
        "        return\n",
        "    \n",
        "    gnll = np.array(details['gnll_uncertainties'])\n",
        "    y_true = np.array(details['y_true'])\n",
        "    \n",
        "    correct_nll = gnll[y_true == 1]\n",
        "    incorrect_nll = gnll[y_true == 0]\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(correct_nll, bins=50, alpha=0.6, label='Correct', color='green', density=True)\n",
        "    plt.hist(incorrect_nll, bins=50, alpha=0.6, label='Incorrect', color='red', density=True)\n",
        "    plt.xlabel('G-NLL (Negative Log-Likelihood)', fontsize=12)\n",
        "    plt.ylabel('Density', fontsize=12)\n",
        "    plt.title(f'G-NLL Distribution: Correct vs Incorrect{title_suffix}', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nStatistics{title_suffix}:\")\n",
        "    print(f\"  Correct answers - Mean G-NLL: {np.mean(correct_nll):.4f}, Std: {np.std(correct_nll):.4f}\")\n",
        "    print(f\"  Incorrect answers - Mean G-NLL: {np.mean(incorrect_nll):.4f}, Std: {np.std(incorrect_nll):.4f}\")\n",
        "\n",
        "# Plot distributions\n",
        "if short_details_rouge:\n",
        "    plot_gnll_distribution(short_details_rouge, \" (Short Answers - ROUGE)\")\n",
        "\n",
        "if short_details_judge:\n",
        "    plot_gnll_distribution(short_details_judge, \" (Short Answers - LLM Judge)\")\n",
        "\n",
        "if long_details_judge:\n",
        "    plot_gnll_distribution(long_details_judge, \" (Long Answers - LLM Judge)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Scatter Plot: G-NLL vs Correctness\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_gnll_vs_correctness(details, title_suffix=\"\"):\n",
        "    \"\"\"Plot scatter plot of G-NLL vs correctness.\"\"\"\n",
        "    if not details:\n",
        "        print(\"No details data available\")\n",
        "        return\n",
        "    \n",
        "    gnll = np.array(details['gnll_uncertainties'])\n",
        "    y_true = np.array(details['y_true'])\n",
        "    \n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(gnll[y_true == 1], y_true[y_true == 1], alpha=0.5, label='Correct', color='green', s=20)\n",
        "    plt.scatter(gnll[y_true == 0], y_true[y_true == 0], alpha=0.5, label='Incorrect', color='red', s=20)\n",
        "    plt.xlabel('G-NLL (Negative Log-Likelihood)', fontsize=12)\n",
        "    plt.ylabel('Correctness (1=Correct, 0=Incorrect)', fontsize=12)\n",
        "    plt.title(f'G-NLL vs Correctness{title_suffix}', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot scatter plots\n",
        "if short_details_rouge:\n",
        "    plot_gnll_vs_correctness(short_details_rouge, \" (Short Answers - ROUGE)\")\n",
        "\n",
        "if short_details_judge:\n",
        "    plot_gnll_vs_correctness(short_details_judge, \" (Short Answers - LLM Judge)\")\n",
        "\n",
        "if long_details_judge:\n",
        "    plot_gnll_vs_correctness(long_details_judge, \" (Long Answers - LLM Judge)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. ROUGE Scores Analysis (Short Answers Only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_rouge_scores(details, title_suffix=\"\"):\n",
        "    \"\"\"Plot ROUGE score distributions and correlations with G-NLL.\"\"\"\n",
        "    if not details or not details['rouge_scores']:\n",
        "        print(\"No ROUGE scores available\")\n",
        "        return\n",
        "    \n",
        "    rouge_scores = details['rouge_scores']\n",
        "    gnll = np.array(details['gnll_uncertainties'])\n",
        "    \n",
        "    rouge1 = [r['rouge1'] for r in rouge_scores if r]\n",
        "    rouge2 = [r['rouge2'] for r in rouge_scores if r]\n",
        "    rougeL = [r['rougeL'] for r in rouge_scores if r]\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # ROUGE score distributions\n",
        "    axes[0, 0].hist(rouge1, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('ROUGE-1 Score')\n",
        "    axes[0, 0].set_ylabel('Frequency')\n",
        "    axes[0, 0].set_title('ROUGE-1 Distribution')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[0, 1].hist(rouge2, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
        "    axes[0, 1].set_xlabel('ROUGE-2 Score')\n",
        "    axes[0, 1].set_ylabel('Frequency')\n",
        "    axes[0, 1].set_title('ROUGE-2 Distribution')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    axes[1, 0].hist(rougeL, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
        "    axes[1, 0].set_xlabel('ROUGE-L Score')\n",
        "    axes[1, 0].set_ylabel('Frequency')\n",
        "    axes[1, 0].set_title('ROUGE-L Distribution')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # G-NLL vs ROUGE-L scatter\n",
        "    axes[1, 1].scatter(rougeL, gnll[:len(rougeL)], alpha=0.5, s=20)\n",
        "    axes[1, 1].set_xlabel('ROUGE-L Score')\n",
        "    axes[1, 1].set_ylabel('G-NLL')\n",
        "    axes[1, 1].set_title('G-NLL vs ROUGE-L')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Calculate correlation\n",
        "    if len(rougeL) > 0:\n",
        "        correlation = np.corrcoef(rougeL, gnll[:len(rougeL)])[0, 1]\n",
        "        axes[1, 1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
        "                       transform=axes[1, 1].transAxes, \n",
        "                       verticalalignment='top',\n",
        "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.suptitle(f'ROUGE Scores Analysis{title_suffix}', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nROUGE Statistics{title_suffix}:\")\n",
        "    print(f\"  ROUGE-1 - Mean: {np.mean(rouge1):.4f}, Std: {np.std(rouge1):.4f}\")\n",
        "    print(f\"  ROUGE-2 - Mean: {np.mean(rouge2):.4f}, Std: {np.std(rouge2):.4f}\")\n",
        "    print(f\"  ROUGE-L - Mean: {np.mean(rougeL):.4f}, Std: {np.std(rougeL):.4f}\")\n",
        "    if len(rougeL) > 0:\n",
        "        print(f\"  G-NLL vs ROUGE-L Correlation: {correlation:.4f}\")\n",
        "\n",
        "if short_details_rouge:\n",
        "    plot_rouge_scores(short_details_rouge, \" (Short Answers)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. LLM Judge Accuracy Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_judge_accuracy(details, title_suffix=\"\"):\n",
        "    \"\"\"Plot LLM judge accuracy distributions and correlations with G-NLL.\"\"\"\n",
        "    if not details or details['judge_accuracies'] is None:\n",
        "        print(\"No LLM judge accuracy data available\")\n",
        "        return\n",
        "    \n",
        "    judge_acc = np.array(details['judge_accuracies'])\n",
        "    gnll = np.array(details['gnll_uncertainties'])\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Judge accuracy distribution\n",
        "    axes[0].hist(judge_acc, bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[0].set_xlabel('LLM Judge Accuracy')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('LLM Judge Accuracy Distribution')\n",
        "    axes[0].axvline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # G-NLL vs Judge Accuracy scatter\n",
        "    axes[1].scatter(judge_acc, gnll, alpha=0.5, s=20)\n",
        "    axes[1].set_xlabel('LLM Judge Accuracy')\n",
        "    axes[1].set_ylabel('G-NLL')\n",
        "    axes[1].set_title('G-NLL vs LLM Judge Accuracy')\n",
        "    axes[1].axvline(0.5, color='red', linestyle='--', alpha=0.5)\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Calculate correlation\n",
        "    correlation = np.corrcoef(judge_acc, gnll)[0, 1]\n",
        "    axes[1].text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
        "                transform=axes[1].transAxes, \n",
        "                verticalalignment='top',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.suptitle(f'LLM Judge Accuracy Analysis{title_suffix}', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nLLM Judge Statistics{title_suffix}:\")\n",
        "    print(f\"  Mean Accuracy: {np.mean(judge_acc):.4f}, Std: {np.std(judge_acc):.4f}\")\n",
        "    print(f\"  Min Accuracy: {np.min(judge_acc):.4f}, Max: {np.max(judge_acc):.4f}\")\n",
        "    print(f\"  G-NLL vs Judge Accuracy Correlation: {correlation:.4f}\")\n",
        "\n",
        "if short_details_judge:\n",
        "    plot_judge_accuracy(short_details_judge, \" (Short Answers)\")\n",
        "\n",
        "if long_details_judge:\n",
        "    plot_judge_accuracy(long_details_judge, \" (Long Answers)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Answers Display\n",
        "\n",
        "Display sample questions, answers, and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def display_sample_answers(details, results, num_samples=10, use_rouge=False, title_suffix=\"\", \n",
        "                          show_extremes=False):\n",
        "    \"\"\"Display sample questions, answers, and metrics.\"\"\"\n",
        "    if not details:\n",
        "        print(\"No details data available\")\n",
        "        return\n",
        "    \n",
        "    gnll = np.array(details['gnll_uncertainties'])\n",
        "    \n",
        "    # If show_extremes, show highest and lowest G-NLL examples\n",
        "    if show_extremes and len(gnll) > 0:\n",
        "        indices = []\n",
        "        # Highest G-NLL (most uncertain)\n",
        "        highest_indices = np.argsort(gnll)[-num_samples//2:][::-1]\n",
        "        # Lowest G-NLL (most confident)\n",
        "        lowest_indices = np.argsort(gnll)[:num_samples//2]\n",
        "        indices = list(lowest_indices) + list(highest_indices)\n",
        "    else:\n",
        "        indices = list(range(min(num_samples, len(details['questions']))))\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Sample Answers{title_suffix}\")\n",
        "    if show_extremes:\n",
        "        print(\"(Showing examples with lowest and highest G-NLL)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Showing {len(indices)} examples (out of {len(details['questions'])})\")\n",
        "    print(f\"Overall AUROC: {results['G-NLL_AUROC']:.4f}, Accuracy: {results['Accuracy']:.4f}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Display in a more readable format\n",
        "    for i, idx in enumerate(indices):\n",
        "        label = \"Lowest G-NLL\" if i < len(indices)//2 else \"Highest G-NLL\"\n",
        "        print(f\"\\n--- Example {i + 1} ({label}) ---\")\n",
        "        print(f\"Question: {details['questions'][idx][:200]}...\")\n",
        "        print(f\"Response: {details['responses'][idx][:200]}...\")\n",
        "        print(f\"G-NLL: {gnll[idx]:.4f}\")\n",
        "        print(f\"Correct: {'✓' if details['y_true'][idx] == 1 else '✗'}\")\n",
        "        \n",
        "        if use_rouge and details['rouge_scores'] and details['rouge_scores'][idx]:\n",
        "            r = details['rouge_scores'][idx]\n",
        "            print(f\"ROUGE-1: {r['rouge1']:.3f}, ROUGE-2: {r['rouge2']:.3f}, ROUGE-L: {r['rougeL']:.3f}\")\n",
        "            if details['references'] and details['references'][idx]:\n",
        "                ref = details['references'][idx][0]\n",
        "                print(f\"Reference: {ref[:150]}...\" if len(ref) > 150 else f\"Reference: {ref}\")\n",
        "        elif details['judge_accuracies'] and details['judge_accuracies'][idx] is not None:\n",
        "            print(f\"Judge Accuracy: {details['judge_accuracies'][idx]:.3f}\")\n",
        "        print()\n",
        "\n",
        "# Display samples\n",
        "if short_details_rouge and short_results_rouge:\n",
        "    display_sample_answers(short_details_rouge, short_results_rouge, num_samples=5, \n",
        "                          use_rouge=True, title_suffix=\" (Short Answers - ROUGE)\")\n",
        "\n",
        "if short_details_judge and short_results_judge:\n",
        "    display_sample_answers(short_details_judge, short_results_judge, num_samples=5, \n",
        "                          use_rouge=False, title_suffix=\" (Short Answers - LLM Judge)\")\n",
        "\n",
        "if long_details_judge and long_results_judge:\n",
        "    display_sample_answers(long_details_judge, long_results_judge, num_samples=5, \n",
        "                          use_rouge=False, title_suffix=\" (Long Answers - LLM Judge)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Examples with Extreme G-NLL Values\n",
        "\n",
        "Show examples with the lowest (most confident) and highest (most uncertain) G-NLL values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display extreme examples\n",
        "if short_details_rouge and short_results_rouge:\n",
        "    display_sample_answers(short_details_rouge, short_results_rouge, num_samples=6, \n",
        "                          use_rouge=True, title_suffix=\" (Short Answers - ROUGE)\", \n",
        "                          show_extremes=True)\n",
        "\n",
        "if short_details_judge and short_results_judge:\n",
        "    display_sample_answers(short_details_judge, short_results_judge, num_samples=6, \n",
        "                          use_rouge=False, title_suffix=\" (Short Answers - LLM Judge)\",\n",
        "                          show_extremes=True)\n",
        "\n",
        "if long_details_judge and long_results_judge:\n",
        "    display_sample_answers(long_details_judge, long_results_judge, num_samples=6, \n",
        "                          use_rouge=False, title_suffix=\" (Long Answers - LLM Judge)\",\n",
        "                          show_extremes=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary Statistics\n",
        "\n",
        "Create a comprehensive summary table of all results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary table\n",
        "summary_data = []\n",
        "\n",
        "if short_results_rouge:\n",
        "    summary_data.append({\n",
        "        'Type': 'Short Answers',\n",
        "        'Metric': 'ROUGE-L',\n",
        "        'AUROC': f\"{short_results_rouge['G-NLL_AUROC']:.4f}\",\n",
        "        'Accuracy': f\"{short_results_rouge['Accuracy']:.4f}\",\n",
        "        'Num Examples': short_results_rouge['Num_examples'],\n",
        "        'Mean G-NLL': f\"{short_results_rouge['Mean_G-NLL']:.4f}\",\n",
        "        'Std G-NLL': f\"{short_results_rouge['Std_G-NLL']:.4f}\"\n",
        "    })\n",
        "\n",
        "if short_results_judge:\n",
        "    summary_data.append({\n",
        "        'Type': 'Short Answers',\n",
        "        'Metric': 'LLM Judge',\n",
        "        'AUROC': f\"{short_results_judge['G-NLL_AUROC']:.4f}\",\n",
        "        'Accuracy': f\"{short_results_judge['Accuracy']:.4f}\",\n",
        "        'Num Examples': short_results_judge['Num_examples'],\n",
        "        'Mean G-NLL': f\"{short_results_judge['Mean_G-NLL']:.4f}\",\n",
        "        'Std G-NLL': f\"{short_results_judge['Std_G-NLL']:.4f}\"\n",
        "    })\n",
        "\n",
        "if long_results_judge:\n",
        "    summary_data.append({\n",
        "        'Type': 'Long Answers',\n",
        "        'Metric': 'LLM Judge',\n",
        "        'AUROC': f\"{long_results_judge['G-NLL_AUROC']:.4f}\",\n",
        "        'Accuracy': f\"{long_results_judge['Accuracy']:.4f}\",\n",
        "        'Num Examples': long_results_judge['Num_examples'],\n",
        "        'Mean G-NLL': f\"{long_results_judge['Mean_G-NLL']:.4f}\",\n",
        "        'Std G-NLL': f\"{long_results_judge['Std_G-NLL']:.4f}\"\n",
        "    })\n",
        "\n",
        "if summary_data:\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SUMMARY STATISTICS\")\n",
        "    print(\"=\"*80)\n",
        "    print(summary_df.to_string(index=False))\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"No results available to summarize.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Results\n",
        "\n",
        "Save results to JSON file for later use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RW-G-NLL Analysis\n",
        "\n",
        "Relevance-Weighted G-NLL (RW-G-NLL) re-weights token log-likelihoods by semantic relevance,\n",
        "filtering out noise from \"generative inequality.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import RW-G-NLL functions\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "from uncertainty_measures.rw_gnll import (\n",
        "    initialize_similarity_model,\n",
        "    compute_rw_gnll\n",
        ")\n",
        "from transformers import AutoTokenizer\n",
        "from models.huggingface_models import get_hf_cache_dir\n",
        "\n",
        "# Configuration for RW-G-NLL\n",
        "SIMILARITY_MODEL_NAME = 'cross-encoder/stsb-roberta-large'  # Default similarity model\n",
        "MODEL_NAME = 'Llama-3.2-1B'  # Update this to match your generation model\n",
        "\n",
        "# Initialize similarity model and tokenizer\n",
        "print(\"Initializing RW-G-NLL components...\")\n",
        "similarity_model = initialize_similarity_model(SIMILARITY_MODEL_NAME)\n",
        "print(f\"✅ Similarity model loaded: {SIMILARITY_MODEL_NAME}\")\n",
        "\n",
        "# Load tokenizer\n",
        "cache_dir = get_hf_cache_dir()\n",
        "if 'llama' in MODEL_NAME.lower():\n",
        "    if 'Llama-3' in MODEL_NAME or 'Llama-3.1' in MODEL_NAME or 'Meta-Llama-3' in MODEL_NAME or 'Llama-2' in MODEL_NAME:\n",
        "        base = 'meta-llama'\n",
        "    else:\n",
        "        base = 'huggyllama'\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        f\"{base}/{MODEL_NAME}\",\n",
        "        token_type_ids=None,\n",
        "        cache_dir=cache_dir\n",
        "    )\n",
        "    print(f\"✅ Tokenizer loaded for model: {MODEL_NAME}\")\n",
        "else:\n",
        "    print(f\"⚠️  Unknown model type: {MODEL_NAME}\")\n",
        "    tokenizer = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute RW-G-NLL for long answers (if available)\n",
        "rw_gnll_results = None\n",
        "rw_gnll_details = None\n",
        "\n",
        "if long_pickle_path and os.path.exists(long_pickle_path) and tokenizer is not None:\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPUTING RW-G-NLL FOR LONG ANSWERS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    with open(long_pickle_path, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(data)} examples\")\n",
        "    \n",
        "    y_true_rw = []\n",
        "    rw_gnll_uncertainties = []\n",
        "    gnll_uncertainties_rw = []\n",
        "    example_ids_rw = []\n",
        "    responses_rw = []\n",
        "    questions_rw = []\n",
        "    relevance_weights_all = []\n",
        "    \n",
        "    similarity_cache = {}\n",
        "    \n",
        "    print(\"Computing RW-G-NLL for each example...\")\n",
        "    for example_id, entry in tqdm(data.items(), desc=\"Processing examples\"):\n",
        "        if 'most_likely_answer' not in entry:\n",
        "            continue\n",
        "        \n",
        "        mla = entry['most_likely_answer']\n",
        "        is_correct = int(mla.get('accuracy', 0.0) > 0.5)\n",
        "        \n",
        "        if 'token_log_likelihoods' in mla:\n",
        "            try:\n",
        "                # Compute RW-G-NLL\n",
        "                rw_gnll_score, relevance_weights = compute_rw_gnll(\n",
        "                    entry, similarity_model, tokenizer,\n",
        "                    cache=similarity_cache,\n",
        "                    return_relevance_weights=True\n",
        "                )\n",
        "                \n",
        "                # Also get G-NLL for comparison\n",
        "                token_log_likelihoods = mla['token_log_likelihoods']\n",
        "                gnll_score = -sum(token_log_likelihoods)\n",
        "                \n",
        "                rw_gnll_uncertainties.append(rw_gnll_score)\n",
        "                gnll_uncertainties_rw.append(gnll_score)\n",
        "                y_true_rw.append(is_correct)\n",
        "                example_ids_rw.append(example_id)\n",
        "                responses_rw.append(mla.get('response', ''))\n",
        "                questions_rw.append(entry.get('question', ''))\n",
        "                relevance_weights_all.append(relevance_weights)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error computing RW-G-NLL for {example_id}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if len(y_true_rw) > 0:\n",
        "        # Compute AUROC\n",
        "        rw_gnll_auroc = roc_auc_score(y_true_rw, -np.array(rw_gnll_uncertainties))\n",
        "        gnll_auroc_rw = roc_auc_score(y_true_rw, -np.array(gnll_uncertainties_rw))\n",
        "        \n",
        "        rw_gnll_results = {\n",
        "            'RW-G-NLL_AUROC': rw_gnll_auroc,\n",
        "            'G-NLL_AUROC': gnll_auroc_rw,\n",
        "            'Accuracy': sum(y_true_rw) / len(y_true_rw),\n",
        "            'Num_examples': len(y_true_rw),\n",
        "            'Mean_RW-G-NLL': np.mean(rw_gnll_uncertainties),\n",
        "            'Std_RW-G-NLL': np.std(rw_gnll_uncertainties),\n",
        "            'Mean_G-NLL': np.mean(gnll_uncertainties_rw),\n",
        "            'Std_G-NLL': np.std(gnll_uncertainties_rw),\n",
        "        }\n",
        "        \n",
        "        rw_gnll_details = {\n",
        "            'example_ids': example_ids_rw,\n",
        "            'y_true': y_true_rw,\n",
        "            'rw_gnll_uncertainties': rw_gnll_uncertainties,\n",
        "            'gnll_uncertainties': gnll_uncertainties_rw,\n",
        "            'responses': responses_rw,\n",
        "            'questions': questions_rw,\n",
        "            'relevance_weights': relevance_weights_all\n",
        "        }\n",
        "        \n",
        "        print(f\"\\nRW-G-NLL Results:\")\n",
        "        print(f\"  RW-G-NLL AUROC: {rw_gnll_auroc:.4f}\")\n",
        "        print(f\"  G-NLL AUROC: {gnll_auroc_rw:.4f}\")\n",
        "        print(f\"  Improvement: {rw_gnll_auroc - gnll_auroc_rw:.4f}\")\n",
        "        print(f\"  Accuracy: {rw_gnll_results['Accuracy']:.4f}\")\n",
        "        print(f\"  Number of examples: {len(y_true_rw)}\")\n",
        "    else:\n",
        "        print(\"❌ No valid examples found for RW-G-NLL computation\")\n",
        "else:\n",
        "    print(\"⚠️  Long answers pickle file not found or tokenizer not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: G-NLL vs RW-G-NLL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare G-NLL vs RW-G-NLL\n",
        "if rw_gnll_results and rw_gnll_details:\n",
        "    # ROC curves comparison\n",
        "    fpr_gnll, tpr_gnll, _ = roc_curve(\n",
        "        rw_gnll_details['y_true'],\n",
        "        -np.array(rw_gnll_details['gnll_uncertainties'])\n",
        "    )\n",
        "    fpr_rw, tpr_rw, _ = roc_curve(\n",
        "        rw_gnll_details['y_true'],\n",
        "        -np.array(rw_gnll_details['rw_gnll_uncertainties'])\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr_gnll, tpr_gnll, label=f'G-NLL (AUROC = {rw_gnll_results[\"G-NLL_AUROC\"]:.4f})', linewidth=2)\n",
        "    plt.plot(fpr_rw, tpr_rw, label=f'RW-G-NLL (AUROC = {rw_gnll_results[\"RW-G-NLL_AUROC\"]:.4f})', linewidth=2)\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random (AUROC = 0.5)', linewidth=1)\n",
        "    plt.xlabel('False Positive Rate', fontsize=12)\n",
        "    plt.ylabel('True Positive Rate', fontsize=12)\n",
        "    plt.title('ROC Curve Comparison: G-NLL vs RW-G-NLL (Long Answers)', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Distribution comparison\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    gnll_vals = np.array(rw_gnll_details['gnll_uncertainties'])\n",
        "    rw_gnll_vals = np.array(rw_gnll_details['rw_gnll_uncertainties'])\n",
        "    y_true_vals = np.array(rw_gnll_details['y_true'])\n",
        "    \n",
        "    # G-NLL distribution\n",
        "    axes[0].hist(gnll_vals[y_true_vals == 1], bins=50, alpha=0.6, label='Correct', color='green', density=True)\n",
        "    axes[0].hist(gnll_vals[y_true_vals == 0], bins=50, alpha=0.6, label='Incorrect', color='red', density=True)\n",
        "    axes[0].set_xlabel('G-NLL', fontsize=12)\n",
        "    axes[0].set_ylabel('Density', fontsize=12)\n",
        "    axes[0].set_title('G-NLL Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # RW-G-NLL distribution\n",
        "    axes[1].hist(rw_gnll_vals[y_true_vals == 1], bins=50, alpha=0.6, label='Correct', color='green', density=True)\n",
        "    axes[1].hist(rw_gnll_vals[y_true_vals == 0], bins=50, alpha=0.6, label='Incorrect', color='red', density=True)\n",
        "    axes[1].set_xlabel('RW-G-NLL', fontsize=12)\n",
        "    axes[1].set_ylabel('Density', fontsize=12)\n",
        "    axes[1].set_title('RW-G-NLL Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle('Uncertainty Distribution: G-NLL vs RW-G-NLL', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Scatter plot: G-NLL vs RW-G-NLL\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.scatter(gnll_vals[y_true_vals == 1], rw_gnll_vals[y_true_vals == 1], \n",
        "               alpha=0.5, label='Correct', color='green', s=20)\n",
        "    plt.scatter(gnll_vals[y_true_vals == 0], rw_gnll_vals[y_true_vals == 0], \n",
        "               alpha=0.5, label='Incorrect', color='red', s=20)\n",
        "    plt.xlabel('G-NLL', fontsize=12)\n",
        "    plt.ylabel('RW-G-NLL', fontsize=12)\n",
        "    plt.title('G-NLL vs RW-G-NLL Scatter Plot', fontsize=14, fontweight='bold')\n",
        "    plt.legend(fontsize=11)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Correlation\n",
        "    correlation = np.corrcoef(gnll_vals, rw_gnll_vals)[0, 1]\n",
        "    print(f\"\\nCorrelation between G-NLL and RW-G-NLL: {correlation:.4f}\")\n",
        "    print(f\"AUROC Improvement: {rw_gnll_results['RW-G-NLL_AUROC'] - rw_gnll_results['G-NLL_AUROC']:.4f}\")\n",
        "else:\n",
        "    print(\"No RW-G-NLL results available for comparison\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relevance Weights Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze relevance weights distribution\n",
        "if rw_gnll_details and rw_gnll_details['relevance_weights']:\n",
        "    all_weights = []\n",
        "    for weights in rw_gnll_details['relevance_weights']:\n",
        "        all_weights.extend(weights)\n",
        "    \n",
        "    all_weights = np.array(all_weights)\n",
        "    \n",
        "    plt.figure(figsize=(12, 5))\n",
        "    \n",
        "    # Histogram of relevance weights\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(all_weights, bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
        "    plt.xlabel('Relevance Weight R_T(y_t)', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.title('Distribution of Token Relevance Weights', fontsize=12, fontweight='bold')\n",
        "    plt.axvline(np.mean(all_weights), color='red', linestyle='--', label=f'Mean: {np.mean(all_weights):.3f}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Box plot by correctness\n",
        "    plt.subplot(1, 2, 2)\n",
        "    correct_weights = []\n",
        "    incorrect_weights = []\n",
        "    for i, weights in enumerate(rw_gnll_details['relevance_weights']):\n",
        "        if rw_gnll_details['y_true'][i] == 1:\n",
        "            correct_weights.extend(weights)\n",
        "        else:\n",
        "            incorrect_weights.extend(weights)\n",
        "    \n",
        "    plt.boxplot([correct_weights, incorrect_weights], labels=['Correct', 'Incorrect'])\n",
        "    plt.ylabel('Relevance Weight R_T(y_t)', fontsize=12)\n",
        "    plt.title('Relevance Weights by Answer Correctness', fontsize=12, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nRelevance Weights Statistics:\")\n",
        "    print(f\"  Mean: {np.mean(all_weights):.4f}\")\n",
        "    print(f\"  Std: {np.std(all_weights):.4f}\")\n",
        "    print(f\"  Min: {np.min(all_weights):.4f}\")\n",
        "    print(f\"  Max: {np.max(all_weights):.4f}\")\n",
        "    print(f\"  Mean (Correct answers): {np.mean(correct_weights):.4f}\")\n",
        "    print(f\"  Mean (Incorrect answers): {np.mean(incorrect_weights):.4f}\")\n",
        "else:\n",
        "    print(\"No relevance weights data available\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results to JSON\n",
        "results_export = {}\n",
        "\n",
        "if short_results_rouge:\n",
        "    results_export['short_rouge'] = {\n",
        "        'G-NLL_AUROC': float(short_results_rouge['G-NLL_AUROC']),\n",
        "        'Accuracy': float(short_results_rouge['Accuracy']),\n",
        "        'Num_examples': int(short_results_rouge['Num_examples']),\n",
        "        'Mean_G-NLL': float(short_results_rouge['Mean_G-NLL']),\n",
        "        'Std_G-NLL': float(short_results_rouge['Std_G-NLL']),\n",
        "        'Min_G-NLL': float(short_results_rouge['Min_G-NLL']),\n",
        "        'Max_G-NLL': float(short_results_rouge['Max_G-NLL'])\n",
        "    }\n",
        "\n",
        "if short_results_judge:\n",
        "    results_export['short_judge'] = {\n",
        "        'G-NLL_AUROC': float(short_results_judge['G-NLL_AUROC']),\n",
        "        'Accuracy': float(short_results_judge['Accuracy']),\n",
        "        'Num_examples': int(short_results_judge['Num_examples']),\n",
        "        'Mean_G-NLL': float(short_results_judge['Mean_G-NLL']),\n",
        "        'Std_G-NLL': float(short_results_judge['Std_G-NLL']),\n",
        "        'Min_G-NLL': float(short_results_judge['Min_G-NLL']),\n",
        "        'Max_G-NLL': float(short_results_judge['Max_G-NLL'])\n",
        "    }\n",
        "\n",
        "if long_results_judge:\n",
        "    results_export['long_judge'] = {\n",
        "        'G-NLL_AUROC': float(long_results_judge['G-NLL_AUROC']),\n",
        "        'Accuracy': float(long_results_judge['Accuracy']),\n",
        "        'Num_examples': int(long_results_judge['Num_examples']),\n",
        "        'Mean_G-NLL': float(long_results_judge['Mean_G-NLL']),\n",
        "        'Std_G-NLL': float(long_results_judge['Std_G-NLL']),\n",
        "        'Min_G-NLL': float(long_results_judge['Min_G-NLL']),\n",
        "        'Max_G-NLL': float(long_results_judge['Max_G-NLL'])\n",
        "    }\n",
        "\n",
        "if results_export:\n",
        "    output_file = 'gnll_baseline_analysis_results.json'\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results_export, f, indent=2)\n",
        "    print(f\"\\n✅ Results exported to {output_file}\")\n",
        "else:\n",
        "    print(\"No results to export.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
