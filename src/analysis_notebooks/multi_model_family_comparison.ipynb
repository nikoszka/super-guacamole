{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model Family Comparison Analysis\n",
    "\n",
    "This notebook analyzes and visualizes results from the multi-model family experiments:\n",
    "- **3 Model Families**: Llama, Qwen, Mistral\n",
    "- **2 Models per Family**: Small (1B-1.5B) and Large (7B-8B)\n",
    "- **2 Datasets**: TriviaQA and SQuAD\n",
    "\n",
    "## Analysis Dimensions:\n",
    "1. Model size effect within families\n",
    "2. Model family comparison at similar sizes\n",
    "3. Dataset effect (TriviaQA vs SQuAD)\n",
    "4. Uncertainty calibration (G-NLL AUROC)\n",
    "5. Accuracy vs uncertainty correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AUROC results\n",
    "results_dir = Path('../../results/multi_model_auroc')\n",
    "df = pd.read_csv(results_dir / 'auroc_results.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} experiment results\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"Overall Statistics\")\n",
    "print(\"=\" * 80)\n",
    "print(df[['G-NLL_AUROC', 'Accuracy', 'Mean_G-NLL']].describe())\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"By Model Family\")\n",
    "print(\"=\" * 80)\n",
    "family_stats = df.groupby('model_family').agg({\n",
    "    'G-NLL_AUROC': ['mean', 'std'],\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'model_name': 'count'\n",
    "}).round(4)\n",
    "family_stats.columns = ['_'.join(col) for col in family_stats.columns]\n",
    "print(family_stats)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"By Model Size\")\n",
    "print(\"=\" * 80)\n",
    "size_stats = df.groupby('model_size').agg({\n",
    "    'G-NLL_AUROC': ['mean', 'std'],\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'model_name': 'count'\n",
    "}).round(4)\n",
    "size_stats.columns = ['_'.join(col) for col in size_stats.columns]\n",
    "print(size_stats)\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"By Dataset\")\n",
    "print(\"=\" * 80)\n",
    "dataset_stats = df.groupby('dataset').agg({\n",
    "    'G-NLL_AUROC': ['mean', 'std'],\n",
    "    'Accuracy': ['mean', 'std'],\n",
    "    'model_name': 'count'\n",
    "}).round(4)\n",
    "dataset_stats.columns = ['_'.join(col) for col in dataset_stats.columns]\n",
    "print(dataset_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualization: AUROC Comparison Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for heatmap\n",
    "pivot_auroc = df.pivot_table(\n",
    "    values='G-NLL_AUROC',\n",
    "    index=['model_family', 'model_size'],\n",
    "    columns='dataset',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Create heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(pivot_auroc, annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "            vmin=0.5, vmax=1.0, ax=ax, cbar_kws={'label': 'G-NLL AUROC'})\n",
    "plt.title('G-NLL AUROC: Model Family × Size × Dataset', fontsize=14, weight='bold')\n",
    "plt.xlabel('Dataset', fontsize=12)\n",
    "plt.ylabel('Model (Family, Size)', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'auroc_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: auroc_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Family Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot: AUROC by model family\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: By family\n",
    "family_auroc = df.groupby('model_family')['G-NLL_AUROC'].mean().sort_values(ascending=False)\n",
    "family_auroc.plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Average G-NLL AUROC by Model Family', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Model Family', fontsize=11)\n",
    "axes[0].set_ylabel('G-NLL AUROC', fontsize=11)\n",
    "axes[0].set_ylim([0.5, 1.0])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: By family and size\n",
    "family_size_auroc = df.groupby(['model_family', 'model_size'])['G-NLL_AUROC'].mean().unstack()\n",
    "family_size_auroc.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "axes[1].set_title('G-NLL AUROC by Model Family and Size', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Model Family', fontsize=11)\n",
    "axes[1].set_ylabel('G-NLL AUROC', fontsize=11)\n",
    "axes[1].set_ylim([0.5, 1.0])\n",
    "axes[1].legend(title='Size', fontsize=10)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'family_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: family_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Size Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze size effect within each family\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: AUROC improvement from small to large\n",
    "size_comparison = df.pivot_table(\n",
    "    values='G-NLL_AUROC',\n",
    "    index='model_family',\n",
    "    columns='model_size',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "if 'Small' in size_comparison.columns and 'Large' in size_comparison.columns:\n",
    "    size_comparison['Improvement'] = size_comparison['Large'] - size_comparison['Small']\n",
    "    size_comparison[['Small', 'Large']].plot(kind='bar', ax=axes[0], width=0.8)\n",
    "    axes[0].set_title('AUROC: Small vs Large Models by Family', fontsize=12, weight='bold')\n",
    "    axes[0].set_xlabel('Model Family', fontsize=11)\n",
    "    axes[0].set_ylabel('G-NLL AUROC', fontsize=11)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy improvement\n",
    "acc_comparison = df.pivot_table(\n",
    "    values='Accuracy',\n",
    "    index='model_family',\n",
    "    columns='model_size',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "if 'Small' in acc_comparison.columns and 'Large' in acc_comparison.columns:\n",
    "    acc_comparison[['Small', 'Large']].plot(kind='bar', ax=axes[1], width=0.8, color=['coral', 'darkred'])\n",
    "    axes[1].set_title('Accuracy: Small vs Large Models by Family', fontsize=12, weight='bold')\n",
    "    axes[1].set_xlabel('Model Family', fontsize=11)\n",
    "    axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'size_effect.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: size_effect.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Effect (TriviaQA vs SQuAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance across datasets\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: AUROC by dataset and family\n",
    "dataset_family = df.groupby(['dataset', 'model_family'])['G-NLL_AUROC'].mean().unstack()\n",
    "dataset_family.plot(kind='bar', ax=axes[0], width=0.8)\n",
    "axes[0].set_title('G-NLL AUROC by Dataset and Model Family', fontsize=12, weight='bold')\n",
    "axes[0].set_xlabel('Dataset', fontsize=11)\n",
    "axes[0].set_ylabel('G-NLL AUROC', fontsize=11)\n",
    "axes[0].legend(title='Model Family', fontsize=10)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy by dataset and family\n",
    "acc_dataset_family = df.groupby(['dataset', 'model_family'])['Accuracy'].mean().unstack()\n",
    "acc_dataset_family.plot(kind='bar', ax=axes[1], width=0.8)\n",
    "axes[1].set_title('Accuracy by Dataset and Model Family', fontsize=12, weight='bold')\n",
    "axes[1].set_xlabel('Dataset', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].legend(title='Model Family', fontsize=10)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'dataset_effect.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: dataset_effect.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Accuracy vs Uncertainty Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Accuracy vs AUROC\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create scatter plot with different colors for families\n",
    "for family in df['model_family'].unique():\n",
    "    family_data = df[df['model_family'] == family]\n",
    "    ax.scatter(family_data['Accuracy'], family_data['G-NLL_AUROC'], \n",
    "               label=family, s=100, alpha=0.7)\n",
    "\n",
    "# Add model labels\n",
    "for idx, row in df.iterrows():\n",
    "    label = f\"{row['model_family']}-{row['model_size'][:1]}\\n{row['dataset'][:3]}\"\n",
    "    ax.annotate(label, (row['Accuracy'], row['G-NLL_AUROC']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Accuracy', fontsize=12)\n",
    "ax.set_ylabel('G-NLL AUROC (Uncertainty Calibration)', fontsize=12)\n",
    "ax.set_title('Accuracy vs Uncertainty Calibration by Model Family', fontsize=14, weight='bold')\n",
    "ax.legend(title='Model Family', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Add diagonal reference line\n",
    "lims = [max(ax.get_xlim()[0], ax.get_ylim()[0]), min(ax.get_xlim()[1], ax.get_ylim()[1])]\n",
    "ax.plot(lims, lims, 'k--', alpha=0.3, zorder=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'accuracy_vs_auroc.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: accuracy_vs_auroc.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Radar Chart: Model Family Performance Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import pi\n",
    "\n",
    "# Prepare data for radar chart\n",
    "metrics = ['G-NLL_AUROC', 'Accuracy', 'Num_examples']\n",
    "family_profiles = df.groupby('model_family')[metrics].mean()\n",
    "\n",
    "# Normalize metrics to 0-1 scale\n",
    "family_profiles_norm = family_profiles.copy()\n",
    "for col in family_profiles.columns:\n",
    "    min_val = family_profiles[col].min()\n",
    "    max_val = family_profiles[col].max()\n",
    "    if max_val > min_val:\n",
    "        family_profiles_norm[col] = (family_profiles[col] - min_val) / (max_val - min_val)\n",
    "\n",
    "# Create radar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "angles = [n / len(metrics) * 2 * pi for n in range(len(metrics))]\n",
    "angles += angles[:1]\n",
    "\n",
    "for family in family_profiles_norm.index:\n",
    "    values = family_profiles_norm.loc[family].tolist()\n",
    "    values += values[:1]\n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=family)\n",
    "    ax.fill(angles, values, alpha=0.15)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Family Performance Profiles (Normalized)', fontsize=14, weight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'radar_chart.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✅ Saved: radar_chart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Best model overall\n",
    "best_model = df.loc[df['G-NLL_AUROC'].idxmax()]\n",
    "print(f\"1. Best Overall Model (G-NLL AUROC):\")\n",
    "print(f\"   {best_model['model_name']} on {best_model['dataset']}\")\n",
    "print(f\"   AUROC: {best_model['G-NLL_AUROC']:.4f}, Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "print()\n",
    "\n",
    "# Best family\n",
    "best_family = df.groupby('model_family')['G-NLL_AUROC'].mean().idxmax()\n",
    "best_family_auroc = df.groupby('model_family')['G-NLL_AUROC'].mean().max()\n",
    "print(f\"2. Best Model Family:\")\n",
    "print(f\"   {best_family} (Average AUROC: {best_family_auroc:.4f})\")\n",
    "print()\n",
    "\n",
    "# Size effect\n",
    "if 'Small' in df['model_size'].values and 'Large' in df['model_size'].values:\n",
    "    small_auroc = df[df['model_size'] == 'Small']['G-NLL_AUROC'].mean()\n",
    "    large_auroc = df[df['model_size'] == 'Large']['G-NLL_AUROC'].mean()\n",
    "    print(f\"3. Size Effect:\")\n",
    "    print(f\"   Small models: {small_auroc:.4f}\")\n",
    "    print(f\"   Large models: {large_auroc:.4f}\")\n",
    "    print(f\"   Improvement: {(large_auroc - small_auroc):.4f} ({(large_auroc/small_auroc - 1)*100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# Dataset effect\n",
    "dataset_auroc = df.groupby('dataset')['G-NLL_AUROC'].mean()\n",
    "print(f\"4. Dataset Effect:\")\n",
    "for dataset, auroc in dataset_auroc.items():\n",
    "    print(f\"   {dataset}: {auroc:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'experiment_overview': {\n",
    "        'total_experiments': len(df),\n",
    "        'model_families': df['model_family'].unique().tolist(),\n",
    "        'datasets': df['dataset'].unique().tolist(),\n",
    "        'model_sizes': df['model_size'].unique().tolist()\n",
    "    },\n",
    "    'best_performers': {\n",
    "        'overall': {\n",
    "            'model': best_model['model_name'],\n",
    "            'dataset': best_model['dataset'],\n",
    "            'auroc': float(best_model['G-NLL_AUROC']),\n",
    "            'accuracy': float(best_model['Accuracy'])\n",
    "        },\n",
    "        'by_family': df.groupby('model_family')['G-NLL_AUROC'].mean().to_dict()\n",
    "    },\n",
    "    'statistics': {\n",
    "        'by_family': family_stats.to_dict(),\n",
    "        'by_size': size_stats.to_dict(),\n",
    "        'by_dataset': dataset_stats.to_dict()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(results_dir / 'analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ Saved: analysis_summary.json\")\n",
    "print(\"\\nAll analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
