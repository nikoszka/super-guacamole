# üöÄ nllSAR Project: Complete Summary

## Table of Contents
1. [Overview](#overview)
2. [Critical Issues Fixed](#critical-issues-fixed)
3. [What We Changed](#what-we-changed)
4. [Why We Made These Changes](#why-we-made-these-changes)
5. [Final Setup & Usage](#final-setup--usage)
6. [Analysis Pipeline](#analysis-pipeline)
7. [Key Files & Structure](#key-files--structure)

---

## Overview

This project implements **G-NLL (Generative Negative Log-Likelihood)** and **RW-G-NLL (Relevance-Weighted G-NLL)** uncertainty quantification methods for question-answering systems. We enhanced the baseline implementation with comprehensive token-level analysis tools and fixed critical bugs in answer extraction and token alignment.

**What this system does:**
- Generates answers to questions using language models (Llama-3.2-1B, etc.)
- Computes uncertainty metrics at multiple levels (token, sentence, answer)
- Analyzes which tokens are most uncertain and most important
- Compares multiple uncertainty metrics (G-NLL, RW-G-NLL, SAR, Semantic Entropy)
- Provides interactive visualization tools for token-level analysis

---

## Critical Issues Fixed

### Issue 1: Broken Answer Extraction with Few-Shot Prompts

**Problem:**
When using few-shot prompts (e.g., 5 examples), the answer extraction logic was fundamentally broken. It used string matching to find "Answer:" markers, but with 5 few-shot examples, there were 6 "Answer:" markers in the output. The code would often extract answers from few-shot examples instead of the actual generated answer.

**Symptoms:**
```yaml
Config: --brief_prompt detailed --model_max_new_tokens 200
Expected: Full detailed sentences (50-100+ tokens)
Got: Short fragments like "professor plum", "ringway" (2-3 tokens)
```

**Root Cause:**
```python
# OLD (BROKEN):
last_answer_idx = full_answer.lower().rfind('answer:')
answer = full_answer[last_answer_idx + len('answer:'):].strip()
# ‚ùå Grabs from few-shot examples!
```

**Solution:**
Changed to **token-based extraction** using exact token positions:

```python
# NEW (FIXED):
generated_token_ids = outputs.sequences[0][n_input_token:]
answer = self.tokenizer.decode(generated_token_ids, skip_special_tokens=True)
# ‚úÖ Always correct!
```

**Impact:**
- ‚úÖ Proper extraction of detailed answers
- ‚úÖ Accurate token counts for analysis
- ‚úÖ Reliable results regardless of prompt format

**Files Modified:** `src/models/huggingface_models.py` (lines ~417-480)

---

### Issue 2: Token Count Mismatches in Analysis

**Problem:**
Analysis scripts were re-tokenizing response text offline, producing different token counts than during generation. This caused **65% of examples to be skipped** with warnings like:

```
WARNING: Token count mismatch (len(tokens)=2, len(log_liks)=3). Skipping.
```

**Root Cause:**
- Generation created token log-likelihoods but didn't store the actual tokens
- Analysis re-tokenized the response text
- Re-tokenization produced different boundaries due to context/special tokens

**Solution:**
Store exact tokens and token IDs during generation:

```python
# NEW: Store exact tokens from generation
most_likely_answer_dict = {
    'response': predicted_answer,
    'token_ids': token_ids,           # NEW: Exact token IDs
    'tokens': tokens,                 # NEW: Token strings
    'token_log_likelihoods': token_log_likelihoods,
    ...
}
```

Analysis scripts now use stored tokens when available:

```python
# Try stored tokens first (exact alignment)
if "tokens" in mla and len(mla["tokens"]) == len(token_log_likelihoods):
    tokens = mla["tokens"]
else:
    # Fallback: re-tokenize (for old pickles)
    tokens = tokenizer.encode_plus(...)
```

**Impact:**
- ‚úÖ 100% success rate in analysis (up from 35%)
- ‚úÖ Perfect token-to-log-likelihood alignment
- ‚úÖ Backwards compatible with old pickles

---

## What We Changed

### Core Generation Pipeline

#### 1. `src/models/huggingface_models.py`

**Changes:**
- Token-based answer extraction (lines ~417-480)
- Extract and return token IDs and token strings (lines 595-613)
- Updated return signature: 3 values ‚Üí 5 values

```python
# OLD:
return sliced_answer, log_likelihoods, last_token_embedding

# NEW:
generated_token_ids = outputs.sequences[0][n_input_token:n_input_token + n_generated].tolist()
generated_tokens = [self.tokenizer.decode([tid]) for tid in generated_token_ids]
return sliced_answer, log_likelihoods, last_token_embedding, generated_token_ids, generated_tokens
```

#### 2. `src/generate_answers.py`

**Changes:**
- Updated to receive 5 return values from `model.predict()`
- Added `token_ids` and `tokens` to pickle structure
- Changed high-temp responses from tuple to dict format

```python
# NEW pickle structure:
most_likely_answer_dict = {
    'response': str,
    'token_ids': List[int],             # NEW
    'tokens': List[str],                # NEW
    'token_log_likelihoods': List[float],
    'sequence_nll': float,
    'sequence_prob': float,
    'embedding': np.ndarray,
    'accuracy': float
}
```

### Analysis Scripts

#### 3. `src/analysis/phase1_5_token_nll_analysis.py`

**Changes:**
- Uses stored tokens when available (lines 118-136)
- Falls back to re-tokenization for old pickles
- Added debug logging for tracking token source

#### 4. `src/analysis/phase2_token_importance.py`

**Changes:**
- Similar token lookup logic as phase1_5
- Graceful fallback for old pickles

### Helper & Utility Scripts

Updated to handle 5-value returns:
- `src/utils/utils.py` (lines 231, 239)
- `src/uncertainty_measures/p_true.py` (line 34)
- `src/uncertainty_measures/semantic_entropy.py` (line 159)

Updated to handle dict format for high-temp responses:
- `src/compute_uncertainty_measures.py` (lines 226-265, 305)
- `src/uncertainty_measures/semantic_entropy.py` (lines 305-321)
- `src/uncertainty_measures/sar.py` (lines 70-84)

---

## Why We Made These Changes

### 1. **Token-Based Extraction is Fundamentally More Reliable**

String-based extraction is inherently unreliable:
- Multiple occurrences of markers ("Answer:")
- Whitespace variations
- Special character handling
- Encoding issues

Token-based extraction is guaranteed correct:
- We know exactly where generation starts (`n_input_token`)
- Token boundaries are unambiguous
- No string encoding confusion
- Works with any prompt format

### 2. **Exact Token Alignment is Critical for Analysis**

Token-level analysis depends on perfect alignment:

```
Token:         [" The", " Battle", " of", " Hast", "ings"]
Log-Lik:       [-0.23,  -0.16,    -0.09, -0.45,   -0.23]
Relevance:     [0.05,   0.12,     0.03,  0.18,    0.22]
```

If tokens mismatch, analysis is meaningless. Storing exact tokens guarantees:
- NLL analysis shows correct uncertainty per token
- Relevance weights align with correct tokens
- Visualization highlights the right words
- Statistical analysis is valid

### 3. **Backwards Compatibility Preserves Old Data**

We maintained compatibility so:
- Old pickles still work (with degraded accuracy)
- No need to re-run expensive generations
- Gradual migration path
- Testing with both old and new data

### 4. **Comprehensive Documentation Enables Reproducibility**

We created multiple guides because:
- Different use cases need different information
- Quick start vs deep technical details
- Setup instructions vs troubleshooting
- Analysis workflow vs generation parameters

---

## Final Setup & Usage

### Installation

```bash
# Clone repository
git clone <repo-url>
cd nllSAR

# Install dependencies
pip install -r requirements.txt

# Set up wandb (optional but recommended)
wandb login
export WANDB_SEM_UNC_ENTITY="your-username"
```

### Quick Start: Generate Answers

#### Short Answers (ROUGE evaluation, ~20-40 min)

```bash
python -m src.generate_answers \
  --model_name Llama-3.2-1B \
  --dataset trivia_qa \
  --num_samples 400 \
  --num_generations 1 \
  --temperature 0.0 \
  --model_max_new_tokens 50 \
  --brief_prompt short \
  --metric squad \
  --use_context False \
  --compute_p_true False \
  --entity your-username \
  --project nllSAR_short
```

**Output:** `src/nikos/uncertainty/wandb/run-XXXXX/files/validation_generations.pkl`

#### Long Answers (LLM judge, ~30-60 min)

```bash
python -m src.generate_answers \
  --model_name Llama-3.2-1B \
  --dataset trivia_qa \
  --num_samples 400 \
  --num_generations 1 \
  --temperature 0.0 \
  --model_max_new_tokens 200 \
  --brief_prompt detailed \
  --metric llm_llama-3.1-70b \
  --use_context True \
  --compute_p_true False \
  --entity your-username \
  --project nllSAR_long
```

### Verify Your Pickle

```python
import pickle

with open('PATH_TO_PICKLE', 'rb') as f:
    data = pickle.load(f)

example = list(data.values())[0]
mla = example['most_likely_answer']

# Check new fields
print("‚úÖ Has token_ids:", 'token_ids' in mla)
print("‚úÖ Has tokens:", 'tokens' in mla)
print("‚úÖ Token count:", len(mla.get('tokens', [])))
print("‚úÖ First 5 tokens:", mla.get('tokens', [])[:5])
```

---

## Analysis Pipeline

Run these in order for complete analysis:

### Phase 1: Baseline Metrics & Token Statistics

Computes basic uncertainty metrics (G-NLL, average NLL, perplexity):

```bash
# Short answers
python -m src.analysis.phase1_baseline_metrics \
  --short-pickle PATH_TO_SHORT_PKL \
  --output-dir results/phase1_short

# Long answers
python -m src.analysis.phase1_baseline_metrics \
  --long-pickle PATH_TO_LONG_PKL \
  --output-dir results/phase1_long
```

**Outputs:** `token_statistics.json`, `baseline_metrics.csv`

---

### Phase 1.5: Token-Level NLL Analysis

Analyzes per-token negative log-likelihood:

```bash
python -m src.analysis.phase1_5_token_nll_analysis \
  --pickle-path PATH_TO_LONG_PKL \
  --model-name Llama-3.2-1B \
  --sample-size 100 \
  --output-dir results/phase1_5_long
```

**Outputs:**
- `nll_vs_position.png`: Token position vs NLL
- `nll_distribution.png`: Distribution of token NLLs
- `nll_by_pos.png`: Mean NLL by part-of-speech tag
- `sentence_level_nll_examples.json`: Examples for visualization

---

### Phase 1.6: Prefix-Level NLL Analysis

Analyzes how confidence evolves during generation:

```bash
# Long answers
python -m src.analysis.phase1_6_prefix_nll_analysis \
  --pickle-path PATH_TO_LONG_PKL \
  --output-dir results/phase1_6_long \
  --max-prefix-len 50 \
  --ks 1 3 5

# Short answers (with ROUGE)
python -m src.analysis.phase1_6_prefix_nll_analysis \
  --pickle-path PATH_TO_SHORT_PKL \
  --output-dir results/phase1_6_short \
  --use-rouge --rouge-threshold 0.3 \
  --max-prefix-len 50 \
  --ks 1 3 5
```

**Outputs:**
- `prefix_mean_nll_curves.png`: Mean NLL vs prefix length (correct vs incorrect)
- `first_token_nll_boxplot.png`: First token NLL distribution
- `prefix_nll_summary.json`: Early-token AUROC metrics

---

### Phase 2: Token Relevance (SAR-Style) Analysis

Computes token importance using SAR-style ablation:

```bash
python -m src.analysis.phase2_token_importance \
  --pickle-path PATH_TO_LONG_PKL \
  --model-name Llama-3.2-1B \
  --similarity-model cross-encoder/stsb-roberta-large \
  --sample-size 50 \
  --output-dir results/phase2_long
```

**Outputs:**
- `relevance_vs_position.png`: Token relevance vs position
- `relevance_vs_nll.png`: Correlation between relevance and NLL
- `token_importance_examples.json`: Examples for visualization (up to 10)
- `analysis_summary.json`: Correlations & summary statistics

---

### Phase 5: Comparative AUROC Analysis

Compares all uncertainty metrics:

```bash
python -m src.analysis.phase5_comparative_analysis \
  --pickle-path PATH_TO_LONG_PKL \
  --model-name Llama-3.2-1B \
  --similarity-model cross-encoder/stsb-roberta-large \
  --output-dir results/phase5_long
```

**Outputs:**
- `auroc_comparison.csv`: AUROC for all metrics
- `roc_curves.png`: ROC curves overlay
- `cost_performance_plot.png`: Cost vs AUROC trade-off

**Metrics Compared:**
- Baselines: G-NLL, Average NLL, Perplexity
- SOTA: SAR, Semantic Entropy
- Hybrid: **RW-G-NLL** (our method)

---

### Interactive Visualization

```bash
streamlit run src/analysis/token_visualization_app.py
```

**In the browser:**
- **Mode 1: Raw NLL** ‚Üí Load `results/phase1_5_long/sentence_level_nll_examples.json`
- **Mode 2: Relevance-weighted** ‚Üí Load `results/phase2_long/token_importance_examples.json`

Visualizes tokens with colored backgrounds showing:
- Token-level NLL (uncertainty)
- Token-level relevance (importance)
- Relevance √ó NLL (RW-G-NLL contribution)

---

## Key Files & Structure

### Generated Data
```
src/nikos/uncertainty/wandb/run-<ID>/files/
‚îú‚îÄ‚îÄ validation_generations.pkl    # Main analysis file
‚îú‚îÄ‚îÄ train_generations.pkl          # Training data (optional)
‚îî‚îÄ‚îÄ experiment_details.pkl         # Run configuration
```

### Analysis Results
```
results/
‚îú‚îÄ‚îÄ phase1_short/
‚îÇ   ‚îú‚îÄ‚îÄ token_statistics.json
‚îÇ   ‚îú‚îÄ‚îÄ baseline_metrics_short.csv
‚îÇ   ‚îî‚îÄ‚îÄ baseline_metrics.json
‚îú‚îÄ‚îÄ phase1_5_long/
‚îÇ   ‚îú‚îÄ‚îÄ nll_vs_position.png
‚îÇ   ‚îú‚îÄ‚îÄ nll_distribution.png
‚îÇ   ‚îî‚îÄ‚îÄ sentence_level_nll_examples.json
‚îú‚îÄ‚îÄ phase2_long/
‚îÇ   ‚îú‚îÄ‚îÄ relevance_vs_position.png
‚îÇ   ‚îú‚îÄ‚îÄ token_importance_examples.json
‚îÇ   ‚îî‚îÄ‚îÄ analysis_summary.json
‚îî‚îÄ‚îÄ phase5_long/
    ‚îú‚îÄ‚îÄ auroc_comparison.csv
    ‚îú‚îÄ‚îÄ roc_curves.png
    ‚îî‚îÄ‚îÄ cost_performance_plot.png
```

### Documentation Files

| File | Purpose |
|------|---------|
| `PROJECT_SUMMARY.md` | **This file** - Complete overview |
| `QUICK_START.md` | Quick commands to get started |
| `GENERATION_SETTINGS_GUIDE.md` | Detailed parameter explanations |
| `ANALYSIS_README.md` | Full analysis pipeline guide |
| `CHANGES_SUMMARY.md` | Technical details of code changes |
| `ANSWER_EXTRACTION_FIX.md` | Deep dive into extraction fix |
| `QUICK_FIX_SUMMARY.md` | Short version of extraction fix |

---

## Key Configuration Parameters

### For Token-Level Analysis (Recommended)

```bash
--num_generations 1          # Just greedy decoding
--temperature 0.0            # Deterministic
--model_max_new_tokens 150   # Medium length for varied patterns
--brief_prompt detailed      # Encourages multi-token answers
--num_few_shot 5            # Good context (unrelated to temperature)
```

**This configuration works for:**
- ‚úÖ Phase 1: Baseline metrics
- ‚úÖ Phase 1.5: Token NLL analysis
- ‚úÖ Phase 1.6: Prefix NLL analysis
- ‚úÖ Phase 2: Token relevance (RW-G-NLL)
- ‚úÖ Phase 5 (partial): Baseline metrics + RW-G-NLL
- ‚ùå Phase 5 (full): SAR/SE (need multiple generations)

### For SAR/SE Analysis (Optional)

```bash
--num_generations 10         # Multiple samples
--temperature 1.0            # Stochastic sampling
```

‚ö†Ô∏è **Note:** With temperature > 0, the i=0 generation is also stochastic (not pure greedy). Consider running two separate experiments if you need both greedy baseline and SAR/SE.

---

## Expected Results

### Before Fixes
- ‚ùå 27-35% of examples successfully analyzed
- ‚ùå Short fragmented answers despite `--brief_prompt detailed`
- ‚ùå Token count mismatches
- ‚ùå Unreliable metrics

### After Fixes
- ‚úÖ 100% of examples successfully analyzed
- ‚úÖ Full detailed answers as configured
- ‚úÖ Perfect token alignment
- ‚úÖ Reliable, reproducible metrics

---

## Troubleshooting

### Still seeing token mismatches?
‚Üí You're using an old pickle. Re-generate with the updated code.

### Getting "professor plum" instead of full sentences?
‚Üí Old code with answer extraction bug. Pull latest changes and regenerate.

### `ValueError: too many values to unpack`?
‚Üí Old analysis script with new pickles. Pull latest changes.

### CUDA out of memory?
‚Üí Reduce `--num_samples` or use smaller model (`Llama-3.2-1B` instead of `8B`).

---

## Next Steps

1. **Generate fresh pickles** using the recommended settings above
2. **Run analysis pipeline** (Phase 1 ‚Üí 1.5 ‚Üí 1.6 ‚Üí 2 ‚Üí 5)
3. **Visualize results** with Streamlit app
4. **Compare metrics** in Phase 5 outputs
5. **Iterate** with different models/datasets

---

## Summary Table

| Component | Status | Impact |
|-----------|--------|--------|
| **Answer Extraction** | ‚úÖ Fixed | Token-based, bulletproof |
| **Token Alignment** | ‚úÖ Fixed | 100% success rate |
| **Few-Shot Prompts** | ‚úÖ Fixed | Works reliably |
| **Pickle Structure** | ‚úÖ Enhanced | +2 fields (token_ids, tokens) |
| **Backwards Compatibility** | ‚úÖ Maintained | Old pickles still work |
| **Analysis Coverage** | ‚úÖ Complete | 5 phases + visualization |
| **Documentation** | ‚úÖ Comprehensive | 7 markdown guides |

---

## Credits

**Original Repository:** Based on SAR/G-NLL uncertainty quantification research  
**Enhancements:** Token-level analysis, bug fixes, comprehensive documentation  
**Models Used:** Llama-3.2-1B, Llama-3.1-8B, Llama-3.1-70B (judge)  
**Datasets:** TriviaQA, SQuAD, BioASQ

---

## Quick Reference Commands

```bash
# Generate short answers
python -m src.generate_answers --model_name Llama-3.2-1B --num_samples 400 \
  --num_generations 1 --temperature 0.0 --model_max_new_tokens 50 \
  --brief_prompt short --metric squad --use_context False

# Generate long answers
python -m src.generate_answers --model_name Llama-3.2-1B --num_samples 400 \
  --num_generations 1 --temperature 0.0 --model_max_new_tokens 200 \
  --brief_prompt detailed --metric llm_llama-3.1-70b --use_context True

# Run all analysis phases
python -m src.analysis.phase1_baseline_metrics --long-pickle PATH --output-dir results/phase1_long
python -m src.analysis.phase1_5_token_nll_analysis --pickle-path PATH --model-name Llama-3.2-1B --output-dir results/phase1_5_long
python -m src.analysis.phase1_6_prefix_nll_analysis --pickle-path PATH --output-dir results/phase1_6_long
python -m src.analysis.phase2_token_importance --pickle-path PATH --model-name Llama-3.2-1B --output-dir results/phase2_long
python -m src.analysis.phase5_comparative_analysis --pickle-path PATH --model-name Llama-3.2-1B --output-dir results/phase5_long

# Visualize
streamlit run src/analysis/token_visualization_app.py
```

---

**You're all set!** üéâ Generate your pickles, run the analysis, and explore token-level uncertainty patterns!


